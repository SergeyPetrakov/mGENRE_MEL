{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Python version and upgrading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kiLr036GIvcc",
    "outputId": "58e463c0-0c4f-4990-a4ef-227b12e090ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/petrakov/.local/lib/python3.8/site-packages (22.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/petrakov/anaconda3/envs/success\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    openssl-1.1.1p             |       h5eee18b_0         2.5 MB\n",
      "    pytorch-1.12.0             |py3.10_cuda11.3_cudnn8.3.2_0        1.19 GB  pytorch\n",
      "    requests-2.28.0            |  py310h06a4308_0          94 KB\n",
      "    torchvision-0.13.0         |      py310_cu113        28.3 MB  pytorch\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        1.22 GB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
      "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu\n",
      "  blas               pkgs/main/linux-64::blas-1.0-mkl\n",
      "  brotlipy           pkgs/main/linux-64::brotlipy-0.7.0-py310h7f8727e_1002\n",
      "  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h7b6447c_0\n",
      "  ca-certificates    pkgs/main/linux-64::ca-certificates-2022.4.26-h06a4308_0\n",
      "  certifi            pkgs/main/linux-64::certifi-2022.6.15-py310h06a4308_0\n",
      "  cffi               pkgs/main/linux-64::cffi-1.15.0-py310hd667e15_1\n",
      "  charset-normalizer pkgs/main/noarch::charset-normalizer-2.0.4-pyhd3eb1b0_0\n",
      "  cryptography       pkgs/main/linux-64::cryptography-37.0.1-py310h9ce1e76_0\n",
      "  cudatoolkit        pkgs/main/linux-64::cudatoolkit-11.3.1-h2bc3f7f_2\n",
      "  ffmpeg             pytorch/linux-64::ffmpeg-4.3-hf484d3e_0\n",
      "  freetype           pkgs/main/linux-64::freetype-2.11.0-h70c0345_0\n",
      "  giflib             pkgs/main/linux-64::giflib-5.2.1-h7b6447c_0\n",
      "  gmp                pkgs/main/linux-64::gmp-6.2.1-h295c915_3\n",
      "  gnutls             pkgs/main/linux-64::gnutls-3.6.15-he1e5248_0\n",
      "  idna               pkgs/main/noarch::idna-3.3-pyhd3eb1b0_0\n",
      "  intel-openmp       pkgs/main/linux-64::intel-openmp-2021.4.0-h06a4308_3561\n",
      "  jpeg               pkgs/main/linux-64::jpeg-9e-h7f8727e_0\n",
      "  lame               pkgs/main/linux-64::lame-3.100-h7b6447c_0\n",
      "  lcms2              pkgs/main/linux-64::lcms2-2.12-h3be6417_0\n",
      "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.38-h1181459_1\n",
      "  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_2\n",
      "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1\n",
      "  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1\n",
      "  libiconv           pkgs/main/linux-64::libiconv-1.16-h7f8727e_2\n",
      "  libidn2            pkgs/main/linux-64::libidn2-2.3.2-h7f8727e_0\n",
      "  libpng             pkgs/main/linux-64::libpng-1.6.37-hbc83047_0\n",
      "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1\n",
      "  libtasn1           pkgs/main/linux-64::libtasn1-4.16.0-h27cfd23_0\n",
      "  libtiff            pkgs/main/linux-64::libtiff-4.2.0-h2818925_1\n",
      "  libunistring       pkgs/main/linux-64::libunistring-0.9.10-h27cfd23_0\n",
      "  libuuid            pkgs/main/linux-64::libuuid-1.0.3-h7f8727e_2\n",
      "  libwebp            pkgs/main/linux-64::libwebp-1.2.2-h55f646e_0\n",
      "  libwebp-base       pkgs/main/linux-64::libwebp-base-1.2.2-h7f8727e_0\n",
      "  lz4-c              pkgs/main/linux-64::lz4-c-1.9.3-h295c915_1\n",
      "  mkl                pkgs/main/linux-64::mkl-2021.4.0-h06a4308_640\n",
      "  mkl-service        pkgs/main/linux-64::mkl-service-2.4.0-py310h7f8727e_0\n",
      "  mkl_fft            pkgs/main/linux-64::mkl_fft-1.3.1-py310hd6ae3a3_0\n",
      "  mkl_random         pkgs/main/linux-64::mkl_random-1.2.2-py310h00e6091_0\n",
      "  ncurses            pkgs/main/linux-64::ncurses-6.3-h7f8727e_2\n",
      "  nettle             pkgs/main/linux-64::nettle-3.7.3-hbbd107a_1\n",
      "  numpy              pkgs/main/linux-64::numpy-1.22.3-py310hfa59a62_0\n",
      "  numpy-base         pkgs/main/linux-64::numpy-base-1.22.3-py310h9585f30_0\n",
      "  openh264           pkgs/main/linux-64::openh264-2.1.1-h4ff587b_0\n",
      "  openssl            pkgs/main/linux-64::openssl-1.1.1p-h5eee18b_0\n",
      "  pillow             pkgs/main/linux-64::pillow-9.0.1-py310h22f2fdc_0\n",
      "  pip                pkgs/main/linux-64::pip-21.2.4-py310h06a4308_0\n",
      "  pycparser          pkgs/main/noarch::pycparser-2.21-pyhd3eb1b0_0\n",
      "  pyopenssl          pkgs/main/noarch::pyopenssl-22.0.0-pyhd3eb1b0_0\n",
      "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py310h06a4308_0\n",
      "  python             pkgs/main/linux-64::python-3.10.4-h12debd9_0\n",
      "  pytorch            pytorch/linux-64::pytorch-1.12.0-py3.10_cuda11.3_cudnn8.3.2_0\n",
      "  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda\n",
      "  readline           pkgs/main/linux-64::readline-8.1.2-h7f8727e_1\n",
      "  requests           pkgs/main/linux-64::requests-2.28.0-py310h06a4308_0\n",
      "  setuptools         pkgs/main/linux-64::setuptools-61.2.0-py310h06a4308_0\n",
      "  six                pkgs/main/noarch::six-1.16.0-pyhd3eb1b0_1\n",
      "  sqlite             pkgs/main/linux-64::sqlite-3.38.5-hc218d9a_0\n",
      "  tk                 pkgs/main/linux-64::tk-8.6.12-h1ccaba5_0\n",
      "  torchvision        pytorch/linux-64::torchvision-0.13.0-py310_cu113\n",
      "  typing_extensions  pkgs/main/noarch::typing_extensions-4.1.1-pyh06a4308_0\n",
      "  tzdata             pkgs/main/noarch::tzdata-2022a-hda174b7_0\n",
      "  urllib3            pkgs/main/linux-64::urllib3-1.26.9-py310h06a4308_0\n",
      "  wheel              pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0\n",
      "  xz                 pkgs/main/linux-64::xz-5.2.5-h7f8727e_1\n",
      "  zlib               pkgs/main/linux-64::zlib-1.2.12-h7f8727e_2\n",
      "  zstd               pkgs/main/linux-64::zstd-1.5.2-ha4553b6_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "requests-2.28.0      | 94 KB     | ##################################### | 100% \n",
      "pytorch-1.12.0       | 1.19 GB   | ##################################### | 100% \n",
      "openssl-1.1.1p       | 2.5 MB    | ##################################### | 100% \n",
      "torchvision-0.13.0   | 28.3 MB   | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: / By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!conda update -n base -c defaults conda\n",
    "!conda install -y pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y numpy \n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations of 3 key libraries: KILT, GENRE and fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFeVVTtwDUAl",
    "outputId": "d2c065f5-98ad-4602-f925-f80df0c4a31f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'KILT'...\n",
      "remote: Enumerating objects: 401, done.\u001b[K\n",
      "remote: Counting objects: 100% (151/151), done.\u001b[K\n",
      "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
      "remote: Total 401 (delta 108), reused 92 (delta 92), pack-reused 250\u001b[K\n",
      "Receiving objects: 100% (401/401), 829.97 KiB | 3.32 MiB/s, done.\n",
      "Resolving deltas: 100% (224/224), done.\n",
      "/home/petrakov/success/mGENRE_MEL/KILT\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0mTraceback (most recent call last):\n",
      "  File \"setup.py\", line 8, in <module>\n",
      "    import setuptools\n",
      "ImportError: No module named setuptools\n",
      "/home/petrakov/success/mGENRE_MEL\n",
      "Cloning into 'GENRE'...\n",
      "remote: Enumerating objects: 454, done.\u001b[K\n",
      "remote: Counting objects: 100% (170/170), done.\u001b[K\n",
      "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
      "remote: Total 454 (delta 112), reused 104 (delta 80), pack-reused 284\u001b[K\n",
      "Receiving objects: 100% (454/454), 10.99 MiB | 4.14 MiB/s, done.\n",
      "Resolving deltas: 100% (259/259), done.\n",
      "/home/petrakov/success/mGENRE_MEL/GENRE\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Processing /home/petrakov/success/mGENRE_MEL/GENRE\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: genre\n",
      "  Building wheel for genre (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for genre: filename=genre-0.1.3-py3-none-any.whl size=22447 sha256=40c8274d163f8562b8df6ab853e53435d1c0ab2da7cbd1b165aa6b74d7f5a43b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-res4gn2v/wheels/62/4a/37/1ff1546f37fa19ed2bd95b919f4e8ca9c5bb97230251c11707\n",
      "Successfully built genre\n",
      "Installing collected packages: genre\n",
      "  Attempting uninstall: genre\n",
      "    Found existing installation: genre 0.1.3\n",
      "    Uninstalling genre-0.1.3:\n",
      "      Successfully uninstalled genre-0.1.3\n",
      "Successfully installed genre-0.1.3\n",
      "Traceback (most recent call last):\n",
      "  File \"./setup.py\", line 7, in <module>\n",
      "    import setuptools\n",
      "ImportError: No module named setuptools\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/petrakov/.local/lib/python3.8/site-packages (0.1.96)\n",
      "Requirement already satisfied: marisa_trie in /home/petrakov/.local/lib/python3.8/site-packages (0.7.7)\n",
      "Requirement already satisfied: setuptools in /home/petrakov/.local/lib/python3.8/site-packages (from marisa_trie) (59.5.0)\n",
      "/home/petrakov/success/mGENRE_MEL\n",
      "Cloning into 'fairseq'...\n",
      "remote: Enumerating objects: 25465, done.\u001b[K\n",
      "remote: Total 25465 (delta 0), reused 0 (delta 0), pack-reused 25465\u001b[K\n",
      "Receiving objects: 100% (25465/25465), 19.79 MiB | 4.63 MiB/s, done.\n",
      "Resolving deltas: 100% (18494/18494), done.\n",
      "/home/petrakov/success/mGENRE_MEL/fairseq\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///home/petrakov/success/mGENRE_MEL/fairseq\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (4.32.1)\n",
      "Requirement already satisfied: cython in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (0.29.30)\n",
      "Requirement already satisfied: numpy in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.17.2)\n",
      "Requirement already satisfied: cffi in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.15.0)\n",
      "Requirement already satisfied: regex in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (2019.8.19)\n",
      "Requirement already satisfied: torch in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.11.0)\n",
      "Requirement already satisfied: hydra-core<1.1 in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.0.7)\n",
      "Requirement already satisfied: omegaconf<2.1 in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (2.0.6)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (2.1.0)\n",
      "Requirement already satisfied: importlib-resources in /home/petrakov/.local/lib/python3.8/site-packages (from hydra-core<1.1->fairseq==1.0.0a0+4c4d5a7) (5.8.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /home/petrakov/.local/lib/python3.8/site-packages (from hydra-core<1.1->fairseq==1.0.0a0+4c4d5a7) (4.8)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in /home/petrakov/.local/lib/python3.8/site-packages (from omegaconf<2.1->fairseq==1.0.0a0+4c4d5a7) (6.0)\n",
      "Requirement already satisfied: typing-extensions in /home/petrakov/.local/lib/python3.8/site-packages (from omegaconf<2.1->fairseq==1.0.0a0+4c4d5a7) (4.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/petrakov/.local/lib/python3.8/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4c4d5a7) (0.8.10)\n",
      "Requirement already satisfied: portalocker in /home/petrakov/.local/lib/python3.8/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4c4d5a7) (2.4.0)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4c4d5a7) (0.4.3)\n",
      "Requirement already satisfied: pycparser in /home/petrakov/.local/lib/python3.8/site-packages (from cffi->fairseq==1.0.0a0+4c4d5a7) (2.21)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/petrakov/.local/lib/python3.8/site-packages (from importlib-resources->hydra-core<1.1->fairseq==1.0.0a0+4c4d5a7) (3.8.0)\n",
      "Installing collected packages: fairseq\n",
      "  Attempting uninstall: fairseq\n",
      "    Found existing installation: fairseq 0.12.2\n",
      "    Uninstalling fairseq-0.12.2:\n",
      "      Successfully uninstalled fairseq-0.12.2\n",
      "  Running setup.py develop for fairseq\n",
      "Successfully installed fairseq\n",
      "Traceback (most recent call last):\n",
      "  File \"setup.py\", line 10, in <module>\n",
      "    from setuptools import setup, find_packages, Extension\n",
      "ImportError: No module named setuptools\n",
      "Traceback (most recent call last):\n",
      "  File \"setup.py\", line 10, in <module>\n",
      "    from setuptools import setup, find_packages, Extension\n",
      "ImportError: No module named setuptools\n",
      "/home/petrakov/success/mGENRE_MEL\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement kilt (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for kilt\u001b[0m\u001b[31m\n",
      "\u001b[0m/home/petrakov/success/mGENRE_MEL/fairseq\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///home/petrakov/success/mGENRE_MEL/fairseq\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: hydra-core<1.1 in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.0.7)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (2.1.0)\n",
      "Requirement already satisfied: regex in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (2019.8.19)\n",
      "Requirement already satisfied: cython in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (0.29.30)\n",
      "Requirement already satisfied: cffi in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.15.0)\n",
      "Requirement already satisfied: numpy in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.17.2)\n",
      "Requirement already satisfied: omegaconf<2.1 in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (2.0.6)\n",
      "Requirement already satisfied: tqdm in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (4.32.1)\n",
      "Requirement already satisfied: torch in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.11.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /home/petrakov/.local/lib/python3.8/site-packages (from hydra-core<1.1->fairseq==1.0.0a0+4c4d5a7) (4.8)\n",
      "Requirement already satisfied: importlib-resources in /home/petrakov/.local/lib/python3.8/site-packages (from hydra-core<1.1->fairseq==1.0.0a0+4c4d5a7) (5.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in /home/petrakov/.local/lib/python3.8/site-packages (from omegaconf<2.1->fairseq==1.0.0a0+4c4d5a7) (6.0)\n",
      "Requirement already satisfied: typing-extensions in /home/petrakov/.local/lib/python3.8/site-packages (from omegaconf<2.1->fairseq==1.0.0a0+4c4d5a7) (4.2.0)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4c4d5a7) (0.4.3)\n",
      "Requirement already satisfied: portalocker in /home/petrakov/.local/lib/python3.8/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4c4d5a7) (2.4.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/petrakov/.local/lib/python3.8/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4c4d5a7) (0.8.10)\n",
      "Requirement already satisfied: pycparser in /home/petrakov/.local/lib/python3.8/site-packages (from cffi->fairseq==1.0.0a0+4c4d5a7) (2.21)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/petrakov/.local/lib/python3.8/site-packages (from importlib-resources->hydra-core<1.1->fairseq==1.0.0a0+4c4d5a7) (3.8.0)\n",
      "Installing collected packages: fairseq\n",
      "  Attempting uninstall: fairseq\n",
      "    Found existing installation: fairseq 1.0.0a0+4c4d5a7\n",
      "    Uninstalling fairseq-1.0.0a0+4c4d5a7:\n",
      "      Successfully uninstalled fairseq-1.0.0a0+4c4d5a7\n",
      "  Running setup.py develop for fairseq\n",
      "Successfully installed fairseq\n",
      "/home/petrakov/success/mGENRE_MEL\n"
     ]
    }
   ],
   "source": [
    "#KILT\n",
    "%rm -rf KILT\n",
    "!git clone https://github.com/facebookresearch/KILT.git\n",
    "%cd KILT\n",
    "!pip install -r requirements.txt\n",
    "!python setup.py install\n",
    "sys.path.append(os.getcwd())\n",
    "%cd ..\n",
    "\n",
    "#GENRE\n",
    "%rm -rf GENRE\n",
    "!git clone https://github.com/facebookresearch/GENRE.git\n",
    "%cd GENRE\n",
    "!pip install ./\n",
    "!python ./setup.py build develop install\n",
    "!pip install sentencepiece marisa_trie\n",
    "sys.path.append(os.getcwd())\n",
    "%cd ..\n",
    "\n",
    "# fairseq\n",
    "%rm -rf fairseq\n",
    "!git clone --branch fixing_prefix_allowed_tokens_fn https://github.com/nicola-decao/fairseq\n",
    "%cd fairseq\n",
    "!sed -i -e '26,27d' fairseq/registry.py\n",
    "!pip install --editable ./\n",
    "!python setup.py build develop\n",
    "!python setup.py install\n",
    "sys.path.append(os.getcwd())\n",
    "%cd ..\n",
    "\n",
    "!pip install kilt\n",
    "\n",
    "#####\n",
    "%cd fairseq\n",
    "!pip install --editable ./\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "from wikidata.client import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data and model (comment cell below if already downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-04 20:42:40--  https://dl.fbaipublicfiles.com/GENRE/fairseq_multilingual_entity_disambiguation.tar.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2057741836 (1.9G) [application/gzip]\n",
      "Saving to: ‘fairseq_multilingual_entity_disambiguation.tar.gz’\n",
      "\n",
      "fairseq_multilingua 100%[===================>]   1.92G  20.8MB/s    in 96s     \n",
      "\n",
      "2022-07-04 20:44:16 (20.5 MB/s) - ‘fairseq_multilingual_entity_disambiguation.tar.gz’ saved [2057741836/2057741836]\n",
      "\n",
      "fairseq_multilingual_entity_disambiguation/\n",
      "fairseq_multilingual_entity_disambiguation/model.pt\n",
      "fairseq_multilingual_entity_disambiguation/dict.source.txt\n",
      "fairseq_multilingual_entity_disambiguation/dict.target.txt\n",
      "fairseq_multilingual_entity_disambiguation/spm_256000.model\n",
      "fairseq_multilingual_entity_disambiguation/spm_256000.vocab\n",
      "--2022-07-04 20:45:05--  https://dl.fbaipublicfiles.com/GENRE/lang_title2wikidataID-normalized_with_redirect.pkl\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3881415585 (3.6G) [application/octet-stream]\n",
      "Saving to: ‘lang_title2wikidataID-normalized_with_redirect.pkl’\n",
      "\n",
      "lang_title2wikidata 100%[===================>]   3.61G  23.2MB/s    in 2m 44s  \n",
      "\n",
      "2022-07-04 20:47:51 (22.5 MB/s) - ‘lang_title2wikidataID-normalized_with_redirect.pkl’ saved [3881415585/3881415585]\n",
      "\n",
      "--2022-07-04 20:47:51--  http://dl.fbaipublicfiles.com/GENRE/titles_lang_all105_marisa_trie_with_redirect.pkl\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 581590386 (555M) [application/octet-stream]\n",
      "Saving to: ‘titles_lang_all105_marisa_trie_with_redirect.pkl’\n",
      "\n",
      "titles_lang_all105_ 100%[===================>] 554.65M  23.1MB/s    in 25s     \n",
      "\n",
      "2022-07-04 20:48:17 (21.9 MB/s) - ‘titles_lang_all105_marisa_trie_with_redirect.pkl’ saved [581590386/581590386]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pretrained model\n",
    "! rm -rf fairseq_multilingual_entity_disambiguation*\n",
    "! wget https://dl.fbaipublicfiles.com/GENRE/fairseq_multilingual_entity_disambiguation.tar.gz\n",
    "! tar -xvf fairseq_multilingual_entity_disambiguation.tar.gz\n",
    "\n",
    "# data\n",
    "! wget https://dl.fbaipublicfiles.com/GENRE/lang_title2wikidataID-normalized_with_redirect.pkl\n",
    "! wget http://dl.fbaipublicfiles.com/GENRE/titles_lang_all105_marisa_trie_with_redirect.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1QpLVXGahVjAoEq8TrE70aKtMLJCNeASh\n",
      "To: /home/petrakov/success/mGENRE_MEL/mentions_test.json\n",
      "100%|██████████| 65.1M/65.1M [00:00<00:00, 71.7MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mentions_test.json'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_ = \"1QpLVXGahVjAoEq8TrE70aKtMLJCNeASh\"\n",
    "gdown.download(id=id_, output=\"mentions_test.json\", quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mentions_test.json\") as f:\n",
    "    test_set = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In original version there were serious of troubles that's why correct version we add here from external google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1EfRYS0V3C0K7u_4SuP6DcEi31T_7dpq_\n",
      "To: /home/petrakov/success/mGENRE_MEL/fairseq_model.py\n",
      "100%|██████████| 5.10k/5.10k [00:00<00:00, 2.32MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fairseq_model.py'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id__ = \"1EfRYS0V3C0K7u_4SuP6DcEi31T_7dpq_\"\n",
    "gdown.download(id=id__, output=\"fairseq_model.py\", quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mv fairseq_model.py GENRE/genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Autoregressive Entity Linking\n",
    "\n",
    "![](s.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are starting with importing our model mGENRE and it's important part, it was slightly changed in previous cells in order to reach stable work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XURCH8bMyn2S"
   },
   "outputs": [],
   "source": [
    "from GENRE.genre.trie import Trie, MarisaTrie\n",
    "from GENRE.genre.fairseq_model import mGENRE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load data that connects titles with wikidata ids and trie that is an important part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lang_title2wikidataID-normalized_with_redirect.pkl\", \"rb\") as f:\n",
    "    lang_title2wikidataID = pickle.load(f)\n",
    "  \n",
    "# with open(\"titles_lang_all105_trie_with_redirect.pkl\", \"rb\") as f:\n",
    "#     trie = Trie.load_from_dict(pickle.load(f))\n",
    "\n",
    "with open(\"titles_lang_all105_marisa_trie_with_redirect.pkl\", \"rb\") as f:\n",
    "    trie = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPU leads to faster inference, so if possible it is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we chosen 4 because there were lot's of free memory on our own server, you can check the memory available for you using the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver version: \u001b[1m495.29.05\u001b[m\r\n",
      "------------------- \u001b[1mDevice 0\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 3090\u001b[m\r\n",
      "Memory usage: 22897MiB / 24268MiB\r\n",
      "Temperature: \u001b[92m47C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n",
      "------------------- \u001b[1mDevice 1\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 3090\u001b[m\r\n",
      "Memory usage: 18743MiB / 24268MiB\r\n",
      "Temperature: \u001b[92m50C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n",
      "------------------- \u001b[1mDevice 2\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 3090\u001b[m\r\n",
      "Memory usage: 16130MiB / 24268MiB\r\n",
      "Temperature: \u001b[92m45C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n",
      "------------------- \u001b[1mDevice 3\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 3090\u001b[m\r\n",
      "Memory usage: 18743MiB / 24268MiB\r\n",
      "Temperature: \u001b[92m52C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n",
      "------------------- \u001b[1mDevice 4\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 2080 Ti\u001b[m\r\n",
      "Memory usage: 10549MiB / 11019MiB\r\n",
      "Temperature: \u001b[92m27C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n",
      "------------------- \u001b[1mDevice 5\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 2080 Ti\u001b[m\r\n",
      "Memory usage:  8127MiB / 11019MiB\r\n",
      "Temperature: \u001b[92m74C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-cdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"#torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Oruf5Ng61T33",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GENREHubInterface(\n",
       "  (models): ModuleList(\n",
       "    (0): BARTModel(\n",
       "      (encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): TransformerDecoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (output_projection): Linear(in_features=1024, out_features=256001, bias=False)\n",
       "      )\n",
       "      (classification_heads): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (model): BARTModel(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (output_projection): Linear(in_features=1024, out_features=256001, bias=False)\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate Wikipedia titles and language IDs\n",
    "model = mGENRE.from_pretrained(\"fairseq_multilingual_entity_disambiguation\").eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petrakov/success/mGENRE_MEL/fairseq/fairseq/search.py:205: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  beams_buf = indices_buf // vocab_size\n",
      "/home/petrakov/success/mGENRE_MEL/fairseq/fairseq/sequence_generator.py:659: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  unfin_idx = idx // beam_size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'id': 'Q937',\n",
       "   'texts': ['Albert Einstein >> en'],\n",
       "   'scores': tensor([-0.7139]),\n",
       "   'score': tensor(-1.5963)},\n",
       "  {'id': 'Q363005',\n",
       "   'texts': ['John Singleton >> en'],\n",
       "   'scores': tensor([-0.7911]),\n",
       "   'score': tensor(-1.9379)},\n",
       "  {'id': 'Q7374',\n",
       "   'texts': ['Alfred Hitchcock >> en'],\n",
       "   'scores': tensor([-0.8763]),\n",
       "   'score': tensor(-2.3185)},\n",
       "  {'id': 'Q715110',\n",
       "   'texts': ['Robert Maynard >> en'],\n",
       "   'scores': tensor([-0.8955]),\n",
       "   'score': tensor(-2.3693)},\n",
       "  {'id': 'Q9353',\n",
       "   'texts': ['John Locke >> en'],\n",
       "   'scores': tensor([-1.0061]),\n",
       "   'score': tensor(-2.4643)}]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"[START] The founder of the theory of relativity [END] received the Nobel Prize.\"]\n",
    "model.sample(\n",
    "    sentences,\n",
    "    beam = 5,\n",
    "    prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
    "        e for e in trie.get(sent.tolist())\n",
    "        if e < len(model.task.target_dictionary)\n",
    "    ],\n",
    "    text_to_id=lambda x: max(lang_title2wikidataID[tuple(reversed(x.split(\" >> \")))], key=lambda y: int(y[1:])),\n",
    "    marginalize=True,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results above demonstrate that each *text* has it's own id and via marginalization procedure we have unique score for predicted entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New experiment GENRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Data and dependencies\n",
    "\n",
    "# id_p_e_m = \"1C2R814tsgZbREaaOk6o9nh3lQn308Wo7\"\n",
    "# gdown.download(id=id_p_e_m, output=\"prob_yago_crosswikis_wikipedia_p_e_m.txt\", quiet=False)\n",
    "\n",
    "# %mkdir data\n",
    "# %cd data\n",
    "# %mkdir dalab\n",
    "# %cd ..\n",
    "\n",
    "# %mv prob_yago_crosswikis_wikipedia_p_e_m.txt data/dalab/prob_yago_crosswikis_wikipedia_p_e_m.txt\n",
    "\n",
    "# ! wget http://resources.mpi-inf.mpg.de/yago-naga/aida/download/aida_means.tsv.bz2\n",
    "# ! bzip2 -dk aida_means.tsv.bz2\n",
    "\n",
    "# %cd data \n",
    "# %mkdir aida\n",
    "# %cd ..\n",
    "\n",
    "# %mv aida_means.tsv data/aida/aida_means.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_dalab_candidates():\n",
    "#     for line in open(\"data/dalab/prob_yago_crosswikis_wikipedia_p_e_m.txt\"):\n",
    "#         line = line[:-1]\n",
    "#         columns = line.split(\"\\t\")\n",
    "#         mention = columns[0]\n",
    "#         for column in columns[2:]:\n",
    "#             if len(column.strip()) == 0:\n",
    "#                 continue\n",
    "#             values = column.split(\",\")\n",
    "#             candidate = \",\".join(values[2:])\n",
    "#             candidate = candidate.replace(\"_\", \" \")\n",
    "#             yield mention, candidate\n",
    "\n",
    "\n",
    "# def hex2int(hexa: str) -> int:\n",
    "#     return int(hexa, 16)\n",
    "\n",
    "\n",
    "# def replace_unicode(u_str):\n",
    "#     matches = set(re.findall(\"\\\\\\\\u....\", u_str))\n",
    "#     for match in matches:\n",
    "#         u_str = u_str.replace(match, chr(hex2int(match[2:])))\n",
    "#     return u_str\n",
    "\n",
    "\n",
    "# PUNCTUATION_CHARS = set(string.punctuation)\n",
    "\n",
    "\n",
    "# def filter_mention(mention):\n",
    "#     if mention[0].islower():\n",
    "#         return True\n",
    "#     if mention[0] in PUNCTUATION_CHARS:\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "\n",
    "# def read_aida_candidates():\n",
    "#     for line in open(\"data/aida/aida_means.tsv\"):\n",
    "#         line = line[:-1]\n",
    "#         values = line.split(\"\\t\")\n",
    "#         mention = replace_unicode(values[0][1:-1])\n",
    "#         candidate = replace_unicode(values[1]).replace(\"_\", \" \")\n",
    "#         yield mention, candidate\n",
    "\n",
    "\n",
    "# #making mention_candidates_dict\n",
    "# #once done no need to do again\n",
    "\n",
    "# mention_candidates_dict = {}\n",
    "# for mention, candidate in itertools.chain(read_dalab_candidates(), read_aida_candidates()):\n",
    "#     if filter_mention(mention):\n",
    "#         continue\n",
    "#     if mention not in mention_candidates_dict:\n",
    "#         mention_candidates_dict[mention] = set()\n",
    "#     mention_candidates_dict[mention].add(candidate)\n",
    "# for mention in mention_candidates_dict:\n",
    "#     mention_candidates_dict[mention] = sorted(mention_candidates_dict[mention])\n",
    "# with open(\"data/mention_candidates_dict.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(mention_candidates_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # making mention_trie\n",
    "# # once done no need to do again\n",
    "\n",
    "# sys.setrecursionlimit(10000)\n",
    "# model_path = \"fairseq_e2e_entity_linking_wiki_abs\"\n",
    "# model = GENRE.from_pretrained(model_path).eval()\n",
    "# with open(\"data/mention_candidates_dict.pkl\", \"rb\") as f:\n",
    "#     mention_to_candidates_dict = pickle.load(f)\n",
    "# mention_trie = Trie()\n",
    "# for mention in tqdm(mention_to_candidates_dict):\n",
    "#     encoded = model.encode(\" {}\".format(mention))[1:].tolist()\n",
    "#     mention_trie.add(encoded)\n",
    "# out_file = \"data/mention_trie.pkl\"\n",
    "# with open(out_file, \"wb\") as f:\n",
    "#     pickle.dump(mention_trie, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example from git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from GENRE.genre.utils import get_entity_spans_fairseq as get_entity_spans\n",
    "from GENRE.genre.utils import get_markdown\n",
    "\n",
    "\n",
    "from GENRE.genre.trie import Trie, MarisaTrie\n",
    "from GENRE.genre.fairseq_model import mGENRE\n",
    "from GENRE.genre.fairseq_model import GENRE\n",
    "\n",
    "\n",
    "model_path_genre = \"fairseq_e2e_entity_linking_wiki_abs\"\n",
    "model_path_mgenre = \"fairseq_multilingual_entity_disambiguation\"\n",
    "dict_path = \"data/mention_candidates_dict.pkl\"\n",
    "trie_path = \"data/mention_trie.pkl\"\n",
    "\n",
    "model = GENRE.from_pretrained(model_path_genre).eval()\n",
    "\n",
    "with open(trie_path, \"rb\") as f:\n",
    "    mention_trie = pickle.load(f)\n",
    "with open(dict_path, \"rb\") as f:\n",
    "    mention_to_candidates_dict = pickle.load(f)\n",
    "\n",
    "text = \"\"\"Home Depot CEO Nardelli quits Home-improvement retailer's chief executive had been criticized over pay ATLANTA - Bob Nardelli abruptly resigned Wednesday as chairman and chief executive of The Home Depot Inc. after a six-year tenure that saw the world’s largest home improvement store chain post big profits but left investors disheartened by poor stock performance. Nardelli has also been under fire by investors for his hefty pay and is leaving with a severance package valued at about $210 million. He became CEO in December 2000 after being passed over for the top job at General Electric Co., where Nardelli had been a senior executive. Home Depot said Nardelli was being replaced by Frank Blake, its vice chairman, effective immediately. Blake’s appointment is permanent, Home Depot spokesman Jerry Shields said. What he will be paid was not immediately disclosed, Shields said. The company declined to make Blake available for comment, and a message left for Nardelli with his secretary was not immediately returned. Before Wednesday’s news, Home Depot’s stock had been down more than 3 percent on a split-adjusted basis since Nardelli took over. Nardelli’s sudden departure was stunning in that he told The Associated Press as recently as Sept. 1 that he had no intention of leaving, and a key director also said that the board was pleased with Nardelli despite the uproar by some investors. Asked in that interview if he had thought of hanging up his orange apron and leaving Home Depot, Nardelli said unequivocally that he hadn’t. Asked what he thought he would be doing 10 years from now, Nardelli said, “Selling hammers.” For The Home Depot? “Absolutely,” he said at the time. Home Depot said Nardelli’s decision to resign was by mutual agreement with the Atlanta-based company. “We are very grateful to Bob for his strong leadership of The Home Depot over the past six years. Under Bob’s tenure, the company made significant and necessary investments that greatly improved the company’s infrastructure and operations, expanded our markets to include wholesale distribution and new geographies, and undertook key strategic initiatives to strengthen the company’s foundation for the future,” Home Depot’s board said in a statement. Nardelli was a nuts-and-bolts leader, a former college football player and friend of President Bush. He helped increase revenue and profits at Home Depot and increase the number of stores the company operates to more than 2,000. Home Depot’s earnings per share have increased by approximately 150 percent over the last five years.\"\"\"\n",
    "\n",
    "sentences = [text]\n",
    "entity_spans = get_entity_spans(\n",
    "    model,\n",
    "    sentences,\n",
    "    mention_trie=mention_trie,\n",
    "    mention_to_candidates_dict=mention_to_candidates_dict\n",
    ")\n",
    "markdown = get_markdown(sentences, entity_spans)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot) CEO Nardelli quits Home-improvement retailer's chief executive had been criticized over pay ATLANTA - [Bob Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) abruptly resigned Wednesday as chairman and chief executive of [The Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot) Inc. after a six-year tenure that saw the world’s largest home improvement store chain post big profits but left investors disheartened by poor stock performance. [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) has also been under fire by investors for his hefty pay and is leaving with a severance package valued at about $210 million. He became CEO in December 2000 after being passed over for the top job at [General Electric](https://en.wikipedia.org/wiki/General_Electric) Co., where [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) had been a senior executive. [Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot) said [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) was being replaced by [Frank Blake](https://en.wikipedia.org/wiki/Frank_Blake), its vice chairman, effective immediately. [Blake](https://en.wikipedia.org/wiki/Frank_Blake)’s appointment is permanent, [Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot) spokesman [Jerry Shields](https://en.wikipedia.org/wiki/Jerry_A._Shields) said. [What](https://en.wikipedia.org/wiki/What?_(film)) he will be paid was not immediately disclosed, [Shields](https://en.wikipedia.org/wiki/Jerry_A._Shields) said. [The](https://en.wikipedia.org/wiki/The_Home_Depot) company declined to make [Blake](https://en.wikipedia.org/wiki/Frank_Blake) available for comment, and a message left for [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) with his secretary was not immediately returned. [Before](https://en.wikipedia.org/wiki/Before_(song)) Wednesday’s news, [Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot)’s stock had been down more than 3 percent on a split-adjusted basis since [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) took over. [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli)’s sudden departure was stunning in that he told [The](https://en.wikipedia.org/wiki/The_Home_Depot) Associated Press as recently as Sept. 1 that he had no intention of leaving, and a key director also said that the board was pleased with [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) despite the uproar by some investors. Asked in that interview if he had thought of hanging up his orange apron and leaving [Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot), [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) said unequivocally that he hadn’t. Asked what he thought he would be doing [10](https://en.wikipedia.org/wiki/10_(film)) years from now, [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) said, “Selling hammers.” For [The](https://en.wikipedia.org/wiki/The_Mr._T_Experience) Home Depot? “Absolutely,” he said at the time. [Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot) said [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli)’s decision to resign was by mutual agreement with the [Atlanta](https://en.wikipedia.org/wiki/Decatur,_Georgia)-based company. “We are very grateful to [Bob](https://en.wikipedia.org/wiki/Barbecue_Bob) for his strong leadership of [The](https://en.wikipedia.org/wiki/U.S._Department_of_State_Foreign_Affairs_Manual) [Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot) over the past six years. Under [Bob](https://en.wikipedia.org/wiki/Bank_of_Baroda)’s tenure, the company made significant and necessary investments that greatly improved the company’s infrastructure and operations, expanded our markets to include wholesale distribution and new geographies, and undertook key strategic initiatives to strengthen the company’s foundation for the future,” [Home](https://en.wikipedia.org/wiki/John_Home) [Depot](https://en.wikipedia.org/wiki/Depot)’s board said in a statement. [Nardelli](https://en.wikipedia.org/wiki/Nardelli) was a nuts-and-bolts leader, a former college football player and friend of [President](https://en.wikipedia.org/wiki/Senegalese_presidential_election,_2007) Bush. [He](https://en.wikipedia.org/wiki/J._M._E._McTaggart) helped increase revenue and profits at [Home](https://en.wikipedia.org/wiki/New_Recreation_Ground) [Depot](https://en.wikipedia.org/wiki/Depot) and increase the number of stores the company operates to more than [2](https://en.wikipedia.org/wiki/U.S._Route_2),000. [Home](https://en.wikipedia.org/wiki/New_Recreation_Ground) [Depot](https://en.wikipedia.org/wiki/Depot)’s earnings per share have increased by approximately [150](https://en.wikipedia.org/wiki/U.S._Route_150) percent over the last five years."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {\n",
    "    \"id_0\": \"In 1921, Einstein received a Nobel Prize.\",\n",
    "    \"id_1\": \"Armstrong was the first man on the Moon.\",\n",
    "}\n",
    "\n",
    "gold_entities = [\n",
    "    (\"id_0\", 3, 4, \"1921\"),\n",
    "    (\"id_0\", 9, 8, 'Albert_Einstein'),\n",
    "    (\"id_0\", 29, 11, 'Nobel_Prize_in_Physics'),\n",
    "    (\"id_1\", 0, 9, 'Neil_Armstrong'),\n",
    "    (\"id_1\", 35, 4, 'Moon'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guess_entities [[(0, 7, 'List_of_Nobel_laureates_by_year_of_appointment'), (9, 8, 'Albert_Einstein'), (29, 11, 'Nobel_Prize_in_Physiology_or_Medicine'), (40, 1, 'List_of_Nobel_laureates_in_Physiology_or_Medicine_by_year_of_appointment')], [(18, 9, 'First_Man_(film)'), (35, 4, 'Moon_(TV_series)_(1968_TV_series,_season_1)'), (39, 1, 'Moon_(TV_series,_season_1)')]]\n",
      "#############\n",
      "guess_entities [('id_0', 0, 7, 'List_of_Nobel_laureates_by_year_of_appointment'), ('id_0', 9, 8, 'Albert_Einstein'), ('id_0', 29, 11, 'Nobel_Prize_in_Physiology_or_Medicine'), ('id_0', 40, 1, 'List_of_Nobel_laureates_in_Physiology_or_Medicine_by_year_of_appointment'), ('id_1', 18, 9, 'First_Man_(film)'), ('id_1', 35, 4, 'Moon_(TV_series)_(1968_TV_series,_season_1)'), ('id_1', 39, 1, 'Moon_(TV_series,_season_1)')]\n"
     ]
    }
   ],
   "source": [
    "guess_entities = get_entity_spans(\n",
    "    model,\n",
    "    list(documents.values()),\n",
    ")\n",
    "print(\"guess_entities\", guess_entities)\n",
    "print(\"#############\")\n",
    "\n",
    "guess_entities = [\n",
    "    (k,) + x\n",
    "    for k, e in zip(documents.keys(), guess_entities)\n",
    "    for x in e\n",
    "]\n",
    "print(\"guess_entities\", guess_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_p=0.1429 micro_r=0.2000, micro_f1=0.1667, macro_p=0.1250, macro_r=0.1667, macro_f1=0.1429\n"
     ]
    }
   ],
   "source": [
    "from GENRE.genre.utils import (\n",
    "    get_micro_precision,\n",
    "    get_micro_recall,\n",
    "    get_micro_f1,\n",
    "    get_macro_precision,\n",
    "    get_macro_recall,\n",
    "    get_macro_f1,\n",
    ")\n",
    "\n",
    "micro_p = get_micro_precision(guess_entities, gold_entities)\n",
    "micro_r = get_micro_recall(guess_entities, gold_entities)\n",
    "micro_f1 = get_micro_f1(guess_entities, gold_entities)\n",
    "macro_p = get_macro_precision(guess_entities, gold_entities)\n",
    "macro_r = get_macro_recall(guess_entities, gold_entities)\n",
    "macro_f1 = get_macro_f1(guess_entities, gold_entities)\n",
    "\n",
    "print(\n",
    "   \"micro_p={:.4f} micro_r={:.4f}, micro_f1={:.4f}, macro_p={:.4f}, macro_r={:.4f}, macro_f1={:.4f}\".format(\n",
    "       micro_p, micro_r, micro_f1, macro_p, macro_r, macro_f1\n",
    "   )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load mewsli-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;31maida_means.tsv.bz2\u001b[0m\r\n",
      "\u001b[01;34mdata\u001b[0m/\r\n",
      "\u001b[01;34mdataset\u001b[0m/\r\n",
      "docs.tsv\r\n",
      "\u001b[01;34mfairseq\u001b[0m/\r\n",
      "\u001b[01;34mfairseq_e2e_entity_linking_wiki_abs\u001b[0m/\r\n",
      "\u001b[01;34mfairseq_multilingual_entity_disambiguation\u001b[0m/\r\n",
      "\u001b[01;31mfairseq_multilingual_entity_disambiguation.tar.gz\u001b[0m\r\n",
      "\u001b[01;34mGENRE\u001b[0m/\r\n",
      "\u001b[01;34mKILT\u001b[0m/\r\n",
      "lang_title2wikidataID-normalized_with_redirect.pkl\r\n",
      "mentions_test.json\r\n",
      "mentions.tsv\r\n",
      "\u001b[01;34mmewsli_9\u001b[0m/\r\n",
      "mgenre_final.ipynb\r\n",
      "mgenre_final.ipynb.invalid\r\n",
      "README.md\r\n",
      "requirements.txt\r\n",
      "titles_lang_all105_marisa_trie_with_redirect.pkl\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_df = pd.read_csv('dataset/en/mentions.tsv', sep='\\t')\n",
    "#en_df_doc = pd.read_csv('docs.tsv', sep='\\t')\n",
    "en_df_doc = pd.read_csv('mewsli_9/dense_representations_for_entity_retrieval/mel/mewsli-9/output/dataset/en/docs.tsv', sep='\\t')\n",
    "en_df_men = pd.read_csv('mewsli_9/dense_representations_for_entity_retrieval/mel/mewsli-9/output/dataset/en/mentions.tsv', sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>title</th>\n",
       "      <th>curid</th>\n",
       "      <th>revid</th>\n",
       "      <th>url</th>\n",
       "      <th>text_md5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en-106602</td>\n",
       "      <td>NOAA says Earth's oceans becoming more acidic</td>\n",
       "      <td>106602</td>\n",
       "      <td>1986523</td>\n",
       "      <td>https://en.wikinews.org/wiki?curid=106602</td>\n",
       "      <td>a504b8679bd5f6eeae93536d2b3bb6d6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en-106608</td>\n",
       "      <td>Two men fined over 2006 German train crash tha...</td>\n",
       "      <td>106608</td>\n",
       "      <td>1100545</td>\n",
       "      <td>https://en.wikinews.org/wiki?curid=106608</td>\n",
       "      <td>b183ca9d8392849395cd56e37c2a5cb4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en-106610</td>\n",
       "      <td>New law to help asbestos sufferers in Victoria...</td>\n",
       "      <td>106610</td>\n",
       "      <td>4360069</td>\n",
       "      <td>https://en.wikinews.org/wiki?curid=106610</td>\n",
       "      <td>86b30cb07eea2779541562c54eb9261e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en-106632</td>\n",
       "      <td>President Bush to meet with LDS Church leaders...</td>\n",
       "      <td>106632</td>\n",
       "      <td>1410456</td>\n",
       "      <td>https://en.wikinews.org/wiki?curid=106632</td>\n",
       "      <td>b5eefea779c04081602521049e579b9f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en-106644</td>\n",
       "      <td>Morgan Tsvangirai returns to Zimbabwe</td>\n",
       "      <td>106644</td>\n",
       "      <td>4366347</td>\n",
       "      <td>https://en.wikinews.org/wiki?curid=106644</td>\n",
       "      <td>7aead039ec500822f9a7c4eeae120963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12674</th>\n",
       "      <td>en-82692</td>\n",
       "      <td>Fußball-Bundesliga 2007–08: Borussia Dortmund ...</td>\n",
       "      <td>82692</td>\n",
       "      <td>4402330</td>\n",
       "      <td>https://en.wikinews.org/wiki?curid=82692</td>\n",
       "      <td>cf7b4f3abfca4b2d26fab8e86aec2e20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12675</th>\n",
       "      <td>en-82698</td>\n",
       "      <td>Blast kills 8, injures 126 at a mall in Manila...</td>\n",
       "      <td>82698</td>\n",
       "      <td>1628197</td>\n",
       "      <td>https://en.wikinews.org/wiki?curid=82698</td>\n",
       "      <td>019d12477b221f7b78bb839f83f58733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12676</th>\n",
       "      <td>en-82700</td>\n",
       "      <td>USPTO partially confirms validity of Amazon \"1...</td>\n",
       "      <td>82700</td>\n",
       "      <td>2470912</td>\n",
       "      <td>https://en.wikinews.org/wiki?curid=82700</td>\n",
       "      <td>fbce4beee7f28115a0811fe8fead83f2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12677</th>\n",
       "      <td>en-82707</td>\n",
       "      <td>Ben Cousins sacked by West Coast in AFL drug s...</td>\n",
       "      <td>82707</td>\n",
       "      <td>2515447</td>\n",
       "      <td>https://en.wikinews.org/wiki?curid=82707</td>\n",
       "      <td>8e2efe3f69fb5981a89b8e30bac51d40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12678</th>\n",
       "      <td>en-82713</td>\n",
       "      <td>South African reggae star Lucky Dube shot dead...</td>\n",
       "      <td>82713</td>\n",
       "      <td>2542116</td>\n",
       "      <td>https://en.wikinews.org/wiki?curid=82713</td>\n",
       "      <td>879cc078736f6abcef9b86d9c5f2e3c2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12679 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           docid                                              title   curid  \\\n",
       "0      en-106602      NOAA says Earth's oceans becoming more acidic  106602   \n",
       "1      en-106608  Two men fined over 2006 German train crash tha...  106608   \n",
       "2      en-106610  New law to help asbestos sufferers in Victoria...  106610   \n",
       "3      en-106632  President Bush to meet with LDS Church leaders...  106632   \n",
       "4      en-106644              Morgan Tsvangirai returns to Zimbabwe  106644   \n",
       "...          ...                                                ...     ...   \n",
       "12674   en-82692  Fußball-Bundesliga 2007–08: Borussia Dortmund ...   82692   \n",
       "12675   en-82698  Blast kills 8, injures 126 at a mall in Manila...   82698   \n",
       "12676   en-82700  USPTO partially confirms validity of Amazon \"1...   82700   \n",
       "12677   en-82707  Ben Cousins sacked by West Coast in AFL drug s...   82707   \n",
       "12678   en-82713  South African reggae star Lucky Dube shot dead...   82713   \n",
       "\n",
       "         revid                                        url  \\\n",
       "0      1986523  https://en.wikinews.org/wiki?curid=106602   \n",
       "1      1100545  https://en.wikinews.org/wiki?curid=106608   \n",
       "2      4360069  https://en.wikinews.org/wiki?curid=106610   \n",
       "3      1410456  https://en.wikinews.org/wiki?curid=106632   \n",
       "4      4366347  https://en.wikinews.org/wiki?curid=106644   \n",
       "...        ...                                        ...   \n",
       "12674  4402330   https://en.wikinews.org/wiki?curid=82692   \n",
       "12675  1628197   https://en.wikinews.org/wiki?curid=82698   \n",
       "12676  2470912   https://en.wikinews.org/wiki?curid=82700   \n",
       "12677  2515447   https://en.wikinews.org/wiki?curid=82707   \n",
       "12678  2542116   https://en.wikinews.org/wiki?curid=82713   \n",
       "\n",
       "                               text_md5  \n",
       "0      a504b8679bd5f6eeae93536d2b3bb6d6  \n",
       "1      b183ca9d8392849395cd56e37c2a5cb4  \n",
       "2      86b30cb07eea2779541562c54eb9261e  \n",
       "3      b5eefea779c04081602521049e579b9f  \n",
       "4      7aead039ec500822f9a7c4eeae120963  \n",
       "...                                 ...  \n",
       "12674  cf7b4f3abfca4b2d26fab8e86aec2e20  \n",
       "12675  019d12477b221f7b78bb839f83f58733  \n",
       "12676  fbce4beee7f28115a0811fe8fead83f2  \n",
       "12677  8e2efe3f69fb5981a89b8e30bac51d40  \n",
       "12678  879cc078736f6abcef9b86d9c5f2e3c2  \n",
       "\n",
       "[12679 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_df_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>position</th>\n",
       "      <th>length</th>\n",
       "      <th>mention</th>\n",
       "      <th>url</th>\n",
       "      <th>lang</th>\n",
       "      <th>qid</th>\n",
       "      <th>qid_in_refs</th>\n",
       "      <th>freq_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en-106602</td>\n",
       "      <td>145</td>\n",
       "      <td>39</td>\n",
       "      <td>Pacific Marine Environmental Laboratory</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Pacific_Marine_En...</td>\n",
       "      <td>en</td>\n",
       "      <td>Q7122548</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en-106602</td>\n",
       "      <td>365</td>\n",
       "      <td>15</td>\n",
       "      <td>Baja California</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Baja_California</td>\n",
       "      <td>en</td>\n",
       "      <td>Q58731</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en-106602</td>\n",
       "      <td>1013</td>\n",
       "      <td>13</td>\n",
       "      <td>oceanographer</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Oceanography</td>\n",
       "      <td>en</td>\n",
       "      <td>Q43518</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en-106602</td>\n",
       "      <td>1109</td>\n",
       "      <td>5</td>\n",
       "      <td>Earth</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Earth</td>\n",
       "      <td>en</td>\n",
       "      <td>Q2</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en-106602</td>\n",
       "      <td>1150</td>\n",
       "      <td>14</td>\n",
       "      <td>carbon dioxide</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Carbon_dioxide</td>\n",
       "      <td>en</td>\n",
       "      <td>Q1997</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80237</th>\n",
       "      <td>en-82707</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>AFL</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Australian_Footba...</td>\n",
       "      <td>en</td>\n",
       "      <td>Q50783</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80238</th>\n",
       "      <td>en-82707</td>\n",
       "      <td>501</td>\n",
       "      <td>6</td>\n",
       "      <td>Valium</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Valium</td>\n",
       "      <td>en</td>\n",
       "      <td>Q210402</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80239</th>\n",
       "      <td>en-82713</td>\n",
       "      <td>161</td>\n",
       "      <td>6</td>\n",
       "      <td>reggae</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Reggae</td>\n",
       "      <td>en</td>\n",
       "      <td>Q9794</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80240</th>\n",
       "      <td>en-82713</td>\n",
       "      <td>173</td>\n",
       "      <td>10</td>\n",
       "      <td>Lucky Dube</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Lucky_Dube</td>\n",
       "      <td>en</td>\n",
       "      <td>Q380147</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80241</th>\n",
       "      <td>en-82713</td>\n",
       "      <td>858</td>\n",
       "      <td>10</td>\n",
       "      <td>Bob Marley</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Bob_Marley</td>\n",
       "      <td>en</td>\n",
       "      <td>Q409</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80242 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           docid  position  length                                  mention  \\\n",
       "0      en-106602       145      39  Pacific Marine Environmental Laboratory   \n",
       "1      en-106602       365      15                          Baja California   \n",
       "2      en-106602      1013      13                            oceanographer   \n",
       "3      en-106602      1109       5                                    Earth   \n",
       "4      en-106602      1150      14                           carbon dioxide   \n",
       "...          ...       ...     ...                                      ...   \n",
       "80237   en-82707        99       3                                      AFL   \n",
       "80238   en-82707       501       6                                   Valium   \n",
       "80239   en-82713       161       6                                   reggae   \n",
       "80240   en-82713       173      10                               Lucky Dube   \n",
       "80241   en-82713       858      10                               Bob Marley   \n",
       "\n",
       "                                                     url lang       qid  \\\n",
       "0      http://en.wikipedia.org/wiki/Pacific_Marine_En...   en  Q7122548   \n",
       "1           http://en.wikipedia.org/wiki/Baja_California   en    Q58731   \n",
       "2              http://en.wikipedia.org/wiki/Oceanography   en    Q43518   \n",
       "3                     http://en.wikipedia.org/wiki/Earth   en        Q2   \n",
       "4            http://en.wikipedia.org/wiki/Carbon_dioxide   en     Q1997   \n",
       "...                                                  ...  ...       ...   \n",
       "80237  http://en.wikipedia.org/wiki/Australian_Footba...   en    Q50783   \n",
       "80238                http://en.wikipedia.org/wiki/Valium   en   Q210402   \n",
       "80239                http://en.wikipedia.org/wiki/Reggae   en     Q9794   \n",
       "80240            http://en.wikipedia.org/wiki/Lucky_Dube   en   Q380147   \n",
       "80241            http://en.wikipedia.org/wiki/Bob_Marley   en      Q409   \n",
       "\n",
       "       qid_in_refs  freq_bin  \n",
       "0             True         2  \n",
       "1             True         5  \n",
       "2             True         4  \n",
       "3             True         5  \n",
       "4             True         5  \n",
       "...            ...       ...  \n",
       "80237         True         5  \n",
       "80238         True         4  \n",
       "80239         True         5  \n",
       "80240         True         3  \n",
       "80241         True         4  \n",
       "\n",
       "[80242 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_df_men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/petrakov/success/mGENRE_MEL/mewsli_9/dense_representations_for_entity_retrieval/mel\n"
     ]
    }
   ],
   "source": [
    "%cd mewsli_9/dense_representations_for_entity_retrieval/mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: absl_py>=0.9.0 in /home/petrakov/.local/lib/python3.8/site-packages (from -r wikinews_extractor/requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: bz2file>=0.98 in /home/petrakov/.local/lib/python3.8/site-packages (from -r wikinews_extractor/requirements.txt (line 2)) (0.98)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /home/petrakov/.local/lib/python3.8/site-packages (from -r wikinews_extractor/requirements.txt (line 3)) (1.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/petrakov/.local/lib/python3.8/site-packages (from pandas>=1.0.5->-r wikinews_extractor/requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/petrakov/.local/lib/python3.8/site-packages (from pandas>=1.0.5->-r wikinews_extractor/requirements.txt (line 3)) (1.23.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/petrakov/.local/lib/python3.8/site-packages (from pandas>=1.0.5->-r wikinews_extractor/requirements.txt (line 3)) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.5->-r wikinews_extractor/requirements.txt (line 3)) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r wikinews_extractor/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/petrakov/success/mGENRE_MEL/mewsli_9/dense_representations_for_entity_retrieval/mel\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ dirname mewsli-9/run_parse_wikinews_i18n.sh\n",
      "+ INPUT_BASE_DIR_DEFAULT=mewsli-9/output/wikiextractor\n",
      "+ INPUT_BASE_DIR=mewsli-9/output/wikiextractor\n",
      "++ dirname mewsli-9/run_parse_wikinews_i18n.sh\n",
      "+ OUTPUT_BASE_DIR_DEFAULT=mewsli-9/output/dataset\n",
      "+ OUTPUT_BASE_DIR=mewsli-9/output/dataset\n",
      "+ LANG_LIST=(ar de en es fa ja sr ta tr)\n",
      "+ MODULE=dense_representations_for_entity_retrieval.mel.wikinews_extractor.parse_wikinews_i18n\n",
      "+ for lang in ${LANG_LIST[@]}\n",
      "+ date\n",
      "Tue 05 Jul 2022 12:36:54 AM MSK\n",
      "+ echo '>Parse '\\''ar'\\''...'\n",
      ">Parse 'ar'...\n",
      "+ output_dir=mewsli-9/output/dataset/ar\n",
      "+ mkdir -p mewsli-9/output/dataset/ar\n",
      "+ echo\n",
      "\n",
      "+ for lang in ${LANG_LIST[@]}\n",
      "+ date\n",
      "+ python -m dense_representations_for_entity_retrieval.mel.wikinews_extractor.parse_wikinews_i18n --mode=dataset '--wikinews_archive=mewsli-9/output/wikiextractor/ar/*/wiki_*.bz2' --language=ar --mention_index_path=mewsli-9/output/dataset/ar/mentions.tsv --doc_index_path=mewsli-9/output/dataset/ar/docs.tsv --output_dir_wiki=mewsli-9/output/dataset/ar/wiki --output_dir_text=mewsli-9/output/dataset/ar/text --logtostderr\n",
      "Tue 05 Jul 2022 12:36:54 AM MSK\n",
      "+ echo '>Parse '\\''de'\\''...'\n",
      ">Parse 'de'...\n",
      "+ output_dir=mewsli-9/output/dataset/de\n",
      "+ mkdir -p mewsli-9/output/dataset/de\n",
      "+ echo\n",
      "\n",
      "+ for lang in ${LANG_LIST[@]}\n",
      "+ date\n",
      "+ python -m dense_representations_for_entity_retrieval.mel.wikinews_extractor.parse_wikinews_i18n --mode=dataset '--wikinews_archive=mewsli-9/output/wikiextractor/de/*/wiki_*.bz2' --language=de --mention_index_path=mewsli-9/output/dataset/de/mentions.tsv --doc_index_path=mewsli-9/output/dataset/de/docs.tsv --output_dir_wiki=mewsli-9/output/dataset/de/wiki --output_dir_text=mewsli-9/output/dataset/de/text --logtostderr\n",
      "Tue 05 Jul 2022 12:36:54 AM MSK\n",
      "+ echo '>Parse '\\''en'\\''...'\n",
      ">Parse 'en'...\n",
      "+ output_dir=mewsli-9/output/dataset/en\n",
      "+ mkdir -p mewsli-9/output/dataset/en\n",
      "+ echo\n",
      "\n",
      "+ for lang in ${LANG_LIST[@]}\n",
      "+ date\n",
      "+ python -m dense_representations_for_entity_retrieval.mel.wikinews_extractor.parse_wikinews_i18n --mode=dataset '--wikinews_archive=mewsli-9/output/wikiextractor/en/*/wiki_*.bz2' --language=en --mention_index_path=mewsli-9/output/dataset/en/mentions.tsv --doc_index_path=mewsli-9/output/dataset/en/docs.tsv --output_dir_wiki=mewsli-9/output/dataset/en/wiki --output_dir_text=mewsli-9/output/dataset/en/text --logtostderr\n",
      "Tue 05 Jul 2022 12:36:54 AM MSK\n",
      "+ echo '>Parse '\\''es'\\''...'\n",
      ">Parse 'es'...\n",
      "+ output_dir=mewsli-9/output/dataset/es\n",
      "+ mkdir -p mewsli-9/output/dataset/es\n",
      "+ echo\n",
      "\n",
      "+ for lang in ${LANG_LIST[@]}\n",
      "+ date\n",
      "+ python -m dense_representations_for_entity_retrieval.mel.wikinews_extractor.parse_wikinews_i18n --mode=dataset '--wikinews_archive=mewsli-9/output/wikiextractor/es/*/wiki_*.bz2' --language=es --mention_index_path=mewsli-9/output/dataset/es/mentions.tsv --doc_index_path=mewsli-9/output/dataset/es/docs.tsv --output_dir_wiki=mewsli-9/output/dataset/es/wiki --output_dir_text=mewsli-9/output/dataset/es/text --logtostderr\n",
      "Tue 05 Jul 2022 12:36:54 AM MSK\n",
      "+ echo '>Parse '\\''fa'\\''...'\n",
      ">Parse 'fa'...\n",
      "+ output_dir=mewsli-9/output/dataset/fa\n",
      "+ mkdir -p mewsli-9/output/dataset/fa\n",
      "+ echo\n",
      "\n",
      "+ for lang in ${LANG_LIST[@]}\n",
      "+ date\n",
      "+ python -m dense_representations_for_entity_retrieval.mel.wikinews_extractor.parse_wikinews_i18n --mode=dataset '--wikinews_archive=mewsli-9/output/wikiextractor/fa/*/wiki_*.bz2' --language=fa --mention_index_path=mewsli-9/output/dataset/fa/mentions.tsv --doc_index_path=mewsli-9/output/dataset/fa/docs.tsv --output_dir_wiki=mewsli-9/output/dataset/fa/wiki --output_dir_text=mewsli-9/output/dataset/fa/text --logtostderr\n",
      "Tue 05 Jul 2022 12:36:54 AM MSK\n",
      "+ echo '>Parse '\\''ja'\\''...'\n",
      ">Parse 'ja'...\n",
      "+ output_dir=mewsli-9/output/dataset/ja\n",
      "+ mkdir -p mewsli-9/output/dataset/ja\n",
      "+ echo\n",
      "\n",
      "+ for lang in ${LANG_LIST[@]}\n",
      "+ date\n",
      "+ python -m dense_representations_for_entity_retrieval.mel.wikinews_extractor.parse_wikinews_i18n --mode=dataset '--wikinews_archive=mewsli-9/output/wikiextractor/ja/*/wiki_*.bz2' --language=ja --mention_index_path=mewsli-9/output/dataset/ja/mentions.tsv --doc_index_path=mewsli-9/output/dataset/ja/docs.tsv --output_dir_wiki=mewsli-9/output/dataset/ja/wiki --output_dir_text=mewsli-9/output/dataset/ja/text --logtostderr\n",
      "Tue 05 Jul 2022 12:36:54 AM MSK\n",
      "+ echo '>Parse '\\''sr'\\''...'\n",
      ">Parse 'sr'...\n",
      "+ output_dir=mewsli-9/output/dataset/sr\n",
      "+ mkdir -p mewsli-9/output/dataset/sr\n",
      "+ echo\n",
      "\n",
      "+ for lang in ${LANG_LIST[@]}\n",
      "+ date\n",
      "+ python -m dense_representations_for_entity_retrieval.mel.wikinews_extractor.parse_wikinews_i18n --mode=dataset '--wikinews_archive=mewsli-9/output/wikiextractor/sr/*/wiki_*.bz2' --language=sr --mention_index_path=mewsli-9/output/dataset/sr/mentions.tsv --doc_index_path=mewsli-9/output/dataset/sr/docs.tsv --output_dir_wiki=mewsli-9/output/dataset/sr/wiki --output_dir_text=mewsli-9/output/dataset/sr/text --logtostderr\n",
      "Tue 05 Jul 2022 12:36:54 AM MSK\n",
      "+ echo '>Parse '\\''ta'\\''...'\n",
      ">Parse 'ta'...\n",
      "+ output_dir=mewsli-9/output/dataset/ta\n",
      "+ mkdir -p mewsli-9/output/dataset/ta\n",
      "+ echo\n",
      "\n",
      "+ for lang in ${LANG_LIST[@]}\n",
      "+ date\n",
      "+ python -m dense_representations_for_entity_retrieval.mel.wikinews_extractor.parse_wikinews_i18n --mode=dataset '--wikinews_archive=mewsli-9/output/wikiextractor/ta/*/wiki_*.bz2' --language=ta --mention_index_path=mewsli-9/output/dataset/ta/mentions.tsv --doc_index_path=mewsli-9/output/dataset/ta/docs.tsv --output_dir_wiki=mewsli-9/output/dataset/ta/wiki --output_dir_text=mewsli-9/output/dataset/ta/text --logtostderr\n",
      "Tue 05 Jul 2022 12:36:54 AM MSK\n",
      "+ echo '>Parse '\\''tr'\\''...'\n",
      ">Parse 'tr'...\n",
      "+ output_dir=mewsli-9/output/dataset/tr\n",
      "+ mkdir -p mewsli-9/output/dataset/tr\n",
      "+ echo\n",
      "\n",
      "+ wait\n",
      "+ python -m dense_representations_for_entity_retrieval.mel.wikinews_extractor.parse_wikinews_i18n --mode=dataset '--wikinews_archive=mewsli-9/output/wikiextractor/tr/*/wiki_*.bz2' --language=tr --mention_index_path=mewsli-9/output/dataset/tr/mentions.tsv --doc_index_path=mewsli-9/output/dataset/tr/docs.tsv --output_dir_wiki=mewsli-9/output/dataset/tr/wiki --output_dir_text=mewsli-9/output/dataset/tr/text --logtostderr\n",
      "+ date\n",
      "Tue 05 Jul 2022 12:36:54 AM MSK\n",
      "+ echo '>Done: mewsli-9/output/dataset'\n",
      ">Done: mewsli-9/output/dataset\n"
     ]
    }
   ],
   "source": [
    "!bash mewsli-9/run_parse_wikinews_i18n.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get-mewsli-9.sh  \u001b[0m\u001b[01;34mmewsli-9\u001b[0m/    \u001b[01;31mmewsli-9.zip\u001b[0m  mewsli-x.md  \u001b[01;34mtools\u001b[0m/\r\n",
      "get-mewsli-x.sh  mewsli-9.md  \u001b[01;34mmewsli_x\u001b[0m/     README.md    \u001b[01;34mwikinews_extractor\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./mewsli-9/output/dataset/es/text/es-44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open(\"mewsli-9/output/download/enwikinews-20190101-pages-articles.xml.bz2\", \"rb\") as f:\n",
    "    # Decompress data from file\n",
    "    en_wiki = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-97f7329869c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_wiki\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataFrame constructor not properly called!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;31m# Argument 1 to \"ensure_index\" has incompatible type \"Collection[Any]\";\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(en_wiki.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "XMLSyntaxError",
     "evalue": "internal error: Huge input lookup, line 3616783, column 11 (<string>, line 3616783)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3331\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-86-d3b9f40cdc7e>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    pd.read_xml(\"mewsli-9/output/download/enwikinews-20190101-pages-articles.xml.bz2\")\n",
      "  File \u001b[1;32m\"/home/petrakov/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\"\u001b[0m, line \u001b[1;32m311\u001b[0m, in \u001b[1;35mwrapper\u001b[0m\n    return func(*args, **kwargs)\n",
      "  File \u001b[1;32m\"/home/petrakov/.local/lib/python3.8/site-packages/pandas/io/xml.py\"\u001b[0m, line \u001b[1;32m938\u001b[0m, in \u001b[1;35mread_xml\u001b[0m\n    return _parse(\n",
      "  File \u001b[1;32m\"/home/petrakov/.local/lib/python3.8/site-packages/pandas/io/xml.py\"\u001b[0m, line \u001b[1;32m733\u001b[0m, in \u001b[1;35m_parse\u001b[0m\n    data_dicts = p.parse_data()\n",
      "  File \u001b[1;32m\"/home/petrakov/.local/lib/python3.8/site-packages/pandas/io/xml.py\"\u001b[0m, line \u001b[1;32m389\u001b[0m, in \u001b[1;35mparse_data\u001b[0m\n    self.xml_doc = XML(self._parse_doc(self.path_or_buffer))\n",
      "  File \u001b[1;32m\"/home/petrakov/.local/lib/python3.8/site-packages/pandas/io/xml.py\"\u001b[0m, line \u001b[1;32m554\u001b[0m, in \u001b[1;35m_parse_doc\u001b[0m\n    doc = fromstring(\n",
      "  File \u001b[1;32m\"src/lxml/etree.pyx\"\u001b[0m, line \u001b[1;32m3254\u001b[0m, in \u001b[1;35mlxml.etree.fromstring\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1913\u001b[0m, in \u001b[1;35mlxml.etree._parseMemoryDocument\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1800\u001b[0m, in \u001b[1;35mlxml.etree._parseDoc\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1141\u001b[0m, in \u001b[1;35mlxml.etree._BaseParser._parseDoc\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m615\u001b[0m, in \u001b[1;35mlxml.etree._ParserContext._handleParseResultDoc\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m725\u001b[0m, in \u001b[1;35mlxml.etree._handleParseResult\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"src/lxml/parser.pxi\"\u001b[0;36m, line \u001b[0;32m654\u001b[0;36m, in \u001b[0;35mlxml.etree._raiseParseError\u001b[0;36m\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32m3616783\u001b[0m\n\u001b[0;31mXMLSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m internal error: Huge input lookup, line 3616783, column 11\n"
     ]
    }
   ],
   "source": [
    "pd.read_xml(\"mewsli-9/output/download/enwikinews-20190101-pages-articles.xml.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<wikidata.entity.Entity Q20145 'IU'>,\n",
       " m'Korea  singer,actress record producer (2017)')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client()  # doctest: +SKIP\n",
    "entity = client.get('Q20145', load=True)\n",
    "entity, entity.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we present a function that illustrates predicted entity, text, right answer basing on the id as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_test_set(idx=0):\n",
    "    print(test_set[str(idx)]['mention_its']+\"\\n\")\n",
    "    start = test_set[str(idx)]['start_index']\n",
    "    end = test_set[str(idx)]['end_index']\n",
    "    mention_id = test_set[str(idx)]['mention_id']\n",
    "    text = test_set[str(idx)]['source_document']['text']\n",
    "    text = text[:start]+\"[START] \"+text[start:end]+\" [END]\"+text[end:]\n",
    "    print(f'{text[:start]}{bcolors.BOLD}{text[start:end+len(\"[START] \")+len(\" [END]\")]}{bcolors.END}{text[end+len(\"[START] \")+len(\" [END]\"):]}\\n')\n",
    "    result = model.sample(\n",
    "    sentences=[text],\n",
    "    text_to_id=lambda x: sorted(list(lang_title2wikidataID.get(\n",
    "        tuple(reversed((x.split(\" >> \")[0], x.split(\" >> \")[1][:2]))), [None])))[0],\n",
    "    marginalize=True)\n",
    "    print(result)\n",
    "    \n",
    "    entity = client.get(mention_id, load=True)\n",
    "    print(f'\\nCorrect entity : {mention_id, entity.label, entity.description}\\n')\n",
    "\n",
    "    candidates = [(i['id'], client.get(i['id'], load=True)) for i in result[0]]\n",
    "    print('Predicted entities:')\n",
    "    for i, entity in candidates:\n",
    "        if i is not None:\n",
    "            print(f'{i, entity.label, entity.description}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New South Wales\n",
      "\n",
      "Bathurst Regional Council, the local government responsible for the city of Bathurst and its surrounds in Central Western \u001b[1m[START] New South Wales [END]\u001b[0m, Australia yesterday revealed it had received a development application for the new Bathurst Base Hospital.The new hospital is to be built behind the current hospital on the same site and is expected to cost the New South Wales government AUD96 million. The Bathurst Hospital will be the first in the Bathurst-Orange-Bloomfield redevelopment project.The new hospital will have 149 beds, up from 85 for the current hospital. The hospital will also feature a mental health unit - previously psychiatric patients had to travel to Orange to the Bloomfield Hospital for treatment.The Bathurst Hospital is expected to have state-of-the art facilities and will share some services with the to be constructed Orange Base Hospital.The Bathurst Regional Council has approved the demolition of 12 buildings on the hospital site for enabling works. The hospital site is heritage listed although council decided that as the buildings do not contribute to the streetscape they may be demolished.The demolitions are expected to take place late next month and will take around six weeks to complete. A temporary driveway will then be built to replace the current service entry for food and linen as it will become part of the work site.Upon completion of the new hospital, the current ward block will be demolished leaving the original building from the late 19th century intact. The original building is expected to become an education centre and consulting rooms.The original building was opened in 1834. Since then the facility has undergone numerous upgrades and add-ons, with the present ward block being opened in stages from 1978 to 1982.Other buildings expected to be retained include the Daffodil Cottage (a cancer care centre) and the original Nurse's quarters known as Poole House.\n",
      "\n",
      "[[{'id': 'Q3224', 'texts': ['New South Wales >> en', 'New South Wales >> de'], 'scores': tensor([-0.0940, -1.9361], device='cuda:4'), 'score': tensor(-0.2194, device='cuda:4')}, {'id': None, 'texts': ['New South Welsh English >> en', 'NewSouth Wales >> en'], 'scores': tensor([-1.5938, -1.6026], device='cuda:4'), 'score': tensor(-3.3674, device='cuda:4')}, {'id': 'Q1353', 'texts': ['National Capital Territory >> en'], 'scores': tensor([-1.6513], device='cuda:4'), 'score': tensor(-4.3689, device='cuda:4')}]]\n",
      "\n",
      "Correct entity : ('Q3224', m'New South Wales', m'state of Australia')\n",
      "\n",
      "Predicted entities:\n",
      "('Q3224', m'New South Wales', m'state of Australia')\n",
      "('Q1353', m'Delhi', m'Indian metropolis and union territory that includes New Delhi')\n"
     ]
    }
   ],
   "source": [
    "predict_from_test_set(2354)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results:\n",
    "\n",
    "Language: **fa** (5 out of 61)\n",
    "\n",
    "Micro average (k=1): **1.0**\n",
    "\n",
    "Language: **sr** (105 out of 451)\n",
    "\n",
    "Micro average (k=1): **0.90476**\n",
    "\n",
    "Language: **ta** (25 out of 366)\n",
    "\n",
    "Micro average (k=1): **1.0**\n",
    "\n",
    "### Macro average (k=1): 0.96825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data.data_utils import collate_tokens\n",
    "\n",
    "\n",
    "def compute_results(idx):\n",
    "    start = test_set[str(idx)]['start_index']\n",
    "    end = test_set[str(idx)]['end_index']\n",
    "    mention_id = test_set[str(idx)]['mention_id']\n",
    "    text = test_set[str(idx)]['source_document']['text']\n",
    "    text = text[:start]+\"[START] \"+text[start:end]+\" [END]\"+text[end:]\n",
    "    print(start, end, min(start, len(text)-end))\n",
    "    #encoder = model.encoder(text)\n",
    "    #print(encoder)\n",
    "\n",
    "    result = model.sample(\n",
    "                    sentences=[text],\n",
    "                   text_to_id=lambda x: sorted(list(lang_title2wikidataID.get(\n",
    "       tuple(reversed((x.split(\" >> \")[0], x.split(\" >> \")[1][:2]))), [None])))[0],\n",
    "                    marginalize=True)\n",
    "    candidates = [(i['id'], client.get(i['id'], load=True)) for i in result[0] if i['id'] is not None]\n",
    "    entity = client.get(mention_id, load=True)\n",
    "    return {\n",
    "            \"correct\": mention_id, \n",
    "            \"predicted\": [i for i, _ in candidates]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 109 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'correct': 'Q36678', 'predicted': ['Q36678', 'Q2564150', 'Q7834492']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(lang='en', k_list=[1, 10, 20, 50, 100]):\n",
    "    scores = []\n",
    "    \n",
    "    for idx in tqdm(langs_idx[lang]):\n",
    "        scores.append(compute_results(idx))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_idx = {\n",
    "    'ar':[], 'en':[], 'tr':[]\n",
    "}\n",
    "for idx in test_set:\n",
    "    el = test_set[idx]\n",
    "    lang = el['document_id'][:2]\n",
    "    if lang in langs_idx:\n",
    "        langs_idx[lang].append(int(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model F:\n",
      "on test dataset:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1799 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738 744 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1799 [00:01<51:19,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 357 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1799 [00:04<1:13:30,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1197 1202 205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1799 [00:05<56:34,  1.89s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1434 1453 652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1799 [00:06<43:36,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496 506 320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1799 [00:09<57:57,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351 357 351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1799 [00:12<1:10:23,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 201 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/1799 [00:14<1:08:25,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166 172 166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/1799 [00:16<1:00:18,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1035 1041 1035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Size of sample #0 is invalid (=(2949, 0)) since max_positions=(1024, 1024), skip this example with --skip-invalid-size-inputs-valid-test",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-34d4e178a756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlangs_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlang2scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-f6c917aeff14>\u001b[0m in \u001b[0;36mcompute_scores\u001b[0;34m(lang, k_list)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlangs_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-c0332abaa4df>\u001b[0m in \u001b[0;36mcompute_results\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#print(encoder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     result = model.sample(\n\u001b[0m\u001b[1;32m     15\u001b[0m                     \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                    text_to_id=lambda x: sorted(list(lang_title2wikidataID.get(\n",
      "\u001b[0;32m~/check/mGENRE_MEL/GENRE/genre/fairseq_model.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sentences, beam, verbose, text_to_id, marginalize, marginalize_lenpen, max_len_a, max_len_b, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtokenized_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         batched_hypos = self.generate(\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mbeam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/check/mGENRE_MEL/GENRE/genre/fairseq_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBARTHubInterface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/check/mGENRE_MEL/fairseq/fairseq/hub_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokenized_sentences, beam, verbose, skip_invalid_size_inputs, inference_step_args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0minference_step_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_step_args\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_invalid_size_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_to_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             translations = self.task.inference_step(\n",
      "\u001b[0;32m~/check/mGENRE_MEL/fairseq/fairseq/hub_utils.py\u001b[0m in \u001b[0;36m_build_batches\u001b[0;34m(self, tokens, skip_invalid_size_inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     ) -> Iterator[Dict[str, Any]]:\n\u001b[1;32m    262\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         batch_iterator = self.task.get_batch_iterator(\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset_for_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/check/mGENRE_MEL/fairseq/fairseq/tasks/fairseq_task.py\u001b[0m in \u001b[0;36mget_batch_iterator\u001b[0;34m(self, dataset, max_tokens, max_sentences, max_positions, ignore_invalid_inputs, required_batch_size_multiple, seed, num_shards, shard_id, num_workers, epoch, data_buffer_size, disable_iterator_cache)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# filter examples that are too large\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_positions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             indices = self.filter_indices_by_size(\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_invalid_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             )\n",
      "\u001b[0;32m~/check/mGENRE_MEL/fairseq/fairseq/tasks/fairseq_task.py\u001b[0m in \u001b[0;36mfilter_indices_by_size\u001b[0;34m(self, indices, dataset, max_positions, ignore_invalid_inputs)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignored\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_invalid_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 raise Exception(\n\u001b[0m\u001b[1;32m    191\u001b[0m                     (\n\u001b[1;32m    192\u001b[0m                         \u001b[0;34m\"Size of sample #{} is invalid (={}) since max_positions={}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Size of sample #0 is invalid (=(2949, 0)) since max_positions=(1024, 1024), skip this example with --skip-invalid-size-inputs-valid-test"
     ]
    }
   ],
   "source": [
    "all_langs_micro_avg = []\n",
    "print('Model F:')\n",
    "print('on test dataset:\\n')\n",
    "\n",
    "lang2scores = {}\n",
    "\n",
    "for l in langs_idx:\n",
    "    lang2scores[l] = compute_scores(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_avg_lang(scores, lang='en', k_list=[1, 10, 20, 50, 100]):    \n",
    "    print(f'Language: {lang}')\n",
    "    results = []\n",
    "    for k in k_list:\n",
    "        n = 0\n",
    "        for i, row in enumerate(scores):\n",
    "            if scores['correct'] in scores['predicted'][:k]:\n",
    "                n += 1\n",
    "        results.append(round(n / total_mentions, 5))\n",
    "    for i, k in enumerate(k_list):\n",
    "        print(f'Micro average (k={k}):', results[i])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of GENRE exploration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
