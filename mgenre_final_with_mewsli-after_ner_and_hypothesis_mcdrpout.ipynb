{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Python version and upgrading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kiLr036GIvcc",
    "outputId": "58e463c0-0c4f-4990-a4ef-227b12e090ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/petrakov/.local/lib/python3.8/site-packages (22.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda update -n base -c defaults conda\n",
    "# !conda install -y pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.23.1\n",
      "Uninstalling numpy-1.23.1:\n",
      "  Successfully uninstalled numpy-1.23.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.23.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "Installing collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n",
      "seqeval 1.2.0 requires numpy==1.19.2, but you have numpy 1.23.1 which is incompatible.\n",
      "en-core-web-lg 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.2.4 which is incompatible.\n",
      "deeppavlov 0.17.4 requires filelock==3.0.12, but you have filelock 3.6.0 which is incompatible.\n",
      "deeppavlov 0.17.4 requires numpy==1.18.0, but you have numpy 1.23.1 which is incompatible.\n",
      "deeppavlov 0.17.4 requires overrides==2.7.0, but you have overrides 3.1.0 which is incompatible.\n",
      "deeppavlov 0.17.4 requires pydantic==1.3, but you have pydantic 1.8.2 which is incompatible.\n",
      "deeppavlov 0.17.4 requires requests==2.22.0, but you have requests 2.28.1 which is incompatible.\n",
      "deeppavlov 0.17.4 requires scikit-learn==0.21.2, but you have scikit-learn 0.23.2 which is incompatible.\n",
      "deeppavlov 0.17.4 requires tqdm==4.62.0, but you have tqdm 4.64.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.23.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numpy \n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations of 3 key libraries: KILT, GENRE and fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFeVVTtwDUAl",
    "outputId": "d2c065f5-98ad-4602-f925-f80df0c4a31f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'KILT'...\n",
      "remote: Enumerating objects: 401, done.\u001b[K\n",
      "remote: Counting objects: 100% (151/151), done.\u001b[K\n",
      "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
      "remote: Total 401 (delta 108), reused 92 (delta 92), pack-reused 250\u001b[K\n",
      "Receiving objects: 100% (401/401), 829.97 KiB | 2.24 MiB/s, done.\n",
      "Resolving deltas: 100% (224/224), done.\n",
      "/home/petrakov/success/mGENRE_MEL/KILT\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0mTraceback (most recent call last):\n",
      "  File \"setup.py\", line 8, in <module>\n",
      "    import setuptools\n",
      "ImportError: No module named setuptools\n",
      "/home/petrakov/success/mGENRE_MEL\n",
      "Cloning into 'GENRE'...\n",
      "remote: Enumerating objects: 454, done.\u001b[K\n",
      "remote: Counting objects: 100% (170/170), done.\u001b[K\n",
      "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
      "remote: Total 454 (delta 112), reused 104 (delta 80), pack-reused 284\u001b[K\n",
      "Receiving objects: 100% (454/454), 10.99 MiB | 3.81 MiB/s, done.\n",
      "Resolving deltas: 100% (259/259), done.\n",
      "/home/petrakov/success/mGENRE_MEL/GENRE\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Processing /home/petrakov/success/mGENRE_MEL/GENRE\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: genre\n",
      "  Building wheel for genre (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for genre: filename=genre-0.1.3-py3-none-any.whl size=22447 sha256=4c5e5f6f5014c085f30db8de3a3dd1233e1cb09fbab484441e6e6dabca0f9395\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-nd7p8vr2/wheels/62/4a/37/1ff1546f37fa19ed2bd95b919f4e8ca9c5bb97230251c11707\n",
      "Successfully built genre\n",
      "Installing collected packages: genre\n",
      "  Attempting uninstall: genre\n",
      "    Found existing installation: genre 0.1.3\n",
      "    Uninstalling genre-0.1.3:\n",
      "      Successfully uninstalled genre-0.1.3\n",
      "Successfully installed genre-0.1.3\n",
      "Traceback (most recent call last):\n",
      "  File \"./setup.py\", line 7, in <module>\n",
      "    import setuptools\n",
      "ImportError: No module named setuptools\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/petrakov/.local/lib/python3.8/site-packages (0.1.95)\n",
      "Requirement already satisfied: marisa_trie in /home/petrakov/.local/lib/python3.8/site-packages (0.7.7)\n",
      "Requirement already satisfied: setuptools in /home/petrakov/.local/lib/python3.8/site-packages (from marisa_trie) (59.5.0)\n",
      "/home/petrakov/success/mGENRE_MEL\n",
      "Cloning into 'fairseq'...\n",
      "remote: Enumerating objects: 25465, done.\u001b[K\n",
      "remote: Total 25465 (delta 0), reused 0 (delta 0), pack-reused 25465\u001b[K\n",
      "Receiving objects: 100% (25465/25465), 19.82 MiB | 5.46 MiB/s, done.\n",
      "Resolving deltas: 100% (18491/18491), done.\n",
      "/home/petrakov/success/mGENRE_MEL/fairseq\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///home/petrakov/success/mGENRE_MEL/fairseq\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cython in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (0.29.14)\n",
      "Requirement already satisfied: regex in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (2019.8.19)\n",
      "Requirement already satisfied: tqdm in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (4.64.0)\n",
      "Requirement already satisfied: numpy in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.23.1)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (2.1.0)\n",
      "Requirement already satisfied: cffi in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.15.0)\n",
      "Requirement already satisfied: omegaconf<2.1 in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (2.0.5)\n",
      "Requirement already satisfied: torch in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.10.0)\n",
      "Requirement already satisfied: hydra-core<1.1 in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.0.7)\n",
      "Requirement already satisfied: importlib-resources in /home/petrakov/.local/lib/python3.8/site-packages (from hydra-core<1.1->fairseq==1.0.0a0+4c4d5a7) (5.8.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /home/petrakov/.local/lib/python3.8/site-packages (from hydra-core<1.1->fairseq==1.0.0a0+4c4d5a7) (4.8)\n",
      "Requirement already satisfied: typing-extensions in /home/petrakov/.local/lib/python3.8/site-packages (from omegaconf<2.1->fairseq==1.0.0a0+4c4d5a7) (4.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in /home/petrakov/.local/lib/python3.8/site-packages (from omegaconf<2.1->fairseq==1.0.0a0+4c4d5a7) (6.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/petrakov/.local/lib/python3.8/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4c4d5a7) (0.8.10)\n",
      "Requirement already satisfied: portalocker in /home/petrakov/.local/lib/python3.8/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4c4d5a7) (2.4.0)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4c4d5a7) (0.4.3)\n",
      "Requirement already satisfied: pycparser in /home/petrakov/.local/lib/python3.8/site-packages (from cffi->fairseq==1.0.0a0+4c4d5a7) (2.21)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/petrakov/.local/lib/python3.8/site-packages (from importlib-resources->hydra-core<1.1->fairseq==1.0.0a0+4c4d5a7) (3.8.0)\n",
      "Installing collected packages: fairseq\n",
      "  Running setup.py develop for fairseq\n",
      "Successfully installed fairseq\n",
      "Traceback (most recent call last):\n",
      "  File \"setup.py\", line 10, in <module>\n",
      "    from setuptools import setup, find_packages, Extension\n",
      "ImportError: No module named setuptools\n",
      "Traceback (most recent call last):\n",
      "  File \"setup.py\", line 10, in <module>\n",
      "    from setuptools import setup, find_packages, Extension\n",
      "ImportError: No module named setuptools\n",
      "/home/petrakov/success/mGENRE_MEL\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement kilt (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for kilt\u001b[0m\u001b[31m\n",
      "\u001b[0m/home/petrakov/success/mGENRE_MEL/fairseq\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///home/petrakov/success/mGENRE_MEL/fairseq\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: hydra-core<1.1 in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.0.7)\n",
      "Requirement already satisfied: tqdm in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (4.64.0)\n",
      "Requirement already satisfied: torch in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.10.0)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (2.1.0)\n",
      "Requirement already satisfied: numpy in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.23.1)\n",
      "Requirement already satisfied: regex in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (2019.8.19)\n",
      "Requirement already satisfied: cffi in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (1.15.0)\n",
      "Requirement already satisfied: omegaconf<2.1 in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (2.0.5)\n",
      "Requirement already satisfied: cython in /home/petrakov/.local/lib/python3.8/site-packages (from fairseq==1.0.0a0+4c4d5a7) (0.29.14)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /home/petrakov/.local/lib/python3.8/site-packages (from hydra-core<1.1->fairseq==1.0.0a0+4c4d5a7) (4.8)\n",
      "Requirement already satisfied: importlib-resources in /home/petrakov/.local/lib/python3.8/site-packages (from hydra-core<1.1->fairseq==1.0.0a0+4c4d5a7) (5.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in /home/petrakov/.local/lib/python3.8/site-packages (from omegaconf<2.1->fairseq==1.0.0a0+4c4d5a7) (6.0)\n",
      "Requirement already satisfied: typing-extensions in /home/petrakov/.local/lib/python3.8/site-packages (from omegaconf<2.1->fairseq==1.0.0a0+4c4d5a7) (4.2.0)\n",
      "Requirement already satisfied: portalocker in /home/petrakov/.local/lib/python3.8/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4c4d5a7) (2.4.0)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4c4d5a7) (0.4.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/petrakov/.local/lib/python3.8/site-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4c4d5a7) (0.8.10)\n",
      "Requirement already satisfied: pycparser in /home/petrakov/.local/lib/python3.8/site-packages (from cffi->fairseq==1.0.0a0+4c4d5a7) (2.21)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/petrakov/.local/lib/python3.8/site-packages (from importlib-resources->hydra-core<1.1->fairseq==1.0.0a0+4c4d5a7) (3.8.0)\n",
      "Installing collected packages: fairseq\n",
      "  Attempting uninstall: fairseq\n",
      "    Found existing installation: fairseq 1.0.0a0+4c4d5a7\n",
      "    Uninstalling fairseq-1.0.0a0+4c4d5a7:\n",
      "      Successfully uninstalled fairseq-1.0.0a0+4c4d5a7\n",
      "  Running setup.py develop for fairseq\n",
      "Successfully installed fairseq\n",
      "/home/petrakov/success/mGENRE_MEL\n"
     ]
    }
   ],
   "source": [
    "#KILT\n",
    "%rm -rf KILT\n",
    "!git clone https://github.com/facebookresearch/KILT.git\n",
    "%cd KILT\n",
    "!pip install -r requirements.txt\n",
    "!python setup.py install\n",
    "sys.path.append(os.getcwd())\n",
    "%cd ..\n",
    "\n",
    "#GENRE\n",
    "%rm -rf GENRE\n",
    "!git clone https://github.com/facebookresearch/GENRE.git\n",
    "%cd GENRE\n",
    "!pip install ./\n",
    "!python ./setup.py build develop install\n",
    "!pip install sentencepiece marisa_trie\n",
    "sys.path.append(os.getcwd())\n",
    "%cd ..\n",
    "\n",
    "# fairseq\n",
    "%rm -rf fairseq\n",
    "!git clone --branch fixing_prefix_allowed_tokens_fn https://github.com/nicola-decao/fairseq\n",
    "%cd fairseq\n",
    "!sed -i -e '26,27d' fairseq/registry.py\n",
    "!pip install --editable ./\n",
    "!python setup.py build develop\n",
    "!python setup.py install\n",
    "sys.path.append(os.getcwd())\n",
    "%cd ..\n",
    "\n",
    "!pip install kilt\n",
    "\n",
    "#####\n",
    "%cd fairseq\n",
    "!pip install --editable ./\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gdown in /home/petrakov/.local/lib/python3.8/site-packages (3.12.2)\n",
      "Requirement already satisfied: requests[socks] in /home/petrakov/.local/lib/python3.8/site-packages (from gdown) (2.28.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown) (1.14.0)\n",
      "Requirement already satisfied: filelock in /home/petrakov/.local/lib/python3.8/site-packages (from gdown) (3.6.0)\n",
      "Requirement already satisfied: tqdm in /home/petrakov/.local/lib/python3.8/site-packages (from gdown) (4.64.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/petrakov/.local/lib/python3.8/site-packages (from requests[socks]->gdown) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/petrakov/.local/lib/python3.8/site-packages (from requests[socks]->gdown) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2019.11.28)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/petrakov/.local/lib/python3.8/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "from wikidata.client import Client\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data and model (comment cell below if already downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pretrained model\n",
    "# ! rm -rf fairseq_multilingual_entity_disambiguation*\n",
    "# ! wget https://dl.fbaipublicfiles.com/GENRE/fairseq_multilingual_entity_disambiguation.tar.gz\n",
    "# ! tar -xvf fairseq_multilingual_entity_disambiguation.tar.gz\n",
    "\n",
    "# # data\n",
    "# ! wget https://dl.fbaipublicfiles.com/GENRE/lang_title2wikidataID-normalized_with_redirect.pkl\n",
    "# ! wget http://dl.fbaipublicfiles.com/GENRE/titles_lang_all105_marisa_trie_with_redirect.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1QpLVXGahVjAoEq8TrE70aKtMLJCNeASh\n",
      "To: /home/petrakov/success/mGENRE_MEL/mentions_test.json\n",
      "100%|██████████| 65.1M/65.1M [00:01<00:00, 64.9MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mentions_test.json'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_ = \"1QpLVXGahVjAoEq8TrE70aKtMLJCNeASh\"\n",
    "gdown.download('https://drive.google.com/uc?id=1QpLVXGahVjAoEq8TrE70aKtMLJCNeASh', output=\"mentions_test.json\", quiet=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mentions_test.json\") as f:\n",
    "    test_set = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In original version there were serious of troubles that's why correct version we add here from external google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fairseq_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fairseq_model.py\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from fairseq import search, utils\n",
    "from fairseq.models.bart import BARTHubInterface, BARTModel\n",
    "from omegaconf import open_dict\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class GENREHubInterface(BARTHubInterface):\n",
    "    def sample(\n",
    "        self,\n",
    "        sentences: List[str],\n",
    "        beam: int = 5,\n",
    "        verbose: bool = False,\n",
    "        text_to_id=None,\n",
    "        marginalize=False,\n",
    "        marginalize_lenpen=0.5,\n",
    "        max_len_a=1024,\n",
    "        max_len_b=1024,\n",
    "        **kwargs,\n",
    "    ) -> List[str]:\n",
    "        if isinstance(sentences, str):\n",
    "            return self.sample([sentences], beam=beam, verbose=verbose, **kwargs)[0]\n",
    "        tokenized_sentences = [self.encode(sentence) for sentence in sentences]\n",
    "\n",
    "        batched_hypos = self.generate(\n",
    "            tokenized_sentences,\n",
    "            beam,\n",
    "            verbose,\n",
    "            max_len_a=max_len_a,\n",
    "            max_len_b=max_len_b,\n",
    "            **kwargs,\n",
    "        )\n",
    "        #print(\"batched_hypos\", batched_hypos)\n",
    "        \n",
    "        outputs = [\n",
    "            [\n",
    "                {\"text\": self.decode(hypo[\"tokens\"]), \"score\": hypo[\"score\"]}\n",
    "                for hypo in hypos\n",
    "            ]\n",
    "            for hypos in batched_hypos\n",
    "        ]\n",
    "        if text_to_id:\n",
    "            outputs = [\n",
    "                [{**hypo, \"id\": text_to_id(hypo[\"text\"])} for hypo in hypos]\n",
    "                for hypos in outputs\n",
    "            ]\n",
    "\n",
    "            if marginalize:\n",
    "                for (i, hypos), hypos_tok in zip(enumerate(outputs), batched_hypos):\n",
    "                    outputs_dict = defaultdict(list)\n",
    "                    for hypo, hypo_tok in zip(hypos, hypos_tok):\n",
    "                        outputs_dict[hypo[\"id\"]].append(\n",
    "                            {**hypo, \"len\": len(hypo_tok[\"tokens\"])}\n",
    "                        )\n",
    "\n",
    "                    outputs[i] = sorted(\n",
    "                        [\n",
    "                            {\n",
    "                                \"id\": _id,\n",
    "                                \"texts\": [hypo[\"text\"] for hypo in hypos],\n",
    "                                \"scores\": torch.stack(\n",
    "                                    [hypo[\"score\"] for hypo in hypos]\n",
    "                                ),\n",
    "                                \"score\": torch.stack(\n",
    "                                    [\n",
    "                                        hypo[\"score\"]\n",
    "                                        * hypo[\"len\"]\n",
    "                                        / (hypo[\"len\"] ** marginalize_lenpen)\n",
    "                                        for hypo in hypos\n",
    "                                    ]\n",
    "                                ).logsumexp(-1),\n",
    "                            }\n",
    "                            for _id, hypos in outputs_dict.items()\n",
    "                        ],\n",
    "                        key=lambda x: x[\"score\"],\n",
    "                        reverse=True,\n",
    "                    )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def generate(self, *args, **kwargs) -> List[List[Dict[str, torch.Tensor]]]:\n",
    "        return super(BARTHubInterface, self).generate(*args, **kwargs)\n",
    "\n",
    "    def encode(self, sentence) -> torch.LongTensor:\n",
    "        tokens = super(BARTHubInterface, self).encode(sentence)\n",
    "        tokens[\n",
    "            tokens >= len(self.task.target_dictionary)\n",
    "        ] = self.task.target_dictionary.unk_index\n",
    "        if tokens[0] != self.task.target_dictionary.bos_index:\n",
    "            return torch.cat(\n",
    "                (torch.tensor([self.task.target_dictionary.bos_index]), tokens)\n",
    "            )\n",
    "        else:\n",
    "            return tokens\n",
    "\n",
    "    \n",
    "class GENRE(BARTModel):\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        model_name_or_path,\n",
    "        checkpoint_file=\"model.pt\",\n",
    "        data_name_or_path=\".\",\n",
    "        bpe=\"gpt2\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        from fairseq import hub_utils\n",
    "\n",
    "        x = hub_utils.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            checkpoint_file,\n",
    "            data_name_or_path,\n",
    "            archive_map=cls.hub_models(),\n",
    "            bpe=bpe,\n",
    "            load_checkpoint_heads=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return GENREHubInterface(x[\"args\"], x[\"task\"], x[\"models\"][0])\n",
    "\n",
    "\n",
    "class mGENRE(BARTModel):\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        model_name_or_path,\n",
    "        sentencepiece_model=\"spm_256000.model\",\n",
    "        checkpoint_file=\"model.pt\",\n",
    "        data_name_or_path=\".\",\n",
    "        bpe=\"sentencepiece\",\n",
    "        layernorm_embedding=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        from fairseq import hub_utils\n",
    "\n",
    "        x = hub_utils.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            checkpoint_file,\n",
    "            data_name_or_path,\n",
    "            archive_map=cls.hub_models(),\n",
    "            bpe=bpe,\n",
    "            load_checkpoint_heads=True,\n",
    "            sentencepiece_model=os.path.join(model_name_or_path, sentencepiece_model),\n",
    "            **kwargs,\n",
    "        )\n",
    "        return GENREHubInterface(x[\"args\"], x[\"task\"], x[\"models\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mv fairseq_model.py GENRE/genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Autoregressive Entity Linking\n",
    "\n",
    "![](s.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are starting with importing our model mGENRE and it's important part, it was slightly changed in previous cells in order to reach stable work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "XURCH8bMyn2S"
   },
   "outputs": [],
   "source": [
    "from GENRE.genre.trie import Trie, MarisaTrie\n",
    "from GENRE.genre.fairseq_model import mGENRE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load data that connects titles with wikidata ids and trie that is an important part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lang_title2wikidataID-normalized_with_redirect.pkl\", \"rb\") as f:\n",
    "    lang_title2wikidataID = pickle.load(f)\n",
    "  \n",
    "# with open(\"titles_lang_all105_trie_with_redirect.pkl\", \"rb\") as f:\n",
    "#     trie = Trie.load_from_dict(pickle.load(f))\n",
    "\n",
    "with open(\"titles_lang_all105_marisa_trie_with_redirect.pkl\", \"rb\") as f:\n",
    "    trie = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPU leads to faster inference, so if possible it is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we chosen 4 because there were lot's of free memory on our own server, you can check the memory available for you using the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver version: \u001b[1m495.29.05\u001b[m\r\n",
      "------------------- \u001b[1mDevice 0\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 3090\u001b[m\r\n",
      "Memory usage: 23986MiB / 24268MiB\r\n",
      "Temperature: \u001b[92m26C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n",
      "------------------- \u001b[1mDevice 1\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 3090\u001b[m\r\n",
      "Memory usage: 20119MiB / 24268MiB\r\n",
      "Temperature: \u001b[92m67C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n",
      "------------------- \u001b[1mDevice 2\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 3090\u001b[m\r\n",
      "Memory usage: 23173MiB / 24268MiB\r\n",
      "Temperature: \u001b[92m22C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n",
      "------------------- \u001b[1mDevice 3\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 2080 Ti\u001b[m\r\n",
      "Memory usage:  3502MiB / 11019MiB\r\n",
      "Temperature: \u001b[92m23C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n",
      "------------------- \u001b[1mDevice 4\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 2080 Ti\u001b[m\r\n",
      "Memory usage: 10251MiB / 11019MiB\r\n",
      "Temperature: \u001b[92m25C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-cdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Oruf5Ng61T33",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petrakov/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:143: UserWarning: \n",
      "NVIDIA GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GENREHubInterface(\n",
       "  (models): ModuleList(\n",
       "    (0): BARTModel(\n",
       "      (encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): TransformerDecoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (output_projection): Linear(in_features=1024, out_features=256001, bias=False)\n",
       "      )\n",
       "      (classification_heads): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (model): BARTModel(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (output_projection): Linear(in_features=1024, out_features=256001, bias=False)\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate Wikipedia titles and language IDs\n",
    "model_mGENRE = mGENRE.from_pretrained(\"fairseq_multilingual_entity_disambiguation\").eval()\n",
    "model_mGENRE.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petrakov/success/mGENRE_MEL/fairseq/fairseq/search.py:205: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  beams_buf = indices_buf // vocab_size\n",
      "/home/petrakov/success/mGENRE_MEL/fairseq/fairseq/sequence_generator.py:659: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  unfin_idx = idx // beam_size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'id': 'Q937',\n",
       "   'texts': ['Albert Einstein >> en'],\n",
       "   'scores': tensor([-0.7137], device='cuda:3'),\n",
       "   'score': tensor(-1.5959, device='cuda:3')},\n",
       "  {'id': 'Q363005',\n",
       "   'texts': ['John Singleton >> en'],\n",
       "   'scores': tensor([-0.7909], device='cuda:3'),\n",
       "   'score': tensor(-1.9374, device='cuda:3')},\n",
       "  {'id': 'Q7374',\n",
       "   'texts': ['Alfred Hitchcock >> en'],\n",
       "   'scores': tensor([-0.8762], device='cuda:3'),\n",
       "   'score': tensor(-2.3182, device='cuda:3')},\n",
       "  {'id': 'Q715110',\n",
       "   'texts': ['Robert Maynard >> en'],\n",
       "   'scores': tensor([-0.8954], device='cuda:3'),\n",
       "   'score': tensor(-2.3689, device='cuda:3')},\n",
       "  {'id': 'Q9353',\n",
       "   'texts': ['John Locke >> en'],\n",
       "   'scores': tensor([-1.0060], device='cuda:3'),\n",
       "   'score': tensor(-2.4641, device='cuda:3')}]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"[START] The founder of the theory of relativity [END] received the Nobel Prize.\"]\n",
    "model_mGENRE.sample(\n",
    "    sentences,\n",
    "    beam = 5,\n",
    "    prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
    "        e for e in trie.get(sent.tolist())\n",
    "        if e < len(model_mGENRE.task.target_dictionary)\n",
    "    ],\n",
    "    text_to_id=lambda x: max(lang_title2wikidataID[tuple(reversed(x.split(\" >> \")))], key=lambda y: int(y[1:])),\n",
    "    marginalize=True,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results above demonstrate that each *text* has it's own id and via marginalization procedure we have unique score for predicted entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis of no [START] and [END]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this part we check that [START] and [END] tokens are really needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'id': 'Q1517374',\n",
       "   'texts': ['History of radio >> en'],\n",
       "   'scores': tensor([-0.4300], device='cuda:4'),\n",
       "   'score': tensor(-1.0534, device='cuda:4')},\n",
       "  {'id': 'Q1516738',\n",
       "   'texts': ['Invention of radio >> en'],\n",
       "   'scores': tensor([-0.6202], device='cuda:4'),\n",
       "   'score': tensor(-1.6408, device='cuda:4')},\n",
       "  {'id': 'Q16887156',\n",
       "   'texts': ['List of inventions in the medieval Islamic world >> en',\n",
       "    'List of inventions in the medieval Islamic world >> ar'],\n",
       "   'scores': tensor([-0.5023, -1.5995], device='cuda:4'),\n",
       "   'score': tensor(-1.7181, device='cuda:4')},\n",
       "  {'id': 'Q4501817',\n",
       "   'texts': ['Timeline of radio >> en'],\n",
       "   'scores': tensor([-0.9116], device='cuda:4'),\n",
       "   'score': tensor(-2.4119, device='cuda:4')}]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"Who invented radio?\"]\n",
    "model_mGENRE.sample(\n",
    "    sentences,\n",
    "    beam = 5,\n",
    "    prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
    "        e for e in trie.get(sent.tolist())\n",
    "        if e < len(model_mGENRE.task.target_dictionary)\n",
    "    ],\n",
    "    text_to_id=lambda x: max(lang_title2wikidataID[tuple(reversed(x.split(\" >> \")))], key=lambda y: int(y[1:])),\n",
    "    marginalize=True,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'id': 'Q362',\n",
       "   'texts': ['World War II >> en', 'Second World War >> en'],\n",
       "   'scores': tensor([-0.4641, -0.6737], device='cuda:4'),\n",
       "   'score': tensor(-0.6678, device='cuda:4')},\n",
       "  {'id': 'Q5370388',\n",
       "   'texts': ['Military history of the United States during World War II >> en'],\n",
       "   'scores': tensor([-0.2719], device='cuda:4'),\n",
       "   'score': tensor(-0.9804, device='cuda:4')},\n",
       "  {'id': 'Q192781',\n",
       "   'texts': ['Military history >> en'],\n",
       "   'scores': tensor([-0.8395], device='cuda:4'),\n",
       "   'score': tensor(-1.8772, device='cuda:4')},\n",
       "  {'id': 'Q131110',\n",
       "   'texts': ['History of the United States >> en'],\n",
       "   'scores': tensor([-0.7007], device='cuda:4'),\n",
       "   'score': tensor(-1.9820, device='cuda:4')}]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"When Second World war started?\"]\n",
    "model_mGENRE.sample(\n",
    "    sentences,\n",
    "    beam = 5,\n",
    "    prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
    "        e for e in trie.get(sent.tolist())\n",
    "        if e < len(model_mGENRE.task.target_dictionary)\n",
    "    ],\n",
    "    text_to_id=lambda x: max(lang_title2wikidataID[tuple(reversed(x.split(\" >> \")))], key=lambda y: int(y[1:])),\n",
    "    marginalize=True,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can say that entities within simple questions like above correctly determine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative assessment of the hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!!before that you have to clone wikidata-simplequestions github repository (https://github.com/askplatypus/wikidata-simplequestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_simple_questions = \"/home/petrakov/wikidata-simplequestions/annotated_wd_data_train_answerable.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object</th>\n",
       "      <th>property</th>\n",
       "      <th>subject</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q12439</td>\n",
       "      <td>R19</td>\n",
       "      <td>Q6106580</td>\n",
       "      <td>who is a musician born in detroit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q6817891</td>\n",
       "      <td>P364</td>\n",
       "      <td>Q1568</td>\n",
       "      <td>what is the language in which mera shikar was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1297</td>\n",
       "      <td>R276</td>\n",
       "      <td>Q2888523</td>\n",
       "      <td>Whats the name of a battle that happened in ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q193592</td>\n",
       "      <td>R413</td>\n",
       "      <td>Q5822614</td>\n",
       "      <td>what player plays the position midfielder?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q6849115</td>\n",
       "      <td>P413</td>\n",
       "      <td>Q336286</td>\n",
       "      <td>what is the position that  mike twellman plays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19476</th>\n",
       "      <td>Q223960</td>\n",
       "      <td>P171</td>\n",
       "      <td>Q128001</td>\n",
       "      <td>what classification does  mountain tapir come ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19477</th>\n",
       "      <td>Q1535153</td>\n",
       "      <td>R136</td>\n",
       "      <td>Q7727315</td>\n",
       "      <td>What's a superhero movie that premiered on too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19478</th>\n",
       "      <td>Q157443</td>\n",
       "      <td>R136</td>\n",
       "      <td>Q966690</td>\n",
       "      <td>What is the name of a comedy film that is also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19479</th>\n",
       "      <td>Q16093542</td>\n",
       "      <td>P27</td>\n",
       "      <td>Q145</td>\n",
       "      <td>What is the nationality of anthony bailey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19480</th>\n",
       "      <td>Q926822</td>\n",
       "      <td>P21</td>\n",
       "      <td>Q6581097</td>\n",
       "      <td>What gender is gastón filgueira</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19481 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          object property   subject  \\\n",
       "0         Q12439      R19  Q6106580   \n",
       "1       Q6817891     P364     Q1568   \n",
       "2          Q1297     R276  Q2888523   \n",
       "3        Q193592     R413  Q5822614   \n",
       "4       Q6849115     P413   Q336286   \n",
       "...          ...      ...       ...   \n",
       "19476    Q223960     P171   Q128001   \n",
       "19477   Q1535153     R136  Q7727315   \n",
       "19478    Q157443     R136   Q966690   \n",
       "19479  Q16093542      P27      Q145   \n",
       "19480    Q926822      P21  Q6581097   \n",
       "\n",
       "                                                question  \n",
       "0                      who is a musician born in detroit  \n",
       "1      what is the language in which mera shikar was ...  \n",
       "2      Whats the name of a battle that happened in ch...  \n",
       "3             what player plays the position midfielder?  \n",
       "4         what is the position that  mike twellman plays  \n",
       "...                                                  ...  \n",
       "19476  what classification does  mountain tapir come ...  \n",
       "19477  What's a superhero movie that premiered on too...  \n",
       "19478  What is the name of a comedy film that is also...  \n",
       "19479          What is the nationality of anthony bailey  \n",
       "19480                    What gender is gastón filgueira  \n",
       "\n",
       "[19481 rows x 4 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_table(path_to_train_simple_questions, header=None).rename(columns = {0:\"object\", 1:\"property\", 2:\"subject\", 3:\"question\"})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:00:42.891695\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "start_time = datetime.now()\n",
    "\n",
    "\n",
    "mGENRE_results = model_mGENRE.sample(\n",
    "                                    list(data.loc[:n, \"question\"]),\n",
    "                                    beam = 3,\n",
    "                                    prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
    "                                        e for e in trie.get(sent.tolist())\n",
    "                                        if e < len(model_mGENRE.task.target_dictionary)\n",
    "                                    ],\n",
    "                                    text_to_id=lambda x: max(lang_title2wikidataID[tuple(reversed(x.split(\" >> \")))], key=lambda y: int(y[1:])),\n",
    "                                    marginalize=True,\n",
    "                                    verbose = True\n",
    "                                )\n",
    "\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [i[0]['id'] for i in mGENRE_results]\n",
    "y_true = list(data.loc[:n, \"object\"])\n",
    "result = [x in y_pred for x in y_true] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we take only the most probable score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2475"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(sum(result)/len(result), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we take an account all predicted ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [[i['id'] for i in mGENRE_results[j]] for j in range(len(mGENRE_results))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for index, out_list in enumerate(out):\n",
    "    if y_true[index] in out_list:\n",
    "        res.append(1)\n",
    "    else:\n",
    "        res.append(0)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3168"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(sum(res)/len(res), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we take on account only those questions where we certain (top 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [-3, -2, -1.5, -1, -0.75, -0.6, -0.4, -0.2, -0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold =  -3 \t accuracy =  2.8400000000000003 %\t number of observations =  387 \t share of observations =  96.75 %\n",
      "threshold =  -2 \t accuracy =  3.0700000000000003 %\t number of observations =  326 \t share of observations =  81.5 %\n",
      "threshold =  -1.5 \t accuracy =  3.35 %\t number of observations =  239 \t share of observations =  59.75 %\n",
      "threshold =  -1 \t accuracy =  3.36 %\t number of observations =  149 \t share of observations =  37.25 %\n",
      "threshold =  -0.75 \t accuracy =  1.87 %\t number of observations =  107 \t share of observations =  26.75 %\n",
      "threshold =  -0.6 \t accuracy =  0.0 %\t number of observations =  78 \t share of observations =  19.5 %\n",
      "threshold =  -0.4 \t accuracy =  0.0 %\t number of observations =  40 \t share of observations =  10.0 %\n",
      "threshold =  -0.2 \t accuracy =  0.0 %\t number of observations =  12 \t share of observations =  3.0 %\n",
      "threshold =  -0.1 \t accuracy =  0.0 %\t number of observations =  5 \t share of observations =  1.25 %\n"
     ]
    }
   ],
   "source": [
    "accuracy_100_top_1 = []\n",
    "share_of_observations_100_top_1 = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    certain_out = [i[0]['id'] for i in mGENRE_results if i[0]['score'] > threshold]\n",
    "    indexes = [1 if i[0]['score'] > threshold else 0 for i in mGENRE_results]\n",
    "    y_true = list(compress(list(data.loc[:n, \"object\"]), indexes))\n",
    "    result = [x in certain_out for x in y_true] \n",
    "    accuracy = np.round(sum(result)/len(result), 4)*100\n",
    "    accuracy_100_top_1.append(accuracy)\n",
    "    share = len(result)/n*100\n",
    "    share_of_observations_100_top_1.append(share)\n",
    "    \n",
    "    print(\"threshold = \", threshold, \"\\t\",\n",
    "          \"accuracy = \", accuracy, \"%\\t\", \n",
    "          \"number of observations = \", len(result), '\\t',\n",
    "          \"share of observations = \", len(result)/n*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "plt.xlabel(\"rejection rate, %\", fontsize = 20)\n",
    "plt.ylabel(\"accuracy, %\", fontsize = 20)\n",
    "plt.plot(share_of_observations_100_top_1[::-1], accuracy_100_top_1, marker = '.', markersize = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add more observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petrakov/success/mGENRE_MEL/fairseq/fairseq/search.py:205: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  beams_buf = indices_buf // vocab_size\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 752.00 MiB (GPU 3; 10.76 GiB total capacity; 4.90 GiB already allocated; 615.56 MiB free; 5.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-57517dbc3a8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m mGENRE_results = model_mGENRE.sample(\n\u001b[0m\u001b[1;32m      6\u001b[0m                                     \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                     \u001b[0mbeam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/success/mGENRE_MEL/GENRE/genre/fairseq_model.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sentences, beam, verbose, text_to_id, marginalize, marginalize_lenpen, max_len_a, max_len_b, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtokenized_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         batched_hypos = self.generate(\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mbeam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/success/mGENRE_MEL/GENRE/genre/fairseq_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBARTHubInterface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/success/mGENRE_MEL/fairseq/fairseq/hub_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokenized_sentences, beam, verbose, skip_invalid_size_inputs, inference_step_args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_invalid_size_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_to_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             translations = self.task.inference_step(\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minference_step_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             )\n",
      "\u001b[0;32m~/success/mGENRE_MEL/fairseq/fairseq/tasks/fairseq_task.py\u001b[0m in \u001b[0;36minference_step\u001b[0;34m(self, generator, models, sample, prefix_tokens, constraints)\u001b[0m\n\u001b[1;32m    499\u001b[0m     ):\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             return generator.generate(\n\u001b[0m\u001b[1;32m    502\u001b[0m                 \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/success/mGENRE_MEL/fairseq/fairseq/sequence_generator.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, models, sample, **kwargs)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0mfinalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0msrc_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"net_input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"src_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/success/mGENRE_MEL/fairseq/fairseq/sequence_generator.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, sample, prefix_tokens, constraints, bos_token)\u001b[0m\n\u001b[1;32m    319\u001b[0m                 )\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             lprobs, avg_attn_scores = self.model.forward_decoder(\n\u001b[0m\u001b[1;32m    322\u001b[0m                 \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mencoder_outs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/success/mGENRE_MEL/fairseq/fairseq/sequence_generator.py\u001b[0m in \u001b[0;36mforward_decoder\u001b[0;34m(self, tokens, encoder_outs, incremental_states, temperature)\u001b[0m\n\u001b[1;32m    800\u001b[0m             )\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m             probs = model.get_normalized_probs(\n\u001b[0m\u001b[1;32m    803\u001b[0m                 \u001b[0mdecoder_out_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             )\n",
      "\u001b[0;32m~/success/mGENRE_MEL/fairseq/fairseq/models/transformer.py\u001b[0m in \u001b[0;36mget_normalized_probs\u001b[0;34m(self, net_output, log_probs, sample)\u001b[0m\n\u001b[1;32m    311\u001b[0m     ):\n\u001b[1;32m    312\u001b[0m         \u001b[0;34m\"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_normalized_probs_scriptable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/success/mGENRE_MEL/fairseq/fairseq/models/fairseq_model.py\u001b[0m in \u001b[0;36mget_normalized_probs_scriptable\u001b[0;34m(self, net_output, log_probs, sample)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;34m\"\"\"Scriptable helper function for get_normalized_probs in ~BaseFairseqModel\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"decoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_normalized_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# syntactic sugar for simple models which don't have a decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/success/mGENRE_MEL/fairseq/fairseq/models/fairseq_decoder.py\u001b[0m in \u001b[0;36mget_normalized_probs\u001b[0;34m(self, net_output, log_probs, sample)\u001b[0m\n\u001b[1;32m     65\u001b[0m     ):\n\u001b[1;32m     66\u001b[0m         \u001b[0;34m\"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_normalized_probs_scriptable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# TorchScript doesn't support super() method so that the scriptable Subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/success/mGENRE_MEL/fairseq/fairseq/models/fairseq_decoder.py\u001b[0m in \u001b[0;36mget_normalized_probs_scriptable\u001b[0;34m(self, net_output, log_probs, sample)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnx_trace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnx_trace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/success/mGENRE_MEL/fairseq/fairseq/utils.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(x, dim, onnx_trace)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1769\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1771\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1772\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 752.00 MiB (GPU 3; 10.76 GiB total capacity; 4.90 GiB already allocated; 615.56 MiB free; 5.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "n = 400\n",
    "start_time = datetime.now()\n",
    "\n",
    "\n",
    "mGENRE_results = model_mGENRE.sample(\n",
    "                                    list(data.sample(n = n, replace = False, random_state=1).loc[:, \"question\"]),\n",
    "                                    beam = 3,\n",
    "                                    prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
    "                                        e for e in trie.get(sent.tolist())\n",
    "                                        if e < len(model_mGENRE.task.target_dictionary)\n",
    "                                    ],\n",
    "                                    text_to_id=lambda x: max(lang_title2wikidataID[tuple(reversed(x.split(\" >> \")))], key=lambda y: int(y[1:])),\n",
    "                                    marginalize=True,\n",
    "                                    verbose = True\n",
    "                                )\n",
    "\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [-10, -3, -2, -1.5, -1, -0.75, -0.6, -0.4, -0.2, -0.1]\n",
    "accuracy_400_top_1 = []\n",
    "share_of_observations_400_top_1 = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    certain_out = [i[0]['id'] for i in mGENRE_results if i[0]['score'] > threshold]\n",
    "    indexes = [1 if i[0]['score'] > threshold else 0 for i in mGENRE_results]\n",
    "    y_true = list(compress(list(data.loc[list(data.sample(n = n, replace = False, random_state=1).index), \"object\"]), indexes))\n",
    "    result = [x in certain_out for x in y_true] \n",
    "    accuracy = np.round(sum(result)/len(result), 2)*100\n",
    "    accuracy_400_top_1.append(accuracy)\n",
    "    share = np.round(len(result)/n*100, 2)\n",
    "    share_of_observations_400_top_1.append(share)\n",
    "    \n",
    "    print(\"threshold = \", threshold, \"\\t\",\n",
    "          \"accuracy = \", accuracy, \"%\\t\", \n",
    "          \"number of observations = \", len(result), '\\t',\n",
    "          \"share of observations = \", share, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "plt.xlabel(\"rejection rate, %\", fontsize = 20)\n",
    "plt.ylabel(\"accuracy, %\", fontsize = 20)\n",
    "plt.title(\"mGENRE certainty basing on 400 random samples\")\n",
    "plt.plot(share_of_observations_400_top_1[::-1], accuracy_400_top_1, marker = '.', markersize = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch approach to add observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part  0  done!\n",
      "part  1  done!\n",
      "part  2  done!\n",
      "part  3  done!\n",
      "part  4  done!\n",
      "part  5  done!\n",
      "part  6  done!\n",
      "part  7  done!\n",
      "part  8  done!\n",
      "part  9  done!\n",
      "part  10  done!\n",
      "part  11  done!\n",
      "part  12  done!\n",
      "part  13  done!\n",
      "part  14  done!\n",
      "part  15  done!\n",
      "part  16  done!\n",
      "part  17  done!\n",
      "part  18  done!\n",
      "part  19  done!\n",
      "part  20  done!\n",
      "part  21  done!\n",
      "part  22  done!\n",
      "part  23  done!\n",
      "part  24  done!\n",
      "part  25  done!\n",
      "part  26  done!\n",
      "part  27  done!\n",
      "part  28  done!\n",
      "part  29  done!\n",
      "part  30  done!\n",
      "part  31  done!\n",
      "part  32  done!\n",
      "part  33  done!\n",
      "part  34  done!\n",
      "part  35  done!\n",
      "part  36  done!\n",
      "part  37  done!\n",
      "part  38  done!\n",
      "part  39  done!\n",
      "part  40  done!\n",
      "part  41  done!\n",
      "part  42  done!\n",
      "part  43  done!\n",
      "part  44  done!\n",
      "part  45  done!\n",
      "part  46  done!\n",
      "part  47  done!\n",
      "part  48  done!\n",
      "part  49  done!\n",
      "part  50  done!\n",
      "part  51  done!\n",
      "part  52  done!\n",
      "part  53  done!\n",
      "part  54  done!\n",
      "part  55  done!\n",
      "part  56  done!\n",
      "part  57  done!\n",
      "part  58  done!\n",
      "part  59  done!\n",
      "part  60  done!\n",
      "part  61  done!\n",
      "part  62  done!\n",
      "part  63  done!\n",
      "part  64  done!\n",
      "part  65  done!\n",
      "part  66  done!\n",
      "part  67  done!\n",
      "part  68  done!\n",
      "part  69  done!\n",
      "part  70  done!\n",
      "part  71  done!\n",
      "part  72  done!\n",
      "part  73  done!\n",
      "part  74  done!\n",
      "part  75  done!\n",
      "part  76  done!\n",
      "part  77  done!\n",
      "part  78  done!\n",
      "part  79  done!\n",
      "Duration: 0:08:17.836753\n"
     ]
    }
   ],
   "source": [
    "n = 400\n",
    "start_time = datetime.now()\n",
    "\n",
    "divider = 80\n",
    "\n",
    "mGENRE_results = []\n",
    "for i in range(divider):\n",
    "\n",
    "    mGENRE_results_i = model_mGENRE.sample(\n",
    "                                        list(data.sample(n = 5, replace = False, random_state=i).loc[:, \"question\"]),\n",
    "                                        beam = 3,\n",
    "                                        prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
    "                                            e for e in trie.get(sent.tolist())\n",
    "                                            if e < len(model_mGENRE.task.target_dictionary)\n",
    "                                        ],\n",
    "                                        text_to_id=lambda x: max(lang_title2wikidataID[tuple(reversed(x.split(\" >> \")))], key=lambda y: int(y[1:])),\n",
    "                                        marginalize=True,\n",
    "                                        verbose = True\n",
    "                                    )\n",
    "    mGENRE_results.append(mGENRE_results_i)\n",
    "    print(\"part \", i, \" done!\")\n",
    "\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mGENRE_results = sum(mGENRE_results, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = []\n",
    "for i in range(divider): \n",
    "    objects.append(list(data.sample(n = 5, replace = False, random_state=i).loc[:, \"object\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = sum(objects, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold =  -10 \t accuracy =  23.0 %\t number of observations =  400 \t share of observations =  100.0 %\n",
      "threshold =  -3 \t accuracy =  24.0 %\t number of observations =  383 \t share of observations =  95.75 %\n",
      "threshold =  -2 \t accuracy =  27.0 %\t number of observations =  331 \t share of observations =  82.75 %\n",
      "threshold =  -1.5 \t accuracy =  31.0 %\t number of observations =  249 \t share of observations =  62.25 %\n",
      "threshold =  -1 \t accuracy =  39.0 %\t number of observations =  141 \t share of observations =  35.25 %\n",
      "threshold =  -0.75 \t accuracy =  43.0 %\t number of observations =  95 \t share of observations =  23.75 %\n",
      "threshold =  -0.6 \t accuracy =  45.0 %\t number of observations =  76 \t share of observations =  19.0 %\n",
      "threshold =  -0.4 \t accuracy =  61.0 %\t number of observations =  46 \t share of observations =  11.5 %\n",
      "threshold =  -0.2 \t accuracy =  68.0 %\t number of observations =  19 \t share of observations =  4.75 %\n",
      "threshold =  -0.1 \t accuracy =  67.0 %\t number of observations =  6 \t share of observations =  1.5 %\n"
     ]
    }
   ],
   "source": [
    "thresholds = [-10, -3, -2, -1.5, -1, -0.75, -0.6, -0.4, -0.2, -0.1]\n",
    "accuracy_400_top_1 = []\n",
    "share_of_observations_400_top_1 = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    certain_out = [i[0]['id'] for i in mGENRE_results if i[0]['score'] > threshold]\n",
    "    indexes = [1 if i[0]['score'] > threshold else 0 for i in mGENRE_results]\n",
    "    y_true = list(compress(objects, indexes))\n",
    "    result = [x in certain_out for x in y_true] \n",
    "    accuracy = np.round(sum(result)/len(result), 2)*100\n",
    "    accuracy_400_top_1.append(accuracy)\n",
    "    share = np.round(len(result)/n*100, 2)\n",
    "    share_of_observations_400_top_1.append(share)\n",
    "    \n",
    "    print(\"threshold = \", threshold, \"\\t\",\n",
    "          \"accuracy = \", accuracy, \"%\\t\", \n",
    "          \"number of observations = \", len(result), '\\t',\n",
    "          \"share of observations = \", share, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "plt.xlabel(\"rejection rate, %\", fontsize = 20)\n",
    "plt.ylabel(\"accuracy, %\", fontsize = 20)\n",
    "plt.title(\"mGENRE certainty basing on 400 random samples\")\n",
    "plt.plot(share_of_observations_400_top_1[::-1], accuracy_400_top_1, marker = '.', markersize = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fairseq_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fairseq_model.py\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\"\"\"\n",
    "Base classes for various fairseq models.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from argparse import Namespace\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fairseq import utils\n",
    "from fairseq.data import Dictionary\n",
    "from fairseq.dataclass.utils import (\n",
    "    convert_namespace_to_omegaconf,\n",
    "    gen_parser_from_dataclass,\n",
    ")\n",
    "from fairseq.models import FairseqDecoder, FairseqEncoder\n",
    "from omegaconf import DictConfig\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BaseFairseqModel(nn.Module):\n",
    "    \"\"\"Base class for fairseq models.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._is_generation_fast = False\n",
    "\n",
    "    @classmethod\n",
    "    def add_args(cls, parser):\n",
    "        \"\"\"Add model-specific arguments to the parser.\"\"\"\n",
    "        dc = getattr(cls, \"__dataclass\", None)\n",
    "        if dc is not None:\n",
    "            # do not set defaults so that settings defaults from various architectures still works\n",
    "            gen_parser_from_dataclass(parser, dc(), delete_default=True)\n",
    "\n",
    "    @classmethod\n",
    "    def build_model(cls, args, task):\n",
    "        \"\"\"Build a new model instance.\"\"\"\n",
    "        raise NotImplementedError(\"Model must implement the build_model method\")\n",
    "\n",
    "    def get_targets(self, sample, net_output):\n",
    "        \"\"\"Get targets from either the sample or the net's output.\"\"\"\n",
    "        return sample[\"target\"]\n",
    "\n",
    "    def get_normalized_probs(\n",
    "        self,\n",
    "        net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],\n",
    "        log_probs: bool,\n",
    "        sample: Optional[Dict[str, Tensor]] = None,\n",
    "    ):\n",
    "        \"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"\n",
    "        return self.get_normalized_probs_scriptable(net_output, log_probs, sample)\n",
    "\n",
    "    # TorchScript doesn't support super() method so that the scriptable Subclass\n",
    "    # can't access the base class model in Torchscript.\n",
    "    # Current workaround is to add a helper function with different name and\n",
    "    # call the helper function from scriptable Subclass.\n",
    "    def get_normalized_probs_scriptable(\n",
    "        self,\n",
    "        net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],\n",
    "        log_probs: bool,\n",
    "        sample: Optional[Dict[str, Tensor]] = None,\n",
    "    ):\n",
    "        \"\"\"Scriptable helper function for get_normalized_probs in ~BaseFairseqModel\"\"\"\n",
    "        if hasattr(self, \"decoder\"):\n",
    "            return self.decoder.get_normalized_probs(net_output, log_probs, sample)\n",
    "        elif torch.is_tensor(net_output):\n",
    "            # syntactic sugar for simple models which don't have a decoder\n",
    "            # (e.g., the classification tutorial)\n",
    "            logits = net_output.float()\n",
    "            if log_probs:\n",
    "                return F.log_softmax(logits, dim=-1)\n",
    "            else:\n",
    "                return F.softmax(logits, dim=-1)\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def extract_features(self, *args, **kwargs):\n",
    "        \"\"\"Similar to *forward* but only return features.\"\"\"\n",
    "        return self(*args, **kwargs)\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum length supported by the model.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def load_state_dict(\n",
    "        self,\n",
    "        state_dict,\n",
    "        strict=True,\n",
    "        model_cfg: Optional[DictConfig] = None,\n",
    "        args: Optional[Namespace] = None,\n",
    "    ):\n",
    "        \"\"\"Copies parameters and buffers from *state_dict* into this module and\n",
    "        its descendants.\n",
    "\n",
    "        Overrides the method in :class:`nn.Module`. Compared with that method\n",
    "        this additionally \"upgrades\" *state_dicts* from old checkpoints.\n",
    "        \"\"\"\n",
    "\n",
    "        if model_cfg is None and args is not None:\n",
    "            logger.warn(\"using 'args' is deprecated, please update your code to use dataclass config\")\n",
    "            model_cfg = convert_namespace_to_omegaconf(args).model\n",
    "\n",
    "        self.upgrade_state_dict(state_dict)\n",
    "\n",
    "        from fairseq.checkpoint_utils import prune_state_dict\n",
    "\n",
    "        new_state_dict = prune_state_dict(state_dict, model_cfg)\n",
    "        return super().load_state_dict(new_state_dict, strict)\n",
    "\n",
    "    def upgrade_state_dict(self, state_dict):\n",
    "        \"\"\"Upgrade old state dicts to work with newer code.\"\"\"\n",
    "        self.upgrade_state_dict_named(state_dict, \"\")\n",
    "\n",
    "    def upgrade_state_dict_named(self, state_dict, name):\n",
    "        \"\"\"Upgrade old state dicts to work with newer code.\n",
    "\n",
    "        Args:\n",
    "            state_dict (dict): state dictionary to upgrade, in place\n",
    "            name (str): the state dict key corresponding to the current module\n",
    "        \"\"\"\n",
    "        assert state_dict is not None\n",
    "\n",
    "        def do_upgrade(m, prefix):\n",
    "            if len(prefix) > 0:\n",
    "                prefix += \".\"\n",
    "\n",
    "            for n, c in m.named_children():\n",
    "                name = prefix + n\n",
    "                if hasattr(c, \"upgrade_state_dict_named\"):\n",
    "                    c.upgrade_state_dict_named(state_dict, name)\n",
    "                elif hasattr(c, \"upgrade_state_dict\"):\n",
    "                    c.upgrade_state_dict(state_dict)\n",
    "                do_upgrade(c, name)\n",
    "\n",
    "        do_upgrade(self, name)\n",
    "\n",
    "    def set_num_updates(self, num_updates):\n",
    "        \"\"\"State from trainer to pass along to model at every update.\"\"\"\n",
    "\n",
    "        def _apply(m):\n",
    "            if hasattr(m, \"set_num_updates\") and m != self:\n",
    "                m.set_num_updates(num_updates)\n",
    "\n",
    "        self.apply(_apply)\n",
    "\n",
    "    def prepare_for_inference_(self, cfg: DictConfig):\n",
    "        \"\"\"Prepare model for inference.\"\"\"\n",
    "        kwargs = {}\n",
    "        kwargs[\"beamable_mm_beam_size\"] = (\n",
    "            None\n",
    "            if getattr(cfg.generation, \"no_beamable_mm\", False)\n",
    "            else getattr(cfg.generation, \"beam\", 5)\n",
    "        )\n",
    "        kwargs[\"need_attn\"] = getattr(cfg.generation, \"print_alignment\", False)\n",
    "        if getattr(cfg.generation, \"retain_dropout\", False):\n",
    "            kwargs[\"retain_dropout\"] = cfg.generation.retain_dropout\n",
    "            kwargs[\"retain_dropout_modules\"] = cfg.generation.retain_dropout_modules\n",
    "        self.make_generation_fast_(**kwargs)\n",
    "\n",
    "    def make_generation_fast_(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Legacy entry point to optimize model for faster generation.\n",
    "        Prefer prepare_for_inference_.\n",
    "        \"\"\"\n",
    "        if self._is_generation_fast:\n",
    "            return  # only apply once\n",
    "        self._is_generation_fast = True\n",
    "\n",
    "        # remove weight norm from all modules in the network\n",
    "        def apply_remove_weight_norm(module):\n",
    "            try:\n",
    "                nn.utils.remove_weight_norm(module)\n",
    "            except (AttributeError, ValueError):  # this module didn't have weight norm\n",
    "                return\n",
    "\n",
    "        self.apply(apply_remove_weight_norm)\n",
    "\n",
    "        def apply_make_generation_fast_(module, prefix):\n",
    "            if len(prefix) > 0:\n",
    "                prefix += \".\"\n",
    "\n",
    "            base_func = BaseFairseqModel.make_generation_fast_\n",
    "            for n, m in module.named_modules():\n",
    "                if (\n",
    "                    m != self\n",
    "                    and hasattr(m, \"make_generation_fast_\")\n",
    "                    # don't call this implementation again, e.g., if\n",
    "                    # children modules also inherit from BaseFairseqModel\n",
    "                    and m.make_generation_fast_.__func__ is not base_func\n",
    "                ):\n",
    "                    name = prefix + n\n",
    "                    m.make_generation_fast_(name=name, **kwargs)\n",
    "\n",
    "        apply_make_generation_fast_(self, \"\")\n",
    "\n",
    "        def train(mode=True):\n",
    "#             if mode:\n",
    "#                 raise RuntimeError(\"cannot train after make_generation_fast\")\n",
    "\n",
    "        # this model should no longer be used for training\n",
    "#        self.eval()\n",
    "            self.train()\n",
    "#        self.train = train\n",
    "\n",
    "    def prepare_for_onnx_export_(self, **kwargs):\n",
    "        \"\"\"Make model exportable via ONNX trace.\"\"\"\n",
    "        seen = set()\n",
    "\n",
    "        def apply_prepare_for_onnx_export_(module):\n",
    "            if (\n",
    "                module != self\n",
    "                and hasattr(module, \"prepare_for_onnx_export_\")\n",
    "                and module not in seen\n",
    "            ):\n",
    "                seen.add(module)\n",
    "                module.prepare_for_onnx_export_(**kwargs)\n",
    "\n",
    "        self.apply(apply_prepare_for_onnx_export_)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        model_name_or_path,\n",
    "        checkpoint_file=\"model.pt\",\n",
    "        data_name_or_path=\".\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load a :class:`~fairseq.models.FairseqModel` from a pre-trained model\n",
    "        file. Downloads and caches the pre-trained model file if needed.\n",
    "\n",
    "        The base implementation returns a\n",
    "        :class:`~fairseq.hub_utils.GeneratorHubInterface`, which can be used to\n",
    "        generate translations or sample from language models. The underlying\n",
    "        :class:`~fairseq.models.FairseqModel` can be accessed via the\n",
    "        *generator.models* attribute.\n",
    "\n",
    "        Other models may override this to implement custom hub interfaces.\n",
    "\n",
    "        Args:\n",
    "            model_name_or_path (str): either the name of a pre-trained model to\n",
    "                load or a path/URL to a pre-trained model state dict\n",
    "            checkpoint_file (str, optional): colon-separated list of checkpoint\n",
    "                files in the model archive to ensemble (default: 'model.pt')\n",
    "            data_name_or_path (str, optional): point args.data to the archive\n",
    "                at the given path/URL. Can start with '.' or './' to reuse the\n",
    "                model archive path.\n",
    "        \"\"\"\n",
    "        from fairseq import hub_utils\n",
    "\n",
    "        x = hub_utils.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            checkpoint_file,\n",
    "            data_name_or_path,\n",
    "            archive_map=cls.hub_models(),\n",
    "            **kwargs,\n",
    "        )\n",
    "        logger.info(x[\"args\"])\n",
    "        return hub_utils.GeneratorHubInterface(x[\"args\"], x[\"task\"], x[\"models\"])\n",
    "\n",
    "    @classmethod\n",
    "    def hub_models(cls):\n",
    "        return {}\n",
    "\n",
    "\n",
    "class FairseqEncoderDecoderModel(BaseFairseqModel):\n",
    "    \"\"\"Base class for encoder-decoder models.\n",
    "\n",
    "    Args:\n",
    "        encoder (FairseqEncoder): the encoder\n",
    "        decoder (FairseqDecoder): the decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        assert isinstance(self.encoder, FairseqEncoder)\n",
    "        assert isinstance(self.decoder, FairseqDecoder)\n",
    "\n",
    "    def forward(self, src_tokens, src_lengths, prev_output_tokens, **kwargs):\n",
    "        \"\"\"\n",
    "        Run the forward pass for an encoder-decoder model.\n",
    "\n",
    "        First feed a batch of source tokens through the encoder. Then, feed the\n",
    "        encoder output and previous decoder outputs (i.e., teacher forcing) to\n",
    "        the decoder to produce the next outputs::\n",
    "\n",
    "            encoder_out = self.encoder(src_tokens, src_lengths)\n",
    "            return self.decoder(prev_output_tokens, encoder_out)\n",
    "\n",
    "        Args:\n",
    "            src_tokens (LongTensor): tokens in the source language of shape\n",
    "                `(batch, src_len)`\n",
    "            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n",
    "            prev_output_tokens (LongTensor): previous decoder outputs of shape\n",
    "                `(batch, tgt_len)`, for teacher forcing\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - the decoder's output of shape `(batch, tgt_len, vocab)`\n",
    "                - a dictionary with any model-specific outputs\n",
    "        \"\"\"\n",
    "        encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n",
    "        decoder_out = self.decoder(\n",
    "            prev_output_tokens, encoder_out=encoder_out, **kwargs\n",
    "        )\n",
    "        return decoder_out\n",
    "\n",
    "    def forward_decoder(self, prev_output_tokens, **kwargs):\n",
    "        return self.decoder(prev_output_tokens, **kwargs)\n",
    "\n",
    "    def extract_features(self, src_tokens, src_lengths, prev_output_tokens, **kwargs):\n",
    "        \"\"\"\n",
    "        Similar to *forward* but only return features.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n",
    "                - a dictionary with any model-specific outputs\n",
    "        \"\"\"\n",
    "        encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n",
    "        features = self.decoder.extract_features(\n",
    "            prev_output_tokens, encoder_out=encoder_out, **kwargs\n",
    "        )\n",
    "        return features\n",
    "\n",
    "    def output_layer(self, features, **kwargs):\n",
    "        \"\"\"Project features to the default output size (typically vocabulary size).\"\"\"\n",
    "        return self.decoder.output_layer(features, **kwargs)\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum length supported by the model.\"\"\"\n",
    "        return (self.encoder.max_positions(), self.decoder.max_positions())\n",
    "\n",
    "    def max_decoder_positions(self):\n",
    "        \"\"\"Maximum length supported by the decoder.\"\"\"\n",
    "        return self.decoder.max_positions()\n",
    "\n",
    "\n",
    "class FairseqModel(FairseqEncoderDecoderModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        utils.deprecation_warning(\n",
    "            \"FairseqModel is deprecated, please use FairseqEncoderDecoderModel \"\n",
    "            \"or BaseFairseqModel instead\",\n",
    "            stacklevel=4,\n",
    "        )\n",
    "\n",
    "\n",
    "class FairseqMultiModel(BaseFairseqModel):\n",
    "    \"\"\"Base class for combining multiple encoder-decoder models.\"\"\"\n",
    "\n",
    "    def __init__(self, encoders, decoders):\n",
    "        super().__init__()\n",
    "        assert encoders.keys() == decoders.keys()\n",
    "        self.keys = list(encoders.keys())\n",
    "        for key in self.keys:\n",
    "            assert isinstance(encoders[key], FairseqEncoder)\n",
    "            assert isinstance(decoders[key], FairseqDecoder)\n",
    "\n",
    "        self.models = nn.ModuleDict(\n",
    "            {\n",
    "                key: FairseqEncoderDecoderModel(encoders[key], decoders[key])\n",
    "                for key in self.keys\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_shared_embeddings(\n",
    "        dicts: Dict[str, Dictionary],\n",
    "        langs: List[str],\n",
    "        embed_dim: int,\n",
    "        build_embedding: callable,\n",
    "        pretrained_embed_path: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Helper function to build shared embeddings for a set of languages after\n",
    "        checking that all dicts corresponding to those languages are equivalent.\n",
    "\n",
    "        Args:\n",
    "            dicts: Dict of lang_id to its corresponding Dictionary\n",
    "            langs: languages that we want to share embeddings for\n",
    "            embed_dim: embedding dimension\n",
    "            build_embedding: callable function to actually build the embedding\n",
    "            pretrained_embed_path: Optional path to load pretrained embeddings\n",
    "        \"\"\"\n",
    "        shared_dict = dicts[langs[0]]\n",
    "        if any(dicts[lang] != shared_dict for lang in langs):\n",
    "            raise ValueError(\n",
    "                \"--share-*-embeddings requires a joined dictionary: \"\n",
    "                \"--share-encoder-embeddings requires a joined source \"\n",
    "                \"dictionary, --share-decoder-embeddings requires a joined \"\n",
    "                \"target dictionary, and --share-all-embeddings requires a \"\n",
    "                \"joint source + target dictionary.\"\n",
    "            )\n",
    "        return build_embedding(shared_dict, embed_dim, pretrained_embed_path)\n",
    "\n",
    "    def forward(self, src_tokens, src_lengths, prev_output_tokens, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum length supported by the model.\"\"\"\n",
    "        return {\n",
    "            key: (\n",
    "                self.models[key].encoder.max_positions(),\n",
    "                self.models[key].decoder.max_positions(),\n",
    "            )\n",
    "            for key in self.keys\n",
    "        }\n",
    "\n",
    "    def max_decoder_positions(self):\n",
    "        \"\"\"Maximum length supported by the decoder.\"\"\"\n",
    "        return min(model.decoder.max_positions() for model in self.models.values())\n",
    "\n",
    "    @property\n",
    "    def encoder(self):\n",
    "        return self.models[self.keys[0]].encoder\n",
    "\n",
    "    @property\n",
    "    def decoder(self):\n",
    "        return self.models[self.keys[0]].decoder\n",
    "\n",
    "    def forward_decoder(self, prev_output_tokens, **kwargs):\n",
    "        return self.decoder(prev_output_tokens, **kwargs)\n",
    "\n",
    "    def load_state_dict(\n",
    "        self,\n",
    "        state_dict,\n",
    "        strict=True,\n",
    "        model_cfg=None,\n",
    "        args: Optional[Namespace] = None,\n",
    "    ):\n",
    "        \"\"\"Copies parameters and buffers from *state_dict* into this module and\n",
    "        its descendants.\n",
    "\n",
    "        Overrides the method in :class:`nn.Module`. Compared with that method\n",
    "        this additionally \"upgrades\" *state_dicts* from old checkpoints.\n",
    "        \"\"\"\n",
    "\n",
    "        if model_cfg is None and args is not None:\n",
    "            logger.warn(\"using 'args' is deprecated, please update your code to use dataclass config\")\n",
    "            model_cfg = convert_namespace_to_omegaconf(args).model\n",
    "\n",
    "        self.upgrade_state_dict(state_dict)\n",
    "\n",
    "        from fairseq.checkpoint_utils import prune_state_dict\n",
    "\n",
    "        new_state_dict = prune_state_dict(state_dict, model_cfg)\n",
    "        return super().load_state_dict(new_state_dict, strict)\n",
    "\n",
    "\n",
    "class FairseqLanguageModel(BaseFairseqModel):\n",
    "    \"\"\"Base class for decoder-only models.\n",
    "\n",
    "    Args:\n",
    "        decoder (FairseqDecoder): the decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        assert isinstance(self.decoder, FairseqDecoder)\n",
    "\n",
    "    def forward(self, src_tokens, **kwargs):\n",
    "        \"\"\"\n",
    "        Run the forward pass for a decoder-only model.\n",
    "\n",
    "        Feeds a batch of tokens through the decoder to predict the next tokens.\n",
    "\n",
    "        Args:\n",
    "            src_tokens (LongTensor): tokens on which to condition the decoder,\n",
    "                of shape `(batch, tgt_len)`\n",
    "            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - the decoder's output of shape `(batch, seq_len, vocab)`\n",
    "                - a dictionary with any model-specific outputs\n",
    "        \"\"\"\n",
    "        return self.decoder(src_tokens, **kwargs)\n",
    "\n",
    "    def forward_decoder(self, prev_output_tokens, **kwargs):\n",
    "        return self.decoder(prev_output_tokens, **kwargs)\n",
    "\n",
    "    def extract_features(self, src_tokens, **kwargs):\n",
    "        \"\"\"\n",
    "        Similar to *forward* but only return features.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - the decoder's features of shape `(batch, seq_len, embed_dim)`\n",
    "                - a dictionary with any model-specific outputs\n",
    "        \"\"\"\n",
    "        return self.decoder.extract_features(src_tokens, **kwargs)\n",
    "\n",
    "    def output_layer(self, features, **kwargs):\n",
    "        \"\"\"Project features to the default output size (typically vocabulary size).\"\"\"\n",
    "        return self.decoder.output_layer(features, **kwargs)\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum length supported by the model.\"\"\"\n",
    "        return self.decoder.max_positions()\n",
    "\n",
    "    def max_decoder_positions(self):\n",
    "        \"\"\"Maximum length supported by the decoder.\"\"\"\n",
    "        return self.decoder.max_positions()\n",
    "\n",
    "    @property\n",
    "    def supported_targets(self):\n",
    "        return {\"future\"}\n",
    "\n",
    "\n",
    "class FairseqEncoderModel(BaseFairseqModel):\n",
    "    \"\"\"Base class for encoder-only models.\n",
    "\n",
    "    Args:\n",
    "        encoder (FairseqEncoder): the encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        assert isinstance(self.encoder, FairseqEncoder)\n",
    "\n",
    "    def forward(self, src_tokens, src_lengths, **kwargs):\n",
    "        \"\"\"\n",
    "        Run the forward pass for a encoder-only model.\n",
    "\n",
    "        Feeds a batch of tokens through the encoder to generate features.\n",
    "\n",
    "        Args:\n",
    "            src_tokens (LongTensor): input tokens of shape `(batch, src_len)`\n",
    "            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n",
    "\n",
    "        Returns:\n",
    "            the encoder's output, typically of shape `(batch, src_len, features)`\n",
    "        \"\"\"\n",
    "        return self.encoder(src_tokens, src_lengths, **kwargs)\n",
    "\n",
    "    def get_normalized_probs(self, net_output, log_probs, sample=None):\n",
    "        \"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"\n",
    "        encoder_out = net_output[\"encoder_out\"]\n",
    "        if torch.is_tensor(encoder_out):\n",
    "            logits = encoder_out.float()\n",
    "            if log_probs:\n",
    "                return F.log_softmax(logits, dim=-1)\n",
    "            else:\n",
    "                return F.softmax(logits, dim=-1)\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum length supported by the model.\"\"\"\n",
    "        return self.encoder.max_positions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mv fairseq_model.py fairseq/fairseq/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver version: \u001b[1m495.29.05\u001b[m\r\n",
      "------------------- \u001b[1mDevice 0\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 3090\u001b[m\r\n",
      "Memory usage: 24237MiB / 24268MiB\r\n",
      "Temperature: \u001b[92m24C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n",
      "------------------- \u001b[1mDevice 1\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 3090\u001b[m\r\n",
      "Memory usage: 24250MiB / 24268MiB\r\n",
      "Temperature: \u001b[92m65C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n",
      "------------------- \u001b[1mDevice 2\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 3090\u001b[m\r\n",
      "Memory usage: 23173MiB / 24268MiB\r\n",
      "Temperature: \u001b[92m20C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n",
      "------------------- \u001b[1mDevice 3\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 2080 Ti\u001b[m\r\n",
      "Memory usage:  9471MiB / 11019MiB\r\n",
      "Temperature: \u001b[92m21C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n",
      "------------------- \u001b[1mDevice 4\u001b[m -------------------\r\n",
      "Name: \u001b[1mNVIDIA GeForce RTX 2080 Ti\u001b[m\r\n",
      "Memory usage: 11014MiB / 11019MiB\r\n",
      "Temperature: \u001b[92m23C\u001b[m\r\n",
      "\u001b[91mRunning processes not found\u001b[m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-cdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 1; 23.70 GiB total capacity; 3.76 GiB already allocated; 17.69 MiB free; 3.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-98cd6efa6297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# generate Wikipedia titles and language IDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_mGENRE_mcdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmGENRE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fairseq_multilingual_entity_disambiguation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_mGENRE_mcdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 1; 23.70 GiB total capacity; 3.76 GiB already allocated; 17.69 MiB free; 3.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# generate Wikipedia titles and language IDs\n",
    "model_mGENRE_mcdropout = mGENRE.from_pretrained(\"fairseq_multilingual_entity_disambiguation\").train()\n",
    "model_mGENRE_mcdropout.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mGENRE_mcdropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fairseq.modules.fairseq_dropout import FairseqDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FairseqDropout()"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FairseqDropout(p = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mGENRE_mcdropout.models[0].encoder.dropout_module.forward(torch.ones(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER + MEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Who is Pushkin?\",\n",
    "             \"Is Pushkinskaya metro station located in Saint Petersburg?\",\n",
    "             \"Is Saint Petersburg located on Mississipi?\",\n",
    "             \n",
    "             \"Did Stephen Hawking discover theory of relativity?\",\n",
    "             \n",
    "             \"When Caesar was an imperor of the Roman empire?\",\n",
    "             \"How many times have people won the jackpot at Caesar?\",\n",
    "             \"Is Caesar a part of the Italian national cuisine?\",\n",
    "             \n",
    "             \"What did Tsiolkovsky invent?\",\n",
    "             \n",
    "             \"Was George Washington the first American President?\",\n",
    "             \"Does Denver State border Washington State?\",\n",
    "             \"How long Alexander Ovechkin play for Washington Capitals?\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NER:\n",
    "    def __init__(self, \n",
    "                 #text, \n",
    "                 model = \"Babelscape/wikineural-multilingual-ner\",\n",
    "                 tokenizer = \"Babelscape/wikineural-multilingual-ner\"):\n",
    "        \n",
    "        #self.text = text\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def set_pipeline(self):   \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(self.model)\n",
    "        self.nlp = pipeline(\"ner\",\n",
    "                            model = self.model,\n",
    "                            tokenizer = self.tokenizer)\n",
    "        \n",
    "    def receive_text(self, text):\n",
    "        self.text = text\n",
    "    \n",
    "    def receive_words(self):\n",
    "        self.text = re.sub('[!@#$?,]', '', self.text)\n",
    "        out = re.split(r' ', self.text)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def receive_enitity(self):\n",
    "        ner_results = self.nlp(self.text)\n",
    "        final = []\n",
    "        result = [i[\"word\"] for i in ner_results]\n",
    "        for elem in result:    \n",
    "            if elem[:2] == \"##\":\n",
    "                final[-1] = final[-1] + elem[2:]\n",
    "            else:\n",
    "                final.append(elem)\n",
    "        return final\n",
    "    \n",
    "    def text_with_marked_entities(self):\n",
    "        output = \"\"\n",
    "        entities = self.receive_enitity()\n",
    "        list_of_words = self.receive_words()\n",
    "        for word in list_of_words:\n",
    "            if word not in entities:\n",
    "                output = output + word + \" \"\n",
    "            else:\n",
    "                output = output + \"[START] \" + word +  \" [END] \"\n",
    "        output = re.sub(\"\\[END\\] \\[START\\] \", \"\", output)\n",
    "        output += \"?\"\n",
    "        return output\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How many times have people won the jackpot at [START] Caesar [END] ?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ner = NER(text = \"How many times have people won the jackpot at Caesar?\")\n",
    "\n",
    "# initialize class\n",
    "ner = NER()\n",
    "\n",
    "# establish pipeline\n",
    "ner.set_pipeline()\n",
    "\n",
    "# load text\n",
    "ner.receive_text(text = \"How many times have people won the jackpot at Caesar?\")\n",
    "\n",
    "# receive text with marked entities\n",
    "output = ner.text_with_marked_entities()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial question: \n",
      " Who is Pushkin?\n",
      "NER over initial question: \n",
      " Who is [START] Pushkin [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Is Pushkinskaya metro station located in Saint Petersburg?\n",
      "NER over initial question: \n",
      " Is [START] Pushkinskaya [END] metro station located in [START] Saint Petersburg [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Is Saint Petersburg located on Mississipi?\n",
      "NER over initial question: \n",
      " Is [START] Saint Petersburg [END] located on [START] Mississipi [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Did Stephen Hawking discover theory of relativity?\n",
      "NER over initial question: \n",
      " Did [START] Stephen Hawking [END] discover theory of relativity ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " When Caesar was an imperor of the Roman empire?\n",
      "NER over initial question: \n",
      " When [START] Caesar [END] was an imperor of the [START] Roman [END] empire ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " How many times have people won the jackpot at Caesar?\n",
      "NER over initial question: \n",
      " How many times have people won the jackpot at [START] Caesar [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Is Caesar a part of the Italian national cuisine?\n",
      "NER over initial question: \n",
      " Is [START] Caesar [END] a part of the [START] Italian [END] national cuisine ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What did Tsiolkovsky invent?\n",
      "NER over initial question: \n",
      " What did [START] Tsiolkovsky [END] invent ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Was George Washington the first American President?\n",
      "NER over initial question: \n",
      " Was [START] George Washington [END] the first [START] American [END] President ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Does Denver State border Washington State?\n",
      "NER over initial question: \n",
      " Does [START] Denver State [END] border [START] Washington State [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " How long Alexander Ovechkin play for Washington Capitals?\n",
      "NER over initial question: \n",
      " How long [START] Alexander Ovechkin [END] play for [START] Washington Capitals [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "changed_texts = []\n",
    "for question in questions:\n",
    "    print(\"Initial question: \\n\", question)\n",
    "    \n",
    "    ner.receive_text(text = question)\n",
    "    \n",
    "    new_text = ner.text_with_marked_entities()\n",
    "    changed_texts.append(new_text)\n",
    "    print(\"NER over initial question: \\n\", new_text)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who is [START] Pushkin [END] ?',\n",
       " 'Is [START] Pushkinskaya [END] metro station located in [START] Saint Petersburg [END] ?',\n",
       " 'Is [START] Saint Petersburg [END] located on [START] Mississipi [END] ?',\n",
       " 'Did [START] Stephen Hawking [END] discover theory of relativity ?',\n",
       " 'When [START] Caesar [END] was an imperor of the [START] Roman [END] empire ?',\n",
       " 'How many times have people won the jackpot at [START] Caesar [END] ?',\n",
       " 'Is [START] Caesar [END] a part of the [START] Italian [END] national cuisine ?',\n",
       " 'What did [START] Tsiolkovsky [END] invent ?',\n",
       " 'Was [START] George Washington [END] the first [START] American [END] President ?',\n",
       " 'Does [START] Denver State [END] border [START] Washington State [END] ?',\n",
       " 'How long [START] Alexander Ovechkin [END] play for [START] Washington Capitals [END] ?']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'id': 'Q7200',\n",
       "   'texts': ['Alexander Pushkin >> en',\n",
       "    'Aleksandr Pushkin >> en',\n",
       "    'Alexandr Pushkin >> en',\n",
       "    'Pushkin >> en',\n",
       "    'Aleksandr Pushkin >> es'],\n",
       "   'scores': tensor([-0.0850, -0.7231, -0.8690, -0.9665, -0.9766], device='cuda:4'),\n",
       "   'score': tensor(0.2237, device='cuda:4')}]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [changed_texts[0]]\n",
    "model_mGENRE.sample(\n",
    "    sentences,\n",
    "    beam = 5,\n",
    "    prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
    "        e for e in trie.get(sent.tolist())\n",
    "        if e < len(model_mGENRE.task.target_dictionary)\n",
    "    ],\n",
    "    text_to_id=lambda x: max(lang_title2wikidataID[tuple(reversed(x.split(\" >> \")))], key=lambda y: int(y[1:])),\n",
    "    marginalize=True,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'id': 'Q7200', 'texts': ['Alexander Pushkin >> en', 'Aleksandr Pushkin >> en', 'Alexandr Pushkin >> en', 'Pushkin >> en', 'Aleksandr Pushkin >> es'], 'scores': tensor([-0.0850, -0.7231, -0.8690, -0.9665, -0.9766], device='cuda:4'), 'score': tensor(0.2237, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "[[{'id': 'Q2079447', 'texts': ['Pushkinskaya (Saint Petersburg Metro) >> en'], 'scores': tensor([-0.1192], device='cuda:4'), 'score': tensor(-0.3955, device='cuda:4')}, {'id': 'Q1627587', 'texts': ['Pushkinskaya (Moscow Metro) >> en'], 'scores': tensor([-0.2643], device='cuda:4'), 'score': tensor(-0.9156, device='cuda:4')}, {'id': 'Q7261815', 'texts': ['Pushkinskaya >> en'], 'scores': tensor([-0.8117], device='cuda:4'), 'score': tensor(-1.9882, device='cuda:4')}, {'id': 'Q2444846', 'texts': ['Pushkinskaya (Minsk Metro) >> en'], 'scores': tensor([-0.6415], device='cuda:4'), 'score': tensor(-2.1277, device='cuda:4')}, {'id': 'Q250225', 'texts': ['Saint Petersburg Metro >> en'], 'scores': tensor([-0.9126], device='cuda:4'), 'score': tensor(-2.2355, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "[[{'id': 'Q1494', 'texts': ['Mississippi >> en', 'Mississippi (state) >> en'], 'scores': tensor([-0.3802, -0.6505], device='cuda:4'), 'score': tensor(-0.4364, device='cuda:4')}, {'id': 'Q1497', 'texts': ['Mississippi River >> en'], 'scores': tensor([-0.3112], device='cuda:4'), 'score': tensor(-0.6958, device='cuda:4')}, {'id': 'Q656', 'texts': ['Saint Petersburg >> en'], 'scores': tensor([-0.7314], device='cuda:4'), 'score': tensor(-1.6355, device='cuda:4')}, {'id': 'Q49236', 'texts': ['Saint Petersburg, Florida >> en'], 'scores': tensor([-0.6712], device='cuda:4'), 'score': tensor(-1.7759, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "[[{'id': 'Q17714', 'texts': ['Stephen Hawking >> en', 'Stephen Hawking >> id', 'Stephen Hawking >> tr', 'Stephen Hawking >> ms', 'Stephen Hawking >> vi'], 'scores': tensor([-0.0850, -1.4570, -1.4586, -1.4961, -1.5068], device='cuda:4'), 'score': tensor(-0.0270, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "[[{'id': 'Q2277', 'texts': ['Roman Empire >> en'], 'scores': tensor([-0.3542], device='cuda:4'), 'score': tensor(-0.7921, device='cuda:4')}, {'id': 'Q188751', 'texts': ['Caesar (title) >> en'], 'scores': tensor([-0.4272], device='cuda:4'), 'score': tensor(-1.1303, device='cuda:4')}, {'id': 'Q1747689', 'texts': ['Ancient Rome >> en'], 'scores': tensor([-0.5286], device='cuda:4'), 'score': tensor(-1.2948, device='cuda:4')}, {'id': 'Q1048', 'texts': ['Julius Caesar >> en'], 'scores': tensor([-0.6120], device='cuda:4'), 'score': tensor(-1.3685, device='cuda:4')}, {'id': 'Q220', 'texts': ['Rome >> en'], 'scores': tensor([-1.0506], device='cuda:4'), 'score': tensor(-2.1013, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "[[{'id': 'Q1048', 'texts': ['Julius Caesar >> en', 'Caesar >> en'], 'scores': tensor([-0.3834, -0.8819], device='cuda:4'), 'score': tensor(-0.5181, device='cuda:4')}, {'id': 'Q27995879', 'texts': ['Gaius Iulius Caesar >> en', 'Gaius Julius Caesar >> en'], 'scores': tensor([-0.4686, -0.6315], device='cuda:4'), 'score': tensor(-0.7902, device='cuda:4')}, {'id': 'Q188751', 'texts': ['Caesar (title) >> en'], 'scores': tensor([-0.5184], device='cuda:4'), 'score': tensor(-1.3716, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "[[{'id': 'Q38', 'texts': ['Italy >> en'], 'scores': tensor([-0.5365], device='cuda:4'), 'score': tensor(-1.0730, device='cuda:4')}, {'id': 'Q192786', 'texts': ['Italian cuisine >> en'], 'scores': tensor([-0.5653], device='cuda:4'), 'score': tensor(-1.2640, device='cuda:4')}, {'id': 'Q188751', 'texts': ['Caesar (title) >> en'], 'scores': tensor([-0.5390], device='cuda:4'), 'score': tensor(-1.4261, device='cuda:4')}, {'id': 'Q50001', 'texts': ['Italians >> en'], 'scores': tensor([-0.7396], device='cuda:4'), 'score': tensor(-1.6538, device='cuda:4')}, {'id': 'Q220', 'texts': ['Rome >> en'], 'scores': tensor([-0.9879], device='cuda:4'), 'score': tensor(-1.9758, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "[[{'id': 'Q41239', 'texts': ['Konstantin Tsiolkovsky >> en', 'Konstantin Eduardovich Tsiolkovsky >> en', 'Konstantin Eduardovich Tsiolkovsky >> vi', 'Konstantin Eduardovich Tsiolkovsky >> pt', 'Konstantin Tsiolkovski >> tr'], 'scores': tensor([-0.1853, -0.2997, -0.3104, -0.5162, -0.6527], device='cuda:4'), 'score': tensor(0.4839, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "[[{'id': 'Q23', 'texts': ['George Washington >> en'], 'scores': tensor([-0.0976], device='cuda:4'), 'score': tensor(-0.2183, device='cuda:4')}, {'id': 'Q35073', 'texts': ['List of presidents of the United States >> en'], 'scores': tensor([-0.8111], device='cuda:4'), 'score': tensor(-2.6900, device='cuda:4')}, {'id': 'Q2824580', 'texts': ['Presidency of George Washington >> en'], 'scores': tensor([-1.0284], device='cuda:4'), 'score': tensor(-2.9088, device='cuda:4')}, {'id': 'Q30', 'texts': ['United States >> en'], 'scores': tensor([-1.3157], device='cuda:4'), 'score': tensor(-2.9420, device='cuda:4')}, {'id': 'Q1223', 'texts': ['Washington (state) >> en'], 'scores': tensor([-1.1492], device='cuda:4'), 'score': tensor(-3.0405, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "[[{'id': 'Q613736', 'texts': ['Denver State University >> en'], 'scores': tensor([-0.2005], device='cuda:4'), 'score': tensor(-0.4910, device='cuda:4')}, {'id': 'Q2998740', 'texts': ['Washington State Cougars >> en'], 'scores': tensor([-0.7815], device='cuda:4'), 'score': tensor(-2.2105, device='cuda:4')}, {'id': 'Q7972235', 'texts': ['Washington State Cougars football >> en'], 'scores': tensor([-0.7548], device='cuda:4'), 'score': tensor(-2.2644, device='cuda:4')}, {'id': 'Q5148746', 'texts': ['Colorado Buffaloes football >> en'], 'scores': tensor([-0.9525], device='cuda:4'), 'score': tensor(-2.5201, device='cuda:4')}, {'id': 'Q16554', 'texts': ['Denver >> en'], 'scores': tensor([-1.3666], device='cuda:4'), 'score': tensor(-2.7331, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "[[{'id': 'Q311374', 'texts': ['Alexander Ovechkin >> en', 'Aleksandr Ovetjkin >> sv', 'Aleksandr Ovechkin >> en', 'Alexander Ovechkin >> de'], 'scores': tensor([-0.2053, -1.1328, -1.1402, -1.4095], device='cuda:4'), 'score': tensor(-0.3886, device='cuda:4')}, {'id': 'Q204627', 'texts': ['Washington Capitals >> en'], 'scores': tensor([-0.6224], device='cuda:4'), 'score': tensor(-1.5245, device='cuda:4')}]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(changed_texts)):\n",
    "    sentences = [changed_texts[i]]\n",
    "    print(model_mGENRE.sample(\n",
    "        sentences,\n",
    "        beam = 5,\n",
    "        prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
    "            e for e in trie.get(sent.tolist())\n",
    "            if e < len(model_mGENRE.task.target_dictionary)\n",
    "        ],\n",
    "        text_to_id=lambda x: max(lang_title2wikidataID[tuple(reversed(x.split(\" >> \")))], key=lambda y: int(y[1:])),\n",
    "        marginalize=True,\n",
    "        verbose = True\n",
    "    ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[\"Q16330302\", \"P19\", \"Q1524\", \"what city was alex golfis born in\"],\n",
    "        [\"Q16225521\", \"R58\", \"Q192979\", \"what film is by the writer phil hay?\"],\n",
    "        [\"Q7358590\", \"P20\", \"Q1637790\", \"Where did roger marquis die\"],\n",
    "        [\"Q154335\", \"P509\", \"Q12152\", \"what was the cause of death of yves klein\"],\n",
    "        [\"Q1761\", \"R19\", \"Q6768445\", \"Which equestrian was born in dublin?\"],\n",
    "        [\"Q11272426\", \"R136\", \"Q5533291\", \"What is a tv action show?\"],\n",
    "        [\"Q2581168\", \"P172\", \"Q810714\", \"what's akbar tandjung's ethnicity\"],\n",
    "        [\"Q2747238\", \"P413\", \"Q5059480\", \"What position does carlos gomez play?\"],\n",
    "        [\"Q62498\", \"P21\", \"Q6581097\", \"how does engelbert zaschka identify\"],\n",
    "        [\"Q182485\", \"P413\", \"Q1143358\", \"what position does pee wee reese play in baseball\"],\n",
    "        [\"Q12152\", \"R509\", \"Q6371569\", \"Which Swiss conductor's cause of death is myocardial infarction?\"],\n",
    "        [\"Q1176417\", \"P136\", \"Q37073\", \"what type of music does david ruffin play\"],\n",
    "        [\"Q7123909\", \"P20\", \"Q3130\", \"where was padraic mcguinness's place of death\"],\n",
    "        [\"Q1189775\", \"P364\", \"Q1860\", \"what language is viper in\"],\n",
    "        [\"Q276817\", \"P737\", \"Q169566\", \"Who influenced michael mcdowell?\"],\n",
    "        [\"Q472382\", \"P19\", \"Q23051\", \"what is the place of birth of sam edwards?\"],\n",
    "        [\"Q7443093\", \"P710\", \"Q214102\", \"which military was involved in the second battle of fort fisher\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd963b1d1a76404f9293a26dc44cbab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=499.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd0599bb7da442789f0afdf844e94ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=824793.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0699cfeb9b4156ac5339a99d3bc125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=2642361.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bf91df90b941c0aad0c1a1bef7fafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e81a8eb8ab3453a8d212293417b09bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=393.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310a063a4aeb4161834250fd16eb0258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=238027683.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/araelectra-base-generator were not used when initializing ElectraForTokenClassification: ['generator_predictions.LayerNorm.weight', 'generator_predictions.LayerNorm.bias', 'generator_predictions.dense.weight', 'generator_predictions.dense.bias', 'generator_lm_head.weight', 'generator_lm_head.bias']\n",
      "- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at aubmindlab/araelectra-base-generator and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial question: \n",
      " what city was alex golfis born in\n",
      "NER over initial question: \n",
      " [START] what city was alex golfis born in [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what film is by the writer phil hay?\n",
      "NER over initial question: \n",
      " [START] what film is by the writer phil hay [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where did roger marquis die\n",
      "NER over initial question: \n",
      " [START] Where did roger marquis die [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what was the cause of death of yves klein\n",
      "NER over initial question: \n",
      " [START] what was the cause of death of yves klein [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which equestrian was born in dublin?\n",
      "NER over initial question: \n",
      " [START] Which equestrian was born in dublin [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is a tv action show?\n",
      "NER over initial question: \n",
      " [START] What is a tv action show [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what's akbar tandjung's ethnicity\n",
      "NER over initial question: \n",
      " what's [START] akbar [END] tandjung's [START] ethnicity [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What position does carlos gomez play?\n",
      "NER over initial question: \n",
      " [START] What position does carlos gomez play [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " how does engelbert zaschka identify\n",
      "NER over initial question: \n",
      " [START] how does engelbert zaschka identify [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what position does pee wee reese play in baseball\n",
      "NER over initial question: \n",
      " [START] what position does pee wee reese play in baseball [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which Swiss conductor's cause of death is myocardial infarction?\n",
      "NER over initial question: \n",
      " [START] Which Swiss [END] conductor's [START] cause of death is myocardial infarction [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what type of music does david ruffin play\n",
      "NER over initial question: \n",
      " [START] what type of music does david ruffin play [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where was padraic mcguinness's place of death\n",
      "NER over initial question: \n",
      " [START] where was padraic [END] mcguinness's [START] place of death [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what language is viper in\n",
      "NER over initial question: \n",
      " [START] what language is viper in [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who influenced michael mcdowell?\n",
      "NER over initial question: \n",
      " [START] Who influenced michael mcdowell [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is the place of birth of sam edwards?\n",
      "NER over initial question: \n",
      " [START] what is the place of birth of sam edwards [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which military was involved in the second battle of fort fisher\n",
      "NER over initial question: \n",
      " [START] which military was involved in the second battle of fort fisher [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "# initialize class\n",
    "method = \"aubmindlab/araelectra-base-generator\"\n",
    "\n",
    "ner = NER(model = method,\n",
    "          tokenizer = method)\n",
    "\n",
    "# establish pipeline\n",
    "ner.set_pipeline()\n",
    "\n",
    "\n",
    "# NER\n",
    "changed_texts = []\n",
    "for question in data:\n",
    "    print(\"Initial question: \\n\", question[3])\n",
    "    \n",
    "    # load text\n",
    "    ner.receive_text(text = question[3])\n",
    "    \n",
    "    # receive text with marked entities\n",
    "    new_text = ner.text_with_marked_entities()\n",
    "    changed_texts.append(new_text)\n",
    "    print(\"NER over initial question: \\n\", new_text)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting deeppavlov\n",
      "  Downloading deeppavlov-0.17.4-py3-none-any.whl (878 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m878.6/878.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting click==7.1.2\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: prometheus-client==0.7.1 in /usr/lib/python3/dist-packages (from deeppavlov) (0.7.1)\n",
      "Requirement already satisfied: protobuf<4 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (3.19.4)\n",
      "Collecting ruamel.yaml==0.15.100\n",
      "  Downloading ruamel.yaml-0.15.100.tar.gz (318 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.6/318.6 kB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nltk==3.4.5 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (3.4.5)\n",
      "Collecting pydantic==1.3\n",
      "  Downloading pydantic-1.3-cp38-cp38-manylinux2010_x86_64.whl (9.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting uvloop==0.14.0\n",
      "  Downloading uvloop-0.14.0-cp38-cp38-manylinux2010_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting overrides==2.7.0\n",
      "  Downloading overrides-2.7.0.tar.gz (4.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-learn==0.21.2\n",
      "  Downloading scikit-learn-0.21.2.tar.gz (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pandas==0.25.3\n",
      "  Downloading pandas-0.25.3-cp38-cp38-manylinux1_x86_64.whl (10.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting uvicorn==0.11.7\n",
      "  Downloading uvicorn-0.11.7-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pymorphy2-dicts-ru\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting h5py==2.10.0\n",
      "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting rusenttokenize==0.0.5\n",
      "  Downloading rusenttokenize-0.0.5-py3-none-any.whl (10 kB)\n",
      "Collecting filelock==3.0.12\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting pytz==2019.1\n",
      "  Downloading pytz-2019.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.9/510.9 kB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sacremoses==0.0.35\n",
      "  Downloading sacremoses-0.0.35.tar.gz (859 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.8/859.8 kB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tqdm==4.62.0\n",
      "  Downloading tqdm-4.62.0-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyopenssl==22.0.0\n",
      "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp38-cp38-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pymorphy2==0.8\n",
      "  Downloading pymorphy2-0.8-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy==1.18.0\n",
      "  Downloading numpy-1.18.0-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests==2.22.0\n",
      "  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi==0.47.1\n",
      "  Downloading fastapi-0.47.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aio-pika==6.4.1\n",
      "  Downloading aio_pika-6.4.1-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytelegrambotapi==3.6.7\n",
      "  Downloading pyTelegramBotAPI-3.6.7.tar.gz (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: yarl in /home/petrakov/.local/lib/python3.8/site-packages (from aio-pika==6.4.1->deeppavlov) (1.7.2)\n",
      "Collecting aiormq<4,>=3.2.0\n",
      "  Downloading aiormq-3.3.1-py3-none-any.whl (28 kB)\n",
      "Collecting starlette<=0.12.9,>=0.12.9\n",
      "  Downloading starlette-0.12.9.tar.gz (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from h5py==2.10.0->deeppavlov) (1.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/petrakov/.local/lib/python3.8/site-packages (from pandas==0.25.3->deeppavlov) (2.8.2)\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dawg-python>=0.7\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/lib/python3/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
      "Collecting cryptography>=35.0\n",
      "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests==2.22.0->deeppavlov) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests==2.22.0->deeppavlov) (2.8)\n",
      "Requirement already satisfied: joblib in /home/petrakov/.local/lib/python3.8/site-packages (from sacremoses==0.0.35->deeppavlov) (1.1.0)\n",
      "Collecting websockets==8.*\n",
      "  Downloading websockets-8.1-cp38-cp38-manylinux2010_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httptools==0.1.*\n",
      "  Downloading httptools-0.1.2-cp38-cp38-manylinux1_x86_64.whl (229 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.6/229.6 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.10,>=0.8\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pamqp==2.3.0\n",
      "  Downloading pamqp-2.3.0-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/petrakov/.local/lib/python3.8/site-packages (from cryptography>=35.0->pyopenssl==22.0.0->deeppavlov) (1.15.0)\n",
      "Requirement already satisfied: multidict>=4.0 in /home/petrakov/.local/lib/python3.8/site-packages (from yarl->aio-pika==6.4.1->deeppavlov) (6.0.2)\n",
      "Requirement already satisfied: pycparser in /home/petrakov/.local/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=35.0->pyopenssl==22.0.0->deeppavlov) (2.21)\n",
      "Building wheels for collected packages: overrides, pytelegrambotapi, ruamel.yaml, sacremoses, scikit-learn, starlette\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for overrides: filename=overrides-2.7.0-py3-none-any.whl size=5604 sha256=e9ab6dd1ca209a5ca1472a18435361e10d53f0d21a8c095aa725fdc2b3f03f95\n",
      "  Stored in directory: /home/petrakov/.cache/pip/wheels/ed/eb/2c/78b16373a55a26e8e24511c7e61b212184c46cdd69abe8a9a1\n",
      "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-py3-none-any.whl size=47178 sha256=c123a96c2e397dae95fcdeaea59bee1744bce0e36598a2749c84d2ff6d13684b\n",
      "  Stored in directory: /home/petrakov/.cache/pip/wheels/06/86/1d/a5a0eec2da47544847c30c147d852ef18551d4f500df18fd20\n",
      "  Building wheel for ruamel.yaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ruamel.yaml: filename=ruamel.yaml-0.15.100-cp38-cp38-linux_x86_64.whl size=864634 sha256=fc11c35250c1cf70e84247e9d6d0c69ca6f2e2ad0c42c6bf3fee89be95925837\n",
      "  Stored in directory: /home/petrakov/.cache/pip/wheels/83/bb/c8/84a46029883fd202fda2e3d0a579eb30641fecb325b9907fb0\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.35-py3-none-any.whl size=883989 sha256=b58b14ba0ac38607d822c701e04593eab6a4e3da0e86d970e5a9ce1c4000155f\n",
      "  Stored in directory: /home/petrakov/.cache/pip/wheels/c4/df/30/3d6c623db99d503dcdbae1f686953b7c1a0754d8a658dc0845\n",
      "  Building wheel for scikit-learn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scikit-learn: filename=scikit_learn-0.21.2-cp38-cp38-linux_x86_64.whl size=24209683 sha256=6ddbffe69f3363f34a1003f3ab949b77104fba60578b984ff0c7b0f85e261358\n",
      "  Stored in directory: /home/petrakov/.cache/pip/wheels/74/11/b6/c7a521de4f5756beb18d8a02a784ed4c2c330353fdf361dbdc\n",
      "  Building wheel for starlette (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for starlette: filename=starlette-0.12.9-py3-none-any.whl size=57252 sha256=def2046ee266626e6d3d020b9d1144c2c18e5575d22f60d35cdb3bd0437c1131\n",
      "  Stored in directory: /home/petrakov/.cache/pip/wheels/8b/55/68/b64af029c0a3ace9f1ed8b1ae0834560584579f8963b15dbab\n",
      "Successfully built overrides pytelegrambotapi ruamel.yaml sacremoses scikit-learn starlette\n",
      "Installing collected packages: uvloop, rusenttokenize, ruamel.yaml, pytz, pymorphy2-dicts-ru, pymorphy2-dicts, pamqp, overrides, httptools, h11, filelock, dawg-python, websockets, urllib3, tqdm, starlette, pymorphy2, pydantic, numpy, Cython, click, uvicorn, scipy, sacremoses, requests, pandas, h5py, fastapi, cryptography, aiormq, scikit-learn, pytelegrambotapi, pyopenssl, aio-pika, deeppavlov\n",
      "  Attempting uninstall: uvloop\n",
      "    Found existing installation: uvloop 0.16.0\n",
      "    Uninstalling uvloop-0.16.0:\n",
      "      Successfully uninstalled uvloop-0.16.0\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2022.1\n",
      "    Uninstalling pytz-2022.1:\n",
      "      Successfully uninstalled pytz-2022.1\n",
      "  Attempting uninstall: overrides\n",
      "    Found existing installation: overrides 3.1.0\n",
      "    Uninstalling overrides-3.1.0:\n",
      "      Successfully uninstalled overrides-3.1.0\n",
      "  Attempting uninstall: httptools\n",
      "    Found existing installation: httptools 0.4.0\n",
      "    Uninstalling httptools-0.4.0:\n",
      "      Successfully uninstalled httptools-0.4.0\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.13.0\n",
      "    Uninstalling h11-0.13.0:\n",
      "      Successfully uninstalled h11-0.13.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.7.1\n",
      "    Uninstalling filelock-3.7.1:\n",
      "      Successfully uninstalled filelock-3.7.1\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 10.3\n",
      "    Uninstalling websockets-10.3:\n",
      "      Successfully uninstalled websockets-10.3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.9\n",
      "    Uninstalling urllib3-1.26.9:\n",
      "      Successfully uninstalled urllib3-1.26.9\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.50.2\n",
      "    Uninstalling tqdm-4.50.2:\n",
      "      Successfully uninstalled tqdm-4.50.2\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.19.1\n",
      "    Uninstalling starlette-0.19.1:\n",
      "      Successfully uninstalled starlette-0.19.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.1\n",
      "    Uninstalling pydantic-1.9.1:\n",
      "      Successfully uninstalled pydantic-1.9.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.1\n",
      "    Uninstalling numpy-1.23.1:\n",
      "      Successfully uninstalled numpy-1.23.1\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.30\n",
      "    Uninstalling Cython-0.29.30:\n",
      "      Successfully uninstalled Cython-0.29.30\n",
      "  Attempting uninstall: uvicorn\n",
      "    Found existing installation: uvicorn 0.17.6\n",
      "    Uninstalling uvicorn-0.17.6:\n",
      "      Successfully uninstalled uvicorn-0.17.6\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.3\n",
      "    Uninstalling scipy-1.7.3:\n",
      "      Successfully uninstalled scipy-1.7.3\n",
      "  Attempting uninstall: sacremoses\n",
      "    Found existing installation: sacremoses 0.0.53\n",
      "    Uninstalling sacremoses-0.0.53:\n",
      "      Successfully uninstalled sacremoses-0.0.53\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.3\n",
      "    Uninstalling pandas-1.1.3:\n",
      "      Successfully uninstalled pandas-1.1.3\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.78.0\n",
      "    Uninstalling fastapi-0.78.0:\n",
      "      Successfully uninstalled fastapi-0.78.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.1\n",
      "    Uninstalling scikit-learn-0.24.1:\n",
      "      Successfully uninstalled scikit-learn-0.24.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\n",
      "pytorch-lightning 1.6.4 requires torch>=1.8.*, but you have torch 1.7.0 which is incompatible.\n",
      "lightning 20220621 requires torch<=1.11.0,>=1.9.*, but you have torch 1.7.0 which is incompatible.\n",
      "lightning-bolts 0.6.0.dev0 requires torch>=1.9.*, but you have torch 1.7.0 which is incompatible.\n",
      "konoha 4.6.5 requires overrides<4.0.0,>=3.0.0, but you have overrides 2.7.0 which is incompatible.\n",
      "konoha 4.6.5 requires requests<3.0.0,>=2.25.1, but you have requests 2.22.0 which is incompatible.\n",
      "flair 0.6.1 requires scikit-learn>=0.21.3, but you have scikit-learn 0.21.2 which is incompatible.\n",
      "datasets 2.3.2 requires tqdm>=4.62.1, but you have tqdm 4.62.0 which is incompatible.\n",
      "aiobotocore 2.1.2 requires botocore<1.23.25,>=1.23.24, but you have botocore 1.27.22 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.3.1 click-7.1.2 cryptography-37.0.4 dawg-python-0.7.2 deeppavlov-0.17.4 fastapi-0.47.1 filelock-3.0.12 h11-0.9.0 h5py-2.10.0 httptools-0.1.2 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pandas-0.25.3 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.417127.4579844 pyopenssl-22.0.0 pytelegrambotapi-3.6.7 pytz-2019.1 requests-2.22.0 ruamel.yaml-0.15.100 rusenttokenize-0.0.5 sacremoses-0.0.35 scikit-learn-0.21.2 scipy-1.4.1 starlette-0.12.9 tqdm-4.62.0 urllib3-1.25.11 uvicorn-0.11.7 uvloop-0.14.0 websockets-8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deeppavlov'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-41e561f14540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeppavlov\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mner_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner_ontonotes_bert_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#ner_model(['World Curling Championship will be held in Antananarivo'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deeppavlov'"
     ]
    }
   ],
   "source": [
    "from deeppavlov import configs, build_model\n",
    "\n",
    "ner_model = build_model(configs.ner.ner_ontonotes_bert_mult, download=True)\n",
    "#ner_model(['World Curling Championship will be held in Antananarivo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what city was alex golfis born in\n",
      "\n",
      "\n",
      "[[{'id': 'Q3561', 'texts': ['Algiers >> en'], 'scores': tensor([-0.7752], device='cuda:4'), 'score': tensor(-1.8988, device='cuda:4')}, {'id': 'Q994', 'texts': ['Tiflis >> en', 'Tbilisi >> en'], 'scores': tensor([-1.1594, -1.1623], device='cuda:4'), 'score': tensor(-2.0191, device='cuda:4')}, {'id': 'Q6343', 'texts': ['Carthage >> en'], 'scores': tensor([-0.9458], device='cuda:4'), 'score': tensor(-2.3167, device='cuda:4')}, {'id': 'Q5818', 'texts': ['Córdoba, Spain >> en'], 'scores': tensor([-0.8913], device='cuda:4'), 'score': tensor(-2.5210, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q16330302\n",
      "true answer subject:  Q1524\n",
      "\n",
      "\n",
      "####################\n",
      "what film is by the writer phil hay?\n",
      "\n",
      "\n",
      "[[{'id': 'Q26786240', 'texts': ['Phil Hay >> en'], 'scores': tensor([-0.2330], device='cuda:4'), 'score': tensor(-0.5211, device='cuda:4')}, {'id': 'Q374585', 'texts': ['Phil Hayes >> en'], 'scores': tensor([-0.5991], device='cuda:4'), 'score': tensor(-1.4675, device='cuda:4')}, {'id': 'Q16225521', 'texts': ['Phil Hay (screenwriter) >> en'], 'scores': tensor([-0.5083], device='cuda:4'), 'score': tensor(-1.5248, device='cuda:4')}, {'id': 'Q2086199', 'texts': ['Philip H. Hayes >> en'], 'scores': tensor([-1.0990], device='cuda:4'), 'score': tensor(-3.1085, device='cuda:4')}, {'id': 'Q16209551', 'texts': ['Phil Hayes (actor) >> en'], 'scores': tensor([-1.0432], device='cuda:4'), 'score': tensor(-3.1296, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q16225521\n",
      "true answer subject:  Q192979\n",
      "\n",
      "\n",
      "####################\n",
      "Where did roger marquis die\n",
      "\n",
      "\n",
      "[[{'id': 'Q209726', 'texts': ['Marquis >> en', 'Marquess >> en', 'Marquis >> fr'], 'scores': tensor([-1.1408, -1.1732, -1.3033], device='cuda:4'), 'score': tensor(-1.6674, device='cuda:4')}, {'id': 'Q237223', 'texts': ['Marquis >> de'], 'scores': tensor([-1.0068], device='cuda:4'), 'score': tensor(-2.2513, device='cuda:4')}, {'id': 'Q819230', 'texts': ['Liste der Adelsgeschlechter namens Berg >> de'], 'scores': tensor([-1.1032], device='cuda:4'), 'score': tensor(-3.8216, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q7358590\n",
      "true answer subject:  Q1637790\n",
      "\n",
      "\n",
      "####################\n",
      "what was the cause of death of yves klein\n",
      "\n",
      "\n",
      "[[{'id': 'Q12204', 'texts': ['Tuberculosis >> en', 'Tuberculose >> en'], 'scores': tensor([-0.8097, -1.6651], device='cuda:4'), 'score': tensor(-2.0433, device='cuda:4')}, {'id': 'Q3048621', 'texts': ['List of diseases of the honey bee >> en'], 'scores': tensor([-0.7211], device='cuda:4'), 'score': tensor(-2.2804, device='cuda:4')}, {'id': 'Q8062282', 'texts': ['Yve >> en'], 'scores': tensor([-1.1153], device='cuda:4'), 'score': tensor(-2.4940, device='cuda:4')}, {'id': 'Q408343', 'texts': ['Yv >> en'], 'scores': tensor([-1.2955], device='cuda:4'), 'score': tensor(-2.8967, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q154335\n",
      "true answer subject:  Q12152\n",
      "\n",
      "\n",
      "####################\n",
      "Which equestrian was born in dublin?\n",
      "\n",
      "\n",
      "[[{'id': 'Q179226', 'texts': ['Equestrianism >> en'], 'scores': tensor([-0.6992], device='cuda:4'), 'score': tensor(-1.8499, device='cuda:4')}, {'id': 'Q1516442', 'texts': ['History of Ireland (1801–1923) >> en'], 'scores': tensor([-0.5952], device='cuda:4'), 'score': tensor(-1.8822, device='cuda:4')}, {'id': 'Q875651', 'texts': ['Irish Travellers >> en'], 'scores': tensor([-0.8705], device='cuda:4'), 'score': tensor(-2.1323, device='cuda:4')}, {'id': 'Q833790', 'texts': ['Equestrian at the Summer Olympics >> en'], 'scores': tensor([-0.7202], device='cuda:4'), 'score': tensor(-2.2774, device='cuda:4')}, {'id': 'Q2457218', 'texts': ['Equestrian at the Summer Paralympics >> en'], 'scores': tensor([-0.8956], device='cuda:4'), 'score': tensor(-3.1025, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q1761\n",
      "true answer subject:  Q6768445\n",
      "\n",
      "\n",
      "####################\n",
      "What is a tv action show?\n",
      "\n",
      "\n",
      "[[{'id': 'Q188473', 'texts': ['Action film >> en'], 'scores': tensor([-0.7117], device='cuda:4'), 'score': tensor(-1.5914, device='cuda:4')}, {'id': 'Q1762165', 'texts': ['Action fiction >> en'], 'scores': tensor([-0.8155], device='cuda:4'), 'score': tensor(-1.8236, device='cuda:4')}, {'id': 'Q15416', 'texts': ['Television show >> en'], 'scores': tensor([-1.0385], device='cuda:4'), 'score': tensor(-2.3221, device='cuda:4')}, {'id': 'Q270948', 'texts': ['Action game >> en'], 'scores': tensor([-1.0421], device='cuda:4'), 'score': tensor(-2.3302, device='cuda:4')}, {'id': 'Q343542', 'texts': ['Action >> en'], 'scores': tensor([-1.2027], device='cuda:4'), 'score': tensor(-2.4054, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q11272426\n",
      "true answer subject:  Q5533291\n",
      "\n",
      "\n",
      "####################\n",
      "what's akbar tandjung's ethnicity\n",
      "\n",
      "\n",
      "[[{'id': 'Q1185994', 'texts': ['Demographics of Indonesia >> en'], 'scores': tensor([-0.5456], device='cuda:4'), 'score': tensor(-1.5433, device='cuda:4')}, {'id': 'Q1185999', 'texts': ['Demographics of Malaysia >> en'], 'scores': tensor([-0.5787], device='cuda:4'), 'score': tensor(-1.6369, device='cuda:4')}, {'id': 'Q49209', 'texts': ['Javanese people >> en'], 'scores': tensor([-0.6640], device='cuda:4'), 'score': tensor(-1.7567, device='cuda:4')}, {'id': 'Q42534', 'texts': ['Indus Valley Civilisation >> en'], 'scores': tensor([-0.8137], device='cuda:4'), 'score': tensor(-2.3014, device='cuda:4')}, {'id': 'Q3268547', 'texts': ['Bangsar >> en'], 'scores': tensor([-1.3446], device='cuda:4'), 'score': tensor(-3.0067, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q2581168\n",
      "true answer subject:  Q810714\n",
      "\n",
      "\n",
      "####################\n",
      "What position does carlos gomez play?\n",
      "\n",
      "\n",
      "[[{'id': 'Q571113', 'texts': ['Glossary of chess >> en'], 'scores': tensor([-0.2453], device='cuda:4'), 'score': tensor(-0.7360, device='cuda:4')}, {'id': 'Q2661943', 'texts': ['Glossary of contract bridge terms >> en'], 'scores': tensor([-0.4275], device='cuda:4'), 'score': tensor(-1.3517, device='cuda:4')}, {'id': 'Q8903', 'texts': ['Glossary of association football terms >> en'], 'scores': tensor([-0.5183], device='cuda:4'), 'score': tensor(-1.6389, device='cuda:4')}, {'id': 'Q787810', 'texts': ['List of chess players >> en'], 'scores': tensor([-0.6256], device='cuda:4'), 'score': tensor(-1.7694, device='cuda:4')}, {'id': 'Q1109985', 'texts': ['Glossary of rugby union terms >> en'], 'scores': tensor([-0.5712], device='cuda:4'), 'score': tensor(-1.8062, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q2747238\n",
      "true answer subject:  Q5059480\n",
      "\n",
      "\n",
      "####################\n",
      "how does engelbert zaschka identify\n",
      "\n",
      "\n",
      "[[{'id': 'Q62498', 'texts': ['Engelbert Zaschka >> en'], 'scores': tensor([-0.4502], device='cuda:4'), 'score': tensor(-1.2733, device='cuda:4')}, {'id': 'Q209082', 'texts': ['Chemical kinetics >> en'], 'scores': tensor([-0.6491], device='cuda:4'), 'score': tensor(-1.8359, device='cuda:4')}, {'id': 'Q204037', 'texts': ['Natural logarithm >> en'], 'scores': tensor([-0.8029], device='cuda:4'), 'score': tensor(-2.2709, device='cuda:4')}, {'id': 'Q83147', 'texts': ['Chemical formula >> en'], 'scores': tensor([-1.0012], device='cuda:4'), 'score': tensor(-2.4524, device='cuda:4')}, {'id': 'Q148189', 'texts': ['Zaschka >> en'], 'scores': tensor([-1.3962], device='cuda:4'), 'score': tensor(-3.4200, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q62498\n",
      "true answer subject:  Q6581097\n",
      "\n",
      "\n",
      "####################\n",
      "what position does pee wee reese play in baseball\n",
      "\n",
      "\n",
      "[[{'id': 'Q1151733', 'texts': ['Baseball positions >> en'], 'scores': tensor([-0.3714], device='cuda:4'), 'score': tensor(-0.9098, device='cuda:4')}, {'id': 'Q5571754', 'texts': ['Glossary of baseball (S) >> en'], 'scores': tensor([-0.4385], device='cuda:4'), 'score': tensor(-1.4545, device='cuda:4')}, {'id': 'Q5571757', 'texts': ['Glossary of baseball (T) >> en'], 'scores': tensor([-0.5087], device='cuda:4'), 'score': tensor(-1.6871, device='cuda:4')}, {'id': 'Q3237411', 'texts': ['Glossary of baseball >> en'], 'scores': tensor([-0.6213], device='cuda:4'), 'score': tensor(-1.7572, device='cuda:4')}, {'id': 'Q5369', 'texts': ['Baseball >> en'], 'scores': tensor([-0.8048], device='cuda:4'), 'score': tensor(-1.7995, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q182485\n",
      "true answer subject:  Q1143358\n",
      "\n",
      "\n",
      "####################\n",
      "Which Swiss conductor's cause of death is myocardial infarction?\n",
      "\n",
      "\n",
      "[[{'id': 'Q12152', 'texts': ['Myocardial infarction >> en', 'Myocardial infarct >> en'], 'scores': tensor([-0.0866, -1.0325], device='cuda:4'), 'score': tensor(-0.2248, device='cuda:4')}, {'id': 'Q207550', 'texts': ['Infarction >> en'], 'scores': tensor([-1.1025], device='cuda:4'), 'score': tensor(-2.7007, device='cuda:4')}, {'id': 'Q202837', 'texts': ['Cardiac arrest >> en'], 'scores': tensor([-1.4250], device='cuda:4'), 'score': tensor(-3.4906, device='cuda:4')}, {'id': 'Q84133', 'texts': ['Miocardio >> es'], 'scores': tensor([-1.6785], device='cuda:4'), 'score': tensor(-4.4409, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q12152\n",
      "true answer subject:  Q6371569\n",
      "\n",
      "\n",
      "####################\n",
      "what type of music does david ruffin play\n",
      "\n",
      "\n",
      "[[{'id': 'Q411350', 'texts': ['Ruffin >> en', 'Ruffin >> fr', 'Ruffin >> de'], 'scores': tensor([-0.1341, -1.8206, -1.8801], device='cuda:4'), 'score': tensor(-0.2575, device='cuda:4')}, {'id': 'Q3473856', 'texts': ['Ruffin >> es'], 'scores': tensor([-2.0299], device='cuda:4'), 'score': tensor(-4.5390, device='cuda:4')}, {'id': 'Q1692072', 'texts': ['Ruffine >> de'], 'scores': tensor([-2.7013], device='cuda:4'), 'score': tensor(-6.6169, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q1176417\n",
      "true answer subject:  Q37073\n",
      "\n",
      "\n",
      "####################\n",
      "where was padraic mcguinness's place of death\n",
      "\n",
      "\n",
      "[[{'id': 'Q1202197', 'texts': ['Place of death >> en'], 'scores': tensor([-0.4011], device='cuda:4'), 'score': tensor(-0.9826, device='cuda:4')}, {'id': 'Q438214', 'texts': [\"Places in The Hitchhiker's Guide to the Galaxy >> en\"], 'scores': tensor([-0.3226], device='cuda:4'), 'score': tensor(-1.3300, device='cuda:4')}, {'id': 'Q208361', 'texts': ['Places in Harry Potter >> en'], 'scores': tensor([-0.6064], device='cuda:4'), 'score': tensor(-1.7151, device='cuda:4')}, {'id': 'Q499618', 'texts': ['Glossary of Dune terminology >> en'], 'scores': tensor([-0.5880], device='cuda:4'), 'score': tensor(-1.9502, device='cuda:4')}, {'id': 'Q5125767', 'texts': ['Clandestine chemistry >> en'], 'scores': tensor([-0.8785], device='cuda:4'), 'score': tensor(-2.4847, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q7123909\n",
      "true answer subject:  Q3130\n",
      "\n",
      "\n",
      "####################\n",
      "what language is viper in\n",
      "\n",
      "\n",
      "[[{'id': 'Q163656', 'texts': ['Viperidae >> en'], 'scores': tensor([-0.3507], device='cuda:4'), 'score': tensor(-0.8591, device='cuda:4')}, {'id': 'Q315', 'texts': ['Language >> en'], 'scores': tensor([-0.7536], device='cuda:4'), 'score': tensor(-1.5072, device='cuda:4')}, {'id': 'Q192056', 'texts': ['Vipera berus >> en'], 'scores': tensor([-0.6086], device='cuda:4'), 'score': tensor(-1.6102, device='cuda:4')}, {'id': 'Q662672', 'texts': ['Vipera >> en'], 'scores': tensor([-1.0540], device='cuda:4'), 'score': tensor(-2.3569, device='cuda:4')}, {'id': 'Q20064', 'texts': ['Viper (Battlestar Galactica) >> en'], 'scores': tensor([-0.6929], device='cuda:4'), 'score': tensor(-2.5925, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q1189775\n",
      "true answer subject:  Q1860\n",
      "\n",
      "\n",
      "####################\n",
      "Who influenced michael mcdowell?\n",
      "\n",
      "\n",
      "[[{'id': 'Q6832662', 'texts': ['Michael McDowell >> en', 'Michael McDowell >> fr'], 'scores': tensor([-0.4771, -1.1546], device='cuda:4'), 'score': tensor(-1.2121, device='cuda:4')}, {'id': 'Q411637', 'texts': ['McDowell >> en'], 'scores': tensor([-0.5194], device='cuda:4'), 'score': tensor(-1.3741, device='cuda:4')}, {'id': 'Q3273867', 'texts': ['McDowell Colony >> en'], 'scores': tensor([-0.9049], device='cuda:4'), 'score': tensor(-2.5594, device='cuda:4')}, {'id': 'Q276817', 'texts': ['Michael McDowell (author) >> en'], 'scores': tensor([-0.8276], device='cuda:4'), 'score': tensor(-2.7447, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q276817\n",
      "true answer subject:  Q169566\n",
      "\n",
      "\n",
      "####################\n",
      "what is the place of birth of sam edwards?\n",
      "\n",
      "\n",
      "[[{'id': 'Q39018', 'texts': ['Emperor >> en'], 'scores': tensor([-0.5983], device='cuda:4'), 'score': tensor(-1.4654, device='cuda:4')}, {'id': 'Q208233', 'texts': ['Emperor of Japan >> en'], 'scores': tensor([-0.5408], device='cuda:4'), 'score': tensor(-1.5297, device='cuda:4')}, {'id': 'Q8445', 'texts': ['Marriage >> en'], 'scores': tensor([-0.8359], device='cuda:4'), 'score': tensor(-1.8692, device='cuda:4')}, {'id': 'Q11995', 'texts': ['Pregnancy >> en'], 'scores': tensor([-0.8217], device='cuda:4'), 'score': tensor(-2.0128, device='cuda:4')}, {'id': 'Q3870479', 'texts': ['Nativity of Saint John the Baptist >> en'], 'scores': tensor([-0.6585], device='cuda:4'), 'score': tensor(-2.2810, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q472382\n",
      "true answer subject:  Q23051\n",
      "\n",
      "\n",
      "####################\n",
      "which military was involved in the second battle of fort fisher\n",
      "\n",
      "\n",
      "[[{'id': 'Q4871042', 'texts': ['Battle of Fort Fisher >> en', 'Battle of Fort Fisher (disambiguation) >> en'], 'scores': tensor([-0.5425, -1.1170], device='cuda:4'), 'score': tensor(-1.3632, device='cuda:4')}, {'id': 'Q7443093', 'texts': ['Second battle of fort fisher >> en', 'Second Battle of Fort Fisher >> en'], 'scores': tensor([-0.6908, -0.8167], device='cuda:4'), 'score': tensor(-1.4910, device='cuda:4')}, {'id': 'Q846674', 'texts': ['Battles of Saratoga >> en'], 'scores': tensor([-0.5792], device='cuda:4'), 'score': tensor(-1.7376, device='cuda:4')}]]\n",
      "\n",
      "\n",
      "true answer object:  Q7443093\n",
      "true answer subject:  Q214102\n",
      "\n",
      "\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "for i in data:   \n",
    "    sentences = [i[3]]\n",
    "    print(i[3])\n",
    "    print(\"\\n\")\n",
    "    print(model_mGENRE.sample(\n",
    "        sentences,\n",
    "        beam = 5,\n",
    "        prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
    "            e for e in trie.get(sent.tolist())\n",
    "            if e < len(model_mGENRE.task.target_dictionary)\n",
    "        ],\n",
    "        text_to_id=lambda x: max(lang_title2wikidataID[tuple(reversed(x.split(\" >> \")))], key=lambda y: int(y[1:])),\n",
    "        marginalize=True,\n",
    "        verbose = True\n",
    "    ))\n",
    "    print(\"\\n\")\n",
    "    print(\"true answer object: \", i[0])\n",
    "    print(\"true answer subject: \", i[2])\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative assessment on Simple Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NER:\n",
    "    def __init__(self, \n",
    "                 #text, \n",
    "                 model = \"Babelscape/wikineural-multilingual-ner\",\n",
    "                 tokenizer = \"Babelscape/wikineural-multilingual-ner\"):\n",
    "        \n",
    "        #self.text = text\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def set_pipeline(self):   \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(self.model)\n",
    "        self.nlp = pipeline(\"ner\",\n",
    "                            model = self.model,\n",
    "                            tokenizer = self.tokenizer)\n",
    "        \n",
    "    def receive_text(self, text):\n",
    "        self.text = text\n",
    "    \n",
    "    def receive_words(self):\n",
    "        self.text = re.sub('[!@#$?,]', '', self.text)\n",
    "        out = re.split(r' ', self.text)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def receive_enitity(self):\n",
    "        ner_results = self.nlp(self.text)\n",
    "        print(\"ner_results\", ner_results)\n",
    "        final = []\n",
    "        result = [i[\"word\"] for i in ner_results]\n",
    "        for elem in result:\n",
    "            print(\"elem\", elem)\n",
    "            if (elem[:2] == \"##\") & (len(final) > 0):\n",
    "                final[-1] = final[-1] + elem[2:]\n",
    "            elif elem[:2] == \"##\":\n",
    "                final.append(elem[2:])\n",
    "            else:\n",
    "                final.append(elem)\n",
    "        return final\n",
    "    \n",
    "    def text_with_marked_entities(self):\n",
    "        output = \"\"\n",
    "        entities = self.receive_enitity()\n",
    "        list_of_words = self.receive_words()\n",
    "        for word in list_of_words:\n",
    "            if word not in entities:\n",
    "                output = output + word + \" \"\n",
    "            else:\n",
    "                output = output + \"[START] \" + word +  \" [END] \"\n",
    "        output = re.sub(\"\\[END\\] \\[START\\] \", \"\", output)\n",
    "        output += \"?\"\n",
    "        return output\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ner = NER(text = \"How many times have people won the jackpot at Caesar?\")\n",
    "\n",
    "# initialize class\n",
    "ner = NER()\n",
    "\n",
    "# establish pipeline\n",
    "ner.set_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(data.sample(n = n, replace = False, random_state=1).loc[:, \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/400 [00:00<00:15, 25.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial question: \n",
      " What is a movie pierce brosnan produced?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What is a movie pierce brosnan produced ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What kind of music is featured on ten new songs\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What kind of music is featured on ten new songs ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which episode was written by chris carter (screenwriter)\n",
      "ner_results [{'word': 'ch', 'score': 0.6152981519699097, 'entity': 'B-ORG', 'index': 6, 'start': 29, 'end': 31}]\n",
      "elem ch\n",
      "NER over initial question: \n",
      " which episode was written by chris carter (screenwriter) ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What kind of celestial object is 1495 helsinki?\n",
      "ner_results [{'word': 'hel', 'score': 0.6927583813667297, 'entity': 'B-MISC', 'index': 9, 'start': 38, 'end': 41}, {'word': '##sin', 'score': 0.4328274130821228, 'entity': 'I-MISC', 'index': 10, 'start': 41, 'end': 44}, {'word': '##ki', 'score': 0.6481525897979736, 'entity': 'I-MISC', 'index': 11, 'start': 44, 'end': 46}]\n",
      "elem hel\n",
      "elem ##sin\n",
      "elem ##ki\n",
      "NER over initial question: \n",
      " What kind of celestial object is 1495 [START] helsinki [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is chris offutt's gender \n",
      "ner_results [{'word': 'ch', 'score': 0.656899094581604, 'entity': 'B-PER', 'index': 3, 'start': 8, 'end': 10}, {'word': 'off', 'score': 0.6616729497909546, 'entity': 'B-PER', 'index': 5, 'start': 14, 'end': 17}, {'word': '##utt', 'score': 0.5504513382911682, 'entity': 'I-PER', 'index': 6, 'start': 17, 'end': 20}]\n",
      "elem ch\n",
      "elem off\n",
      "elem ##utt\n",
      "NER over initial question: \n",
      " what is chris offutt's gender  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who is an african american character actor and retired professional wrestler? \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6/400 [00:01<01:24,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'af', 'score': 0.7936720848083496, 'entity': 'B-MISC', 'index': 4, 'start': 10, 'end': 12}, {'word': '##rica', 'score': 0.6354202032089233, 'entity': 'B-MISC', 'index': 5, 'start': 12, 'end': 16}, {'word': '##n', 'score': 0.5097811222076416, 'entity': 'B-MISC', 'index': 6, 'start': 16, 'end': 17}, {'word': 'american', 'score': 0.609046459197998, 'entity': 'B-MISC', 'index': 7, 'start': 18, 'end': 26}]\n",
      "elem af\n",
      "elem ##rica\n",
      "elem ##n\n",
      "elem american\n",
      "NER over initial question: \n",
      " Who is an [START] african american [END] character actor and retired professional wrestler  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Name a professional writer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/400 [00:01<01:14,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " Name a professional writer ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which country is marcel landers from?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Which country is marcel landers from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is an example of a city that can be found in  north american central time zone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 10/400 [00:02<01:36,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'american', 'score': 0.7435474395751953, 'entity': 'B-MISC', 'index': 14, 'start': 57, 'end': 65}]\n",
      "elem american\n",
      "NER over initial question: \n",
      " what is an example of a city that can be found in  north [START] american [END] central time zone ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " In what language was inside man filmed?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " In what language was inside man filmed ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what genre is serpico in?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 15/400 [00:02<00:42,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " what genre is serpico in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What type of music is bobby kildea known for\n",
      "ner_results [{'word': 'bob', 'score': 0.6602774858474731, 'entity': 'B-MISC', 'index': 6, 'start': 22, 'end': 25}, {'word': '##by', 'score': 0.5224658250808716, 'entity': 'I-MISC', 'index': 7, 'start': 25, 'end': 27}, {'word': 'ki', 'score': 0.6515940427780151, 'entity': 'B-MISC', 'index': 8, 'start': 28, 'end': 30}, {'word': '##lde', 'score': 0.38386669754981995, 'entity': 'I-MISC', 'index': 9, 'start': 30, 'end': 33}, {'word': '##a', 'score': 0.6285944581031799, 'entity': 'I-MISC', 'index': 10, 'start': 33, 'end': 34}]\n",
      "elem bob\n",
      "elem ##by\n",
      "elem ki\n",
      "elem ##lde\n",
      "elem ##a\n",
      "NER over initial question: \n",
      " What type of music is [START] bobby kildea [END] known for ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what was earle birney's birth place\n",
      "ner_results [{'word': '##ney', 'score': 0.4409066140651703, 'entity': 'I-PER', 'index': 6, 'start': 18, 'end': 21}]\n",
      "elem ##ney\n",
      "NER over initial question: \n",
      " what was earle birney's birth place ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who created the album the traveling wilburys collection\n",
      "ner_results [{'word': 'wil', 'score': 0.5339022874832153, 'entity': 'B-MISC', 'index': 7, 'start': 36, 'end': 39}, {'word': '##bury', 'score': 0.29286718368530273, 'entity': 'I-PER', 'index': 8, 'start': 39, 'end': 43}]\n",
      "elem wil\n",
      "elem ##bury\n",
      "NER over initial question: \n",
      " who created the album the traveling wilburys collection ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What kind of music does roman holiday record?\n",
      "ner_results [{'word': 'roman', 'score': 0.9365183711051941, 'entity': 'B-MISC', 'index': 6, 'start': 24, 'end': 29}]\n",
      "elem roman\n",
      "NER over initial question: \n",
      " What kind of music does [START] roman [END] holiday record ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What game play mode is the computer video game Yoshis Safari?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 20/400 [00:03<00:41,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'Yo', 'score': 0.9951897263526917, 'entity': 'B-MISC', 'index': 10, 'start': 47, 'end': 49}, {'word': '##shi', 'score': 0.9979313611984253, 'entity': 'I-MISC', 'index': 11, 'start': 49, 'end': 52}, {'word': '##s', 'score': 0.9992203712463379, 'entity': 'I-MISC', 'index': 12, 'start': 52, 'end': 53}, {'word': 'Safari', 'score': 0.9970843195915222, 'entity': 'I-MISC', 'index': 13, 'start': 54, 'end': 60}]\n",
      "elem Yo\n",
      "elem ##shi\n",
      "elem ##s\n",
      "elem Safari\n",
      "NER over initial question: \n",
      " What game play mode is the computer video game [START] Yoshis Safari [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where exactly was marian goliński born\n",
      "ner_results [{'word': 'mari', 'score': 0.8758665919303894, 'entity': 'B-PER', 'index': 4, 'start': 18, 'end': 22}, {'word': '##an', 'score': 0.6453502178192139, 'entity': 'I-PER', 'index': 5, 'start': 22, 'end': 24}, {'word': 'goli', 'score': 0.56134033203125, 'entity': 'I-PER', 'index': 6, 'start': 25, 'end': 29}]\n",
      "elem mari\n",
      "elem ##an\n",
      "elem goli\n",
      "NER over initial question: \n",
      " Where exactly was [START] marian [END] goliński born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what type of album is after hours at the london house \n",
      "ner_results [{'word': 'lo', 'score': 0.5610107183456421, 'entity': 'B-MISC', 'index': 10, 'start': 41, 'end': 43}]\n",
      "elem lo\n",
      "NER over initial question: \n",
      " what type of album is after hours at the london house  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what nationality is simon scardifield\n",
      "ner_results [{'word': 'sim', 'score': 0.7651222348213196, 'entity': 'B-MISC', 'index': 4, 'start': 20, 'end': 23}]\n",
      "elem sim\n",
      "NER over initial question: \n",
      " what nationality is simon scardifield ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What film did mansoor khan contribute to?\n",
      "ner_results [{'word': 'mans', 'score': 0.6828532218933105, 'entity': 'B-PER', 'index': 4, 'start': 14, 'end': 18}, {'word': '##oor', 'score': 0.4860226511955261, 'entity': 'I-PER', 'index': 5, 'start': 18, 'end': 21}]\n",
      "elem mans\n",
      "elem ##oor\n",
      "NER over initial question: \n",
      " What film did [START] mansoor [END] khan contribute to ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what time zone is lindstrom in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 25/400 [00:03<00:29, 12.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'li', 'score': 0.6235854625701904, 'entity': 'B-LOC', 'index': 5, 'start': 18, 'end': 20}]\n",
      "elem li\n",
      "NER over initial question: \n",
      " what time zone is lindstrom in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is mohamed safwat's gender\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what is mohamed safwat's gender ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which german city is faiz kevin mangat from\n",
      "ner_results [{'word': 'german', 'score': 0.950069785118103, 'entity': 'B-MISC', 'index': 2, 'start': 6, 'end': 12}]\n",
      "elem german\n",
      "NER over initial question: \n",
      " which [START] german [END] city is faiz kevin mangat from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What nation is tulsi ramsay from?\n",
      "ner_results [{'word': 'tu', 'score': 0.39693763852119446, 'entity': 'B-MISC', 'index': 4, 'start': 15, 'end': 17}, {'word': '##ls', 'score': 0.27607446908950806, 'entity': 'B-MISC', 'index': 5, 'start': 17, 'end': 19}, {'word': '##i', 'score': 0.41698989272117615, 'entity': 'I-LOC', 'index': 6, 'start': 19, 'end': 20}]\n",
      "elem tu\n",
      "elem ##ls\n",
      "elem ##i\n",
      "NER over initial question: \n",
      " What nation is [START] tulsi [END] ramsay from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where did william hedgcock die\n",
      "ner_results [{'word': 'will', 'score': 0.7169914841651917, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 14}]\n",
      "elem will\n",
      "NER over initial question: \n",
      " Where did william hedgcock die ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is sonja skarstedt's place of birth?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 28/400 [00:03<00:25, 14.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'son', 'score': 0.9524672031402588, 'entity': 'B-PER', 'index': 3, 'start': 8, 'end': 11}, {'word': '##ja', 'score': 0.7951620817184448, 'entity': 'I-PER', 'index': 4, 'start': 11, 'end': 13}, {'word': 'ska', 'score': 0.9669378995895386, 'entity': 'I-PER', 'index': 5, 'start': 14, 'end': 17}, {'word': '##rst', 'score': 0.9139488339424133, 'entity': 'I-PER', 'index': 6, 'start': 17, 'end': 20}, {'word': '##edt', 'score': 0.8780090808868408, 'entity': 'I-PER', 'index': 7, 'start': 20, 'end': 23}]\n",
      "elem son\n",
      "elem ##ja\n",
      "elem ska\n",
      "elem ##rst\n",
      "elem ##edt\n",
      "NER over initial question: \n",
      " what is [START] sonja [END] skarstedt's place of birth ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is someone born in barcelona\n",
      "ner_results [{'word': 'bar', 'score': 0.4818287789821625, 'entity': 'B-LOC', 'index': 6, 'start': 23, 'end': 26}, {'word': '##cel', 'score': 0.39018744230270386, 'entity': 'I-LOC', 'index': 7, 'start': 26, 'end': 29}, {'word': '##ona', 'score': 0.4143126606941223, 'entity': 'I-LOC', 'index': 8, 'start': 29, 'end': 32}]\n",
      "elem bar\n",
      "elem ##cel\n",
      "elem ##ona\n",
      "NER over initial question: \n",
      " who is someone born in [START] barcelona [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What position does uroš tripković play?\n",
      "ner_results [{'word': 'ur', 'score': 0.791358470916748, 'entity': 'B-PER', 'index': 4, 'start': 19, 'end': 21}, {'word': '##oš', 'score': 0.6234731078147888, 'entity': 'I-PER', 'index': 5, 'start': 21, 'end': 23}]\n",
      "elem ur\n",
      "elem ##oš\n",
      "NER over initial question: \n",
      " What position does [START] uroš [END] tripković play ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what's kumar sangakkara's ethnicity?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what's kumar sangakkara's ethnicity ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is an album that was released by marc almond?\n",
      "ner_results [{'word': 'marc', 'score': 0.8326408863067627, 'entity': 'B-ORG', 'index': 9, 'start': 38, 'end': 42}, {'word': 'al', 'score': 0.5100455284118652, 'entity': 'I-ORG', 'index': 10, 'start': 43, 'end': 45}, {'word': '##mond', 'score': 0.5979183316230774, 'entity': 'I-ORG', 'index': 11, 'start': 45, 'end': 49}]\n",
      "elem marc\n",
      "elem al\n",
      "elem ##mond\n",
      "NER over initial question: \n",
      " What is an album that was released by [START] marc almond [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which city was thilo kleibauer born in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 34/400 [00:03<00:20, 17.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'thi', 'score': 0.836921751499176, 'entity': 'B-PER', 'index': 6, 'start': 15, 'end': 18}, {'word': '##lo', 'score': 0.5845729112625122, 'entity': 'I-PER', 'index': 7, 'start': 18, 'end': 20}, {'word': 'kl', 'score': 0.5662601590156555, 'entity': 'I-PER', 'index': 8, 'start': 21, 'end': 23}]\n",
      "elem thi\n",
      "elem ##lo\n",
      "elem kl\n",
      "NER over initial question: \n",
      " Which city was [START] thilo [END] kleibauer born in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is a parent of casey johnson\n",
      "ner_results [{'word': 'case', 'score': 0.5049830079078674, 'entity': 'B-MISC', 'index': 6, 'start': 19, 'end': 23}]\n",
      "elem case\n",
      "NER over initial question: \n",
      " who is a parent of casey johnson ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which artist records under columbia records\n",
      "ner_results [{'word': 'col', 'score': 0.5369917154312134, 'entity': 'B-LOC', 'index': 5, 'start': 27, 'end': 30}, {'word': '##um', 'score': 0.3647492229938507, 'entity': 'I-LOC', 'index': 6, 'start': 30, 'end': 32}, {'word': '##bia', 'score': 0.3557332456111908, 'entity': 'I-LOC', 'index': 7, 'start': 32, 'end': 35}]\n",
      "elem col\n",
      "elem ##um\n",
      "elem ##bia\n",
      "NER over initial question: \n",
      " which artist records under [START] columbia [END] records ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Name a film written by jerry lewis\n",
      "ner_results [{'word': 'jer', 'score': 0.4341519773006439, 'entity': 'B-ORG', 'index': 6, 'start': 23, 'end': 26}]\n",
      "elem jer\n",
      "NER over initial question: \n",
      " Name a film written by jerry lewis ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is 10161 nakanoshima classified as\n",
      "ner_results [{'word': 'nak', 'score': 0.47087135910987854, 'entity': 'B-LOC', 'index': 5, 'start': 14, 'end': 17}, {'word': '##anos', 'score': 0.6242559552192688, 'entity': 'I-LOC', 'index': 6, 'start': 17, 'end': 21}, {'word': '##hima', 'score': 0.7379871010780334, 'entity': 'I-LOC', 'index': 7, 'start': 21, 'end': 25}]\n",
      "elem nak\n",
      "elem ##anos\n",
      "elem ##hima\n",
      "NER over initial question: \n",
      " What is 10161 [START] nakanoshima [END] classified as ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what series has the episode rosebud\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what series has the episode rosebud ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which artist records under the earache records label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 37/400 [00:03<00:24, 15.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'ear', 'score': 0.8605076670646667, 'entity': 'B-ORG', 'index': 6, 'start': 31, 'end': 34}, {'word': '##ache', 'score': 0.8077614903450012, 'entity': 'I-ORG', 'index': 7, 'start': 34, 'end': 38}]\n",
      "elem ear\n",
      "elem ##ache\n",
      "NER over initial question: \n",
      " which artist records under the [START] earache [END] records label ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " This jazz was released in 1959.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 39/400 [00:04<00:40,  9.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'jazz', 'score': 0.6714209914207458, 'entity': 'B-MISC', 'index': 2, 'start': 5, 'end': 9}]\n",
      "elem jazz\n",
      "NER over initial question: \n",
      " This [START] jazz [END] was released in 1959. ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what does 1255 schilowa orbit\n",
      "ner_results [{'word': '##a', 'score': 0.4741567075252533, 'entity': 'I-LOC', 'index': 7, 'start': 22, 'end': 23}]\n",
      "elem ##a\n",
      "NER over initial question: \n",
      " what does 1255 schilowa orbit ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what games were published by ea sports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 44/400 [00:04<00:32, 10.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'ea', 'score': 0.9060685038566589, 'entity': 'B-ORG', 'index': 6, 'start': 29, 'end': 31}]\n",
      "elem ea\n",
      "NER over initial question: \n",
      " what games were published by [START] ea [END] sports ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who is a missionary that was buried in highgate cemetery\n",
      "ner_results [{'word': 'high', 'score': 0.5930889248847961, 'entity': 'B-LOC', 'index': 9, 'start': 39, 'end': 43}]\n",
      "elem high\n",
      "NER over initial question: \n",
      " Who is a missionary that was buried in highgate cemetery ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which city was robert s. langer born in\n",
      "ner_results [{'word': 'robe', 'score': 0.9190661907196045, 'entity': 'B-PER', 'index': 6, 'start': 15, 'end': 19}, {'word': '##rt', 'score': 0.8464214205741882, 'entity': 'I-PER', 'index': 7, 'start': 19, 'end': 21}, {'word': 's', 'score': 0.5457531213760376, 'entity': 'I-PER', 'index': 8, 'start': 22, 'end': 23}, {'word': 'langer', 'score': 0.6147791147232056, 'entity': 'I-PER', 'index': 10, 'start': 25, 'end': 31}]\n",
      "elem robe\n",
      "elem ##rt\n",
      "elem s\n",
      "elem langer\n",
      "NER over initial question: \n",
      " Which city was [START] robert [END] s. [START] langer [END] born in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what religion does swaran singh practice\n",
      "ner_results [{'word': 's', 'score': 0.5052897930145264, 'entity': 'B-MISC', 'index': 4, 'start': 19, 'end': 20}, {'word': '##wara', 'score': 0.3820399045944214, 'entity': 'B-MISC', 'index': 5, 'start': 20, 'end': 24}]\n",
      "elem s\n",
      "elem ##wara\n",
      "NER over initial question: \n",
      " what religion does swaran singh practice ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which label is the artist gil evans signed to\n",
      "ner_results [{'word': 'W', 'score': 0.472793847322464, 'entity': 'B-ORG', 'index': 1, 'start': 0, 'end': 1}, {'word': 'gi', 'score': 0.8063279390335083, 'entity': 'B-ORG', 'index': 8, 'start': 26, 'end': 28}]\n",
      "elem W\n",
      "elem gi\n",
      "NER over initial question: \n",
      " Which label is the artist gil evans signed to ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which city was william grant stairs born\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 46/400 [00:05<00:32, 11.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'will', 'score': 0.41644400358200073, 'entity': 'B-PER', 'index': 4, 'start': 15, 'end': 19}]\n",
      "elem will\n",
      "NER over initial question: \n",
      " which city was william grant stairs born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who wrote city of women\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " who wrote city of women ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where was carolyn mitchell born?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 48/400 [00:05<00:35,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'car', 'score': 0.895651638507843, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 13}, {'word': '##ol', 'score': 0.8396389484405518, 'entity': 'I-PER', 'index': 4, 'start': 13, 'end': 15}, {'word': '##yn', 'score': 0.9128303527832031, 'entity': 'I-PER', 'index': 5, 'start': 15, 'end': 17}]\n",
      "elem car\n",
      "elem ##ol\n",
      "elem ##yn\n",
      "NER over initial question: \n",
      " where was [START] carolyn [END] mitchell born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " in what language was earthlings filmed\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " in what language was earthlings filmed ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who was born in greenville, south carolina?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 50/400 [00:05<00:31, 10.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'green', 'score': 0.6822827458381653, 'entity': 'B-LOC', 'index': 5, 'start': 16, 'end': 21}, {'word': 'car', 'score': 0.5034179091453552, 'entity': 'B-LOC', 'index': 9, 'start': 34, 'end': 37}]\n",
      "elem green\n",
      "elem car\n",
      "NER over initial question: \n",
      " Who was born in greenville south carolina ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what was the gender of the fictional character barry ohanlon\n",
      "ner_results [{'word': 'bar', 'score': 0.6024009585380554, 'entity': 'B-MISC', 'index': 9, 'start': 47, 'end': 50}, {'word': '##han', 'score': 0.3049720227718353, 'entity': 'B-MISC', 'index': 12, 'start': 54, 'end': 57}, {'word': '##lon', 'score': 0.29107603430747986, 'entity': 'I-MISC', 'index': 13, 'start': 57, 'end': 60}]\n",
      "elem bar\n",
      "elem ##han\n",
      "elem ##lon\n",
      "NER over initial question: \n",
      " what was the gender of the fictional character barry ohanlon ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what albums are by scissor sisters\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what albums are by scissor sisters ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " WHat's a basin country of lam ta klong river\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 55/400 [00:06<00:41,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'W', 'score': 0.9899585247039795, 'entity': 'B-LOC', 'index': 1, 'start': 0, 'end': 1}, {'word': '##H', 'score': 0.9522283673286438, 'entity': 'I-LOC', 'index': 2, 'start': 1, 'end': 2}, {'word': '##at', 'score': 0.9861955642700195, 'entity': 'I-LOC', 'index': 3, 'start': 2, 'end': 4}]\n",
      "elem W\n",
      "elem ##H\n",
      "elem ##at\n",
      "NER over initial question: \n",
      " WHat's a basin country of lam ta klong river ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who is a publisher of the game final lap 3\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Who is a publisher of the game final lap 3 ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is the gender of andreas leitner\n",
      "ner_results [{'word': 'andre', 'score': 0.3943864405155182, 'entity': 'B-PER', 'index': 6, 'start': 22, 'end': 27}]\n",
      "elem andre\n",
      "NER over initial question: \n",
      " What is the gender of andreas leitner ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where did prospero alpini die\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " where did prospero alpini die ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " whose music is in the sacrifice?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 57/400 [00:06<00:36,  9.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " whose music is in the sacrifice ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What type of film was the film the glorious adventure\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What type of film was the film the glorious adventure ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which language is the film germany, year zero in?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 59/400 [00:07<01:02,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " Which language is the film germany year zero in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What position in football does john macdonald play\n",
      "ner_results [{'word': 'jo', 'score': 0.40434104204177856, 'entity': 'B-PER', 'index': 6, 'start': 31, 'end': 33}]\n",
      "elem jo\n",
      "NER over initial question: \n",
      " What position in football does john macdonald play ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which kind of rock music appears on spanish dance troupe\n",
      "ner_results [{'word': 'span', 'score': 0.9920292496681213, 'entity': 'B-MISC', 'index': 8, 'start': 36, 'end': 40}, {'word': '##ish', 'score': 0.4141361713409424, 'entity': 'I-MISC', 'index': 9, 'start': 40, 'end': 43}]\n",
      "elem span\n",
      "elem ##ish\n",
      "NER over initial question: \n",
      " which kind of rock music appears on [START] spanish [END] dance troupe ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what kind of subgenre of rock music is on trailblazer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 62/400 [00:08<01:16,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " what kind of subgenre of rock music is on trailblazer ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What musician plays glam punk\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What musician plays glam punk ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where is the birthplace of mala\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 63/400 [00:08<01:13,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'mala', 'score': 0.4666701555252075, 'entity': 'B-PER', 'index': 7, 'start': 27, 'end': 31}]\n",
      "elem mala\n",
      "NER over initial question: \n",
      " where is the birthplace of [START] mala [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what language is spoken in newtonin moondram vidhi?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 64/400 [00:09<02:09,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'new', 'score': 0.9197072982788086, 'entity': 'B-MISC', 'index': 6, 'start': 27, 'end': 30}]\n",
      "elem new\n",
      "NER over initial question: \n",
      " what language is spoken in newtonin moondram vidhi ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what position does stephen mcgowan play \n",
      "ner_results [{'word': 'step', 'score': 0.517386257648468, 'entity': 'B-MISC', 'index': 4, 'start': 19, 'end': 23}]\n",
      "elem step\n",
      "NER over initial question: \n",
      " what position does stephen mcgowan play  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what kind of film is the fruitful vine?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 66/400 [00:09<01:39,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " what kind of film is the fruitful vine ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what position does footballer francis bossman play \n",
      "ner_results [{'word': 'franc', 'score': 0.8202551603317261, 'entity': 'B-MISC', 'index': 5, 'start': 30, 'end': 35}]\n",
      "elem franc\n",
      "NER over initial question: \n",
      " what position does footballer francis bossman play  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what city is schandor kallosh from\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what city is schandor kallosh from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What position does harizul izuan abdul rani play?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 73/400 [00:10<00:39,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " What position does harizul izuan abdul rani play ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is the microsoft train simulator developer\n",
      "ner_results [{'word': 'micro', 'score': 0.943723201751709, 'entity': 'B-MISC', 'index': 4, 'start': 11, 'end': 16}, {'word': '##so', 'score': 0.8019287586212158, 'entity': 'I-MISC', 'index': 5, 'start': 16, 'end': 18}, {'word': '##ft', 'score': 0.910367488861084, 'entity': 'I-MISC', 'index': 6, 'start': 18, 'end': 20}]\n",
      "elem micro\n",
      "elem ##so\n",
      "elem ##ft\n",
      "NER over initial question: \n",
      " who is the [START] microsoft [END] train simulator developer ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what language is spoken in the clown\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what language is spoken in the clown ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What language was along the rio grande filmed in?\n",
      "ner_results [{'word': 'rio', 'score': 0.828364908695221, 'entity': 'B-LOC', 'index': 6, 'start': 28, 'end': 31}, {'word': 'grande', 'score': 0.7522680163383484, 'entity': 'I-LOC', 'index': 7, 'start': 32, 'end': 38}]\n",
      "elem rio\n",
      "elem grande\n",
      "NER over initial question: \n",
      " What language was along the [START] rio grande [END] filmed in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is the profession of khushwant singh?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What is the profession of khushwant singh ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is absalom willis robertson's place of birth\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what is absalom willis robertson's place of birth ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what company released fire emblem: the sacred stones?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 75/400 [00:12<01:52,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " what company released fire emblem: the sacred stones ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What type of space object is 1870 glaukos\n",
      "ner_results [{'word': '1870', 'score': 0.6092402338981628, 'entity': 'B-LOC', 'index': 7, 'start': 29, 'end': 33}, {'word': 'g', 'score': 0.32340046763420105, 'entity': 'I-LOC', 'index': 8, 'start': 34, 'end': 35}, {'word': '##lau', 'score': 0.4440949857234955, 'entity': 'I-MISC', 'index': 9, 'start': 35, 'end': 38}, {'word': '##kos', 'score': 0.4502880871295929, 'entity': 'I-LOC', 'index': 10, 'start': 38, 'end': 41}]\n",
      "elem 1870\n",
      "elem g\n",
      "elem ##lau\n",
      "elem ##kos\n",
      "NER over initial question: \n",
      " What type of space object is [START] 1870 glaukos [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is the gender of  nicole whippy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 79/400 [00:12<01:14,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'nic', 'score': 0.5533607602119446, 'entity': 'B-MISC', 'index': 6, 'start': 23, 'end': 26}]\n",
      "elem nic\n",
      "NER over initial question: \n",
      " what is the gender of  nicole whippy ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who entity was involved in the war of the second coalition\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " who entity was involved in the war of the second coalition ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who was the writer of the episode the jacket\n",
      "ner_results [{'word': 'the', 'score': 0.6908398866653442, 'entity': 'B-MISC', 'index': 8, 'start': 34, 'end': 37}]\n",
      "elem the\n",
      "NER over initial question: \n",
      " who was [START] the [END] writer of [START] the [END] episode [START] the [END] jacket ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which country did the film girlfriend come from\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Which country did the film girlfriend come from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What country was carmen originally from\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 81/400 [00:12<01:00,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " What country was carmen originally from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what type of music is movin' wes?\n",
      "ner_results [{'word': 'mo', 'score': 0.8373691439628601, 'entity': 'B-MISC', 'index': 6, 'start': 22, 'end': 24}, {'word': '##vin', 'score': 0.6295785903930664, 'entity': 'I-MISC', 'index': 7, 'start': 24, 'end': 27}, {'word': \"'\", 'score': 0.7095722556114197, 'entity': 'I-MISC', 'index': 8, 'start': 27, 'end': 28}, {'word': 'we', 'score': 0.6806068420410156, 'entity': 'I-MISC', 'index': 9, 'start': 29, 'end': 31}]\n",
      "elem mo\n",
      "elem ##vin\n",
      "elem '\n",
      "elem we\n",
      "NER over initial question: \n",
      " what type of music is movin' wes ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what was the title of jewel's christmas album\n",
      "ner_results [{'word': 'je', 'score': 0.4068068265914917, 'entity': 'B-PER', 'index': 6, 'start': 22, 'end': 24}]\n",
      "elem je\n",
      "NER over initial question: \n",
      " what was the title of jewel's christmas album ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what position does steve grilli play \n",
      "ner_results [{'word': 'st', 'score': 0.37435638904571533, 'entity': 'B-PER', 'index': 4, 'start': 19, 'end': 21}]\n",
      "elem st\n",
      "NER over initial question: \n",
      " what position does steve grilli play  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Name a romance novel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 87/400 [00:13<00:39,  7.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " Name a romance novel ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which actress and signer was born in eugene, oregon\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " which actress and signer was born in eugene oregon ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what position in football does sanders commings play \n",
      "ner_results [{'word': 'football', 'score': 0.6246776580810547, 'entity': 'B-ORG', 'index': 4, 'start': 17, 'end': 25}]\n",
      "elem football\n",
      "NER over initial question: \n",
      " what position in [START] football [END] does sanders commings play  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which genre is raavanan\n",
      "ner_results [{'word': 'ra', 'score': 0.8885257244110107, 'entity': 'B-MISC', 'index': 4, 'start': 15, 'end': 17}, {'word': '##avan', 'score': 0.5800934433937073, 'entity': 'I-MISC', 'index': 5, 'start': 17, 'end': 21}, {'word': '##an', 'score': 0.45828381180763245, 'entity': 'I-MISC', 'index': 6, 'start': 21, 'end': 23}]\n",
      "elem ra\n",
      "elem ##avan\n",
      "elem ##an\n",
      "NER over initial question: \n",
      " which genre is [START] raavanan [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what language was spoken in the film  jigarwala\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 89/400 [00:13<00:34,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'ji', 'score': 0.8459409475326538, 'entity': 'B-MISC', 'index': 8, 'start': 38, 'end': 40}, {'word': '##gar', 'score': 0.7563301920890808, 'entity': 'I-MISC', 'index': 9, 'start': 40, 'end': 43}, {'word': '##wala', 'score': 0.8504384160041809, 'entity': 'I-MISC', 'index': 10, 'start': 43, 'end': 47}]\n",
      "elem ji\n",
      "elem ##gar\n",
      "elem ##wala\n",
      "NER over initial question: \n",
      " what language was spoken in the film  [START] jigarwala [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what does ivar bjørnson play for an instrument\n",
      "ner_results [{'word': 'i', 'score': 0.9075009822845459, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 11}, {'word': '##var', 'score': 0.7580533027648926, 'entity': 'I-PER', 'index': 4, 'start': 11, 'end': 14}, {'word': 'b', 'score': 0.7717334628105164, 'entity': 'I-PER', 'index': 5, 'start': 15, 'end': 16}, {'word': '##jø', 'score': 0.9065485000610352, 'entity': 'I-PER', 'index': 6, 'start': 16, 'end': 18}, {'word': '##rns', 'score': 0.9097579121589661, 'entity': 'I-PER', 'index': 7, 'start': 18, 'end': 21}, {'word': '##on', 'score': 0.955184280872345, 'entity': 'I-PER', 'index': 8, 'start': 21, 'end': 23}]\n",
      "elem i\n",
      "elem ##var\n",
      "elem b\n",
      "elem ##jø\n",
      "elem ##rns\n",
      "elem ##on\n",
      "NER over initial question: \n",
      " what does [START] ivar bjørnson [END] play for an instrument ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who directed awesome; i fuckin' shot that!?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 93/400 [00:13<00:38,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'i', 'score': 0.3242684006690979, 'entity': 'B-MISC', 'index': 7, 'start': 22, 'end': 23}]\n",
      "elem i\n",
      "NER over initial question: \n",
      " Who directed awesome; [START] i [END] fuckin' shot that ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where is dag from \n",
      "ner_results []\n",
      "NER over initial question: \n",
      " where is dag from  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who are military personnel involved in world war ii\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " who are military personnel involved in world war ii ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What position did chris anstey play in basketball?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 98/400 [00:14<00:23, 12.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'basketball', 'score': 0.7797657251358032, 'entity': 'B-ORG', 'index': 11, 'start': 39, 'end': 49}]\n",
      "elem basketball\n",
      "NER over initial question: \n",
      " What position did chris anstey play in [START] basketball [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which artists are contracted to arista records\n",
      "ner_results [{'word': 'ari', 'score': 0.5459069609642029, 'entity': 'B-ORG', 'index': 6, 'start': 32, 'end': 35}, {'word': '##sta', 'score': 0.33506935834884644, 'entity': 'I-ORG', 'index': 7, 'start': 35, 'end': 38}]\n",
      "elem ari\n",
      "elem ##sta\n",
      "NER over initial question: \n",
      " which artists are contracted to [START] arista [END] records ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is someone that was born in blantyre\n",
      "ner_results [{'word': 'blant', 'score': 0.7995791435241699, 'entity': 'B-LOC', 'index': 8, 'start': 32, 'end': 37}, {'word': '##yre', 'score': 0.6050191521644592, 'entity': 'I-LOC', 'index': 9, 'start': 37, 'end': 40}]\n",
      "elem blant\n",
      "elem ##yre\n",
      "NER over initial question: \n",
      " who is someone that was born in [START] blantyre [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what position did gintarė petronytė play \n",
      "ner_results [{'word': 'gi', 'score': 0.8835962414741516, 'entity': 'B-MISC', 'index': 4, 'start': 18, 'end': 20}, {'word': '##ntar', 'score': 0.7654872536659241, 'entity': 'I-MISC', 'index': 5, 'start': 20, 'end': 24}, {'word': '##ė', 'score': 0.7843338251113892, 'entity': 'I-MISC', 'index': 6, 'start': 24, 'end': 25}]\n",
      "elem gi\n",
      "elem ##ntar\n",
      "elem ##ė\n",
      "NER over initial question: \n",
      " what position did [START] gintarė [END] petronytė play  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what position does anderson roberto da silva luiz play \n",
      "ner_results [{'word': 'anders', 'score': 0.4213170111179352, 'entity': 'B-PER', 'index': 4, 'start': 19, 'end': 25}]\n",
      "elem anders\n",
      "NER over initial question: \n",
      " what position does anderson roberto da silva luiz play  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What fictional character was created by jim aparo?\n",
      "ner_results [{'word': 'jim', 'score': 0.690272867679596, 'entity': 'B-MISC', 'index': 7, 'start': 40, 'end': 43}, {'word': 'apa', 'score': 0.44630783796310425, 'entity': 'I-PER', 'index': 8, 'start': 44, 'end': 47}, {'word': '##ro', 'score': 0.32818514108657837, 'entity': 'I-PER', 'index': 9, 'start': 47, 'end': 49}]\n",
      "elem jim\n",
      "elem apa\n",
      "elem ##ro\n",
      "NER over initial question: \n",
      " What fictional character was created by [START] jim aparo [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what language was spoken in the film\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 103/400 [00:14<00:18, 15.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " what language was spoken in the film ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What was the nationality of vincent riotta\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What was the nationality of vincent riotta ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what country was the the bone snatcher filmed in\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what country was the the bone snatcher filmed in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is the name of an action film\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What is the name of an action film ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is eddie brigati's nationality \n",
      "ner_results [{'word': 'ed', 'score': 0.9266459345817566, 'entity': 'B-PER', 'index': 3, 'start': 8, 'end': 10}, {'word': '##die', 'score': 0.7843695878982544, 'entity': 'I-PER', 'index': 4, 'start': 10, 'end': 13}, {'word': 'br', 'score': 0.8143344521522522, 'entity': 'I-PER', 'index': 5, 'start': 14, 'end': 16}, {'word': '##iga', 'score': 0.886716365814209, 'entity': 'I-PER', 'index': 6, 'start': 16, 'end': 19}, {'word': '##ti', 'score': 0.8939248323440552, 'entity': 'I-PER', 'index': 7, 'start': 19, 'end': 21}]\n",
      "elem ed\n",
      "elem ##die\n",
      "elem br\n",
      "elem ##iga\n",
      "elem ##ti\n",
      "NER over initial question: \n",
      " what is [START] eddie [END] brigati's nationality  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is mike boddicker position in baseball\n",
      "ner_results [{'word': 'baseball', 'score': 0.6972742676734924, 'entity': 'B-ORG', 'index': 10, 'start': 35, 'end': 43}]\n",
      "elem baseball\n",
      "NER over initial question: \n",
      " what is mike boddicker position in [START] baseball [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what style of music does gillespiana play \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 106/400 [00:14<00:15, 18.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'gi', 'score': 0.6629898548126221, 'entity': 'B-MISC', 'index': 6, 'start': 25, 'end': 27}, {'word': '##pian', 'score': 0.2618119418621063, 'entity': 'I-ORG', 'index': 8, 'start': 31, 'end': 35}, {'word': '##a', 'score': 0.40976497530937195, 'entity': 'I-ORG', 'index': 9, 'start': 35, 'end': 36}]\n",
      "elem gi\n",
      "elem ##pian\n",
      "elem ##a\n",
      "NER over initial question: \n",
      " what style of music does gillespiana play  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where is paris theodore from\n",
      "ner_results [{'word': 'pari', 'score': 0.8638589978218079, 'entity': 'B-PER', 'index': 3, 'start': 9, 'end': 13}]\n",
      "elem pari\n",
      "NER over initial question: \n",
      " Where is paris theodore from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is the name of a compilation album by chantal kreviazuk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 109/400 [00:15<00:28, 10.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'kr', 'score': 0.8713497519493103, 'entity': 'B-ORG', 'index': 12, 'start': 51, 'end': 53}, {'word': '##evi', 'score': 0.4043131172657013, 'entity': 'B-ORG', 'index': 13, 'start': 53, 'end': 56}, {'word': '##azu', 'score': 0.7765238881111145, 'entity': 'I-ORG', 'index': 14, 'start': 56, 'end': 59}, {'word': '##k', 'score': 0.9081424474716187, 'entity': 'I-ORG', 'index': 15, 'start': 59, 'end': 60}]\n",
      "elem kr\n",
      "elem ##evi\n",
      "elem ##azu\n",
      "elem ##k\n",
      "NER over initial question: \n",
      " What is the name of a compilation album by chantal [START] kreviazuk [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where was j. n. williamson birthed\n",
      "ner_results [{'word': 'j', 'score': 0.9358385801315308, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 11}, {'word': '##iam', 'score': 0.6982325911521912, 'entity': 'I-PER', 'index': 8, 'start': 20, 'end': 23}, {'word': '##son', 'score': 0.8048983216285706, 'entity': 'I-PER', 'index': 9, 'start': 23, 'end': 26}]\n",
      "elem j\n",
      "elem ##iam\n",
      "elem ##son\n",
      "NER over initial question: \n",
      " Where was j. n. williamson birthed ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is jo bunting a professional at doing\n",
      "ner_results [{'word': 'what', 'score': 0.4775312840938568, 'entity': 'B-MISC', 'index': 1, 'start': 0, 'end': 4}]\n",
      "elem what\n",
      "NER over initial question: \n",
      " [START] what [END] is jo bunting a professional at doing ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what language is blind date in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 111/400 [00:15<00:28, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " what language is blind date in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where is mario tosi from \n",
      "ner_results [{'word': 'mari', 'score': 0.4340492784976959, 'entity': 'B-MISC', 'index': 3, 'start': 9, 'end': 13}]\n",
      "elem mari\n",
      "NER over initial question: \n",
      " where is mario tosi from  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where does shiek mahmud-bey claim nationality from?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 115/400 [00:15<00:34,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " Where does shiek mahmud-bey claim nationality from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where was piotr grabowski born\n",
      "ner_results [{'word': 'pi', 'score': 0.7640106678009033, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 12}, {'word': 'gra', 'score': 0.5278941988945007, 'entity': 'B-PER', 'index': 6, 'start': 16, 'end': 19}, {'word': '##bow', 'score': 0.7315147519111633, 'entity': 'I-PER', 'index': 7, 'start': 19, 'end': 22}, {'word': '##ski', 'score': 0.8502122759819031, 'entity': 'I-PER', 'index': 8, 'start': 22, 'end': 25}]\n",
      "elem pi\n",
      "elem gra\n",
      "elem ##bow\n",
      "elem ##ski\n",
      "NER over initial question: \n",
      " Where was piotr [START] grabowski [END] born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what are the names of death metal albums\n",
      "ner_results [{'word': 'death', 'score': 0.6406100988388062, 'entity': 'B-MISC', 'index': 6, 'start': 22, 'end': 27}]\n",
      "elem death\n",
      "NER over initial question: \n",
      " what are the names of [START] death [END] metal albums ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is the place of birth of bill atkinson?\n",
      "ner_results [{'word': 'bill', 'score': 0.39526981115341187, 'entity': 'B-PER', 'index': 8, 'start': 30, 'end': 34}, {'word': 'at', 'score': 0.33358463644981384, 'entity': 'I-PER', 'index': 9, 'start': 35, 'end': 37}, {'word': '##kins', 'score': 0.36749547719955444, 'entity': 'I-PER', 'index': 10, 'start': 37, 'end': 41}, {'word': '##on', 'score': 0.5139395594596863, 'entity': 'I-PER', 'index': 11, 'start': 41, 'end': 43}]\n",
      "elem bill\n",
      "elem at\n",
      "elem ##kins\n",
      "elem ##on\n",
      "NER over initial question: \n",
      " What is the place of birth of [START] bill atkinson [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is the place of death of carmen scarpitta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 119/400 [00:16<00:27, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " What is the place of death of carmen scarpitta ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where was the film bitter rice located\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " where was the film bitter rice located ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who authored the book islam and the west?\n",
      "ner_results [{'word': 'islam', 'score': 0.9228566288948059, 'entity': 'B-MISC', 'index': 5, 'start': 22, 'end': 27}]\n",
      "elem islam\n",
      "NER over initial question: \n",
      " Who authored the book [START] islam [END] and the west ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where was friedrich böhm born?\n",
      "ner_results [{'word': 'b', 'score': 0.6508483290672302, 'entity': 'I-PER', 'index': 6, 'start': 20, 'end': 21}, {'word': '##öhm', 'score': 0.9660583734512329, 'entity': 'I-PER', 'index': 7, 'start': 21, 'end': 24}]\n",
      "elem b\n",
      "elem ##öhm\n",
      "NER over initial question: \n",
      " where was friedrich [START] böhm [END] born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what conflict did  john norwood participate in\n",
      "ner_results [{'word': 'jo', 'score': 0.34629130363464355, 'entity': 'B-MISC', 'index': 4, 'start': 19, 'end': 21}]\n",
      "elem jo\n",
      "NER over initial question: \n",
      " what conflict did  john norwood participate in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what mode do you play monster madness: battle for suburbia in?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 124/400 [00:16<00:28,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " what mode do you play monster madness: battle for suburbia in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is franz von soxhlet's gender\n",
      "ner_results [{'word': 'fra', 'score': 0.8011653423309326, 'entity': 'B-MISC', 'index': 3, 'start': 8, 'end': 11}]\n",
      "elem fra\n",
      "NER over initial question: \n",
      " What is franz von soxhlet's gender ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which country is the almaty province located in\n",
      "ner_results [{'word': 'alma', 'score': 0.6685354113578796, 'entity': 'B-LOC', 'index': 7, 'start': 21, 'end': 25}]\n",
      "elem alma\n",
      "NER over initial question: \n",
      " Which country is the almaty province located in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is ethnically african american\n",
      "ner_results [{'word': 'af', 'score': 0.8564050197601318, 'entity': 'B-MISC', 'index': 5, 'start': 18, 'end': 20}, {'word': '##rica', 'score': 0.6118485927581787, 'entity': 'B-MISC', 'index': 6, 'start': 20, 'end': 24}]\n",
      "elem af\n",
      "elem ##rica\n",
      "NER over initial question: \n",
      " who is ethnically african american ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is a structure designed by owen jones?\n",
      "ner_results [{'word': 'o', 'score': 0.6708939671516418, 'entity': 'B-MISC', 'index': 7, 'start': 32, 'end': 33}, {'word': '##wen', 'score': 0.4031206965446472, 'entity': 'B-MISC', 'index': 8, 'start': 33, 'end': 36}]\n",
      "elem o\n",
      "elem ##wen\n",
      "NER over initial question: \n",
      " What is a structure designed by [START] owen [END] jones ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " mikhail glinka is known for what kind of music\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 129/400 [00:17<00:20, 12.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'mi', 'score': 0.6536584496498108, 'entity': 'B-MISC', 'index': 1, 'start': 0, 'end': 2}, {'word': '##kha', 'score': 0.3387717604637146, 'entity': 'I-ORG', 'index': 2, 'start': 2, 'end': 5}, {'word': '##il', 'score': 0.31817272305488586, 'entity': 'I-ORG', 'index': 3, 'start': 5, 'end': 7}]\n",
      "elem mi\n",
      "elem ##kha\n",
      "elem ##il\n",
      "NER over initial question: \n",
      " [START] mikhail [END] glinka is known for what kind of music ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is trigg county named after?\n",
      "ner_results [{'word': 'tri', 'score': 0.8398342728614807, 'entity': 'B-LOC', 'index': 3, 'start': 7, 'end': 10}]\n",
      "elem tri\n",
      "NER over initial question: \n",
      " who is trigg county named after ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is son of meg ryan\n",
      "ner_results [{'word': 'meg', 'score': 0.41147541999816895, 'entity': 'B-MISC', 'index': 5, 'start': 14, 'end': 17}]\n",
      "elem meg\n",
      "NER over initial question: \n",
      " who is son of [START] meg [END] ryan ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what type of film is sira` fi al-wadi?\n",
      "ner_results [{'word': 'sir', 'score': 0.8556435108184814, 'entity': 'B-MISC', 'index': 6, 'start': 21, 'end': 24}, {'word': '##a', 'score': 0.7863214015960693, 'entity': 'I-MISC', 'index': 7, 'start': 24, 'end': 25}, {'word': '`', 'score': 0.7431710958480835, 'entity': 'I-MISC', 'index': 8, 'start': 25, 'end': 26}, {'word': 'fi', 'score': 0.6976622939109802, 'entity': 'I-MISC', 'index': 9, 'start': 27, 'end': 29}, {'word': 'al', 'score': 0.6086456775665283, 'entity': 'I-MISC', 'index': 10, 'start': 30, 'end': 32}, {'word': 'wadi', 'score': 0.7310431599617004, 'entity': 'I-MISC', 'index': 12, 'start': 33, 'end': 37}]\n",
      "elem sir\n",
      "elem ##a\n",
      "elem `\n",
      "elem fi\n",
      "elem al\n",
      "elem wadi\n",
      "NER over initial question: \n",
      " what type of film is sira` [START] fi [END] al-wadi ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where did kathy staff spend her last day?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 135/400 [00:17<00:14, 18.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'kat', 'score': 0.4736872911453247, 'entity': 'B-ORG', 'index': 3, 'start': 10, 'end': 13}, {'word': '##hy', 'score': 0.37183406949043274, 'entity': 'I-ORG', 'index': 4, 'start': 13, 'end': 15}]\n",
      "elem kat\n",
      "elem ##hy\n",
      "NER over initial question: \n",
      " Where did [START] kathy [END] staff spend her last day ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what city was jack matthews born in\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what city was jack matthews born in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what religion does fauziah latiff practice\n",
      "ner_results [{'word': 'fa', 'score': 0.40971654653549194, 'entity': 'B-ORG', 'index': 4, 'start': 19, 'end': 21}]\n",
      "elem fa\n",
      "NER over initial question: \n",
      " what religion does fauziah latiff practice ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who wrote the story used in the film resurrection\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Who wrote the story used in the film resurrection ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which country is hiram n. breed from\n",
      "ner_results [{'word': 'hir', 'score': 0.6944187879562378, 'entity': 'B-LOC', 'index': 6, 'start': 17, 'end': 20}]\n",
      "elem hir\n",
      "NER over initial question: \n",
      " Which country is hiram n. breed from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is cb dollaway's gender\n",
      "ner_results [{'word': 'c', 'score': 0.3478547930717468, 'entity': 'B-MISC', 'index': 3, 'start': 8, 'end': 9}]\n",
      "elem c\n",
      "NER over initial question: \n",
      " what is cb dollaway's gender ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who was born in khartoum?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 141/400 [00:17<00:12, 21.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'k', 'score': 0.9960538744926453, 'entity': 'B-LOC', 'index': 5, 'start': 16, 'end': 17}, {'word': '##hart', 'score': 0.7186481356620789, 'entity': 'I-LOC', 'index': 6, 'start': 17, 'end': 21}, {'word': '##oum', 'score': 0.9956541657447815, 'entity': 'I-LOC', 'index': 7, 'start': 21, 'end': 24}]\n",
      "elem k\n",
      "elem ##hart\n",
      "elem ##oum\n",
      "NER over initial question: \n",
      " Who was born in [START] khartoum [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who is the child of elen (saint)?\n",
      "ner_results [{'word': 'ele', 'score': 0.6270607113838196, 'entity': 'B-PER', 'index': 6, 'start': 20, 'end': 23}, {'word': '##n', 'score': 0.39840567111968994, 'entity': 'B-PER', 'index': 7, 'start': 23, 'end': 24}]\n",
      "elem ele\n",
      "elem ##n\n",
      "NER over initial question: \n",
      " Who is the child of [START] elen [END] (saint) ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what battle did john paul jones fight in\n",
      "ner_results [{'word': 'jo', 'score': 0.5247562527656555, 'entity': 'B-PER', 'index': 4, 'start': 16, 'end': 18}, {'word': 'pau', 'score': 0.5004957318305969, 'entity': 'I-PER', 'index': 6, 'start': 21, 'end': 24}]\n",
      "elem jo\n",
      "elem pau\n",
      "NER over initial question: \n",
      " what battle did john paul jones fight in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which country was son of the shark filmed?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Which country was son of the shark filmed ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where was aileen manning given birth\n",
      "ner_results [{'word': 'aile', 'score': 0.8900325298309326, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 14}, {'word': '##en', 'score': 0.852545976638794, 'entity': 'I-PER', 'index': 4, 'start': 14, 'end': 16}, {'word': 'mann', 'score': 0.63608318567276, 'entity': 'I-PER', 'index': 5, 'start': 17, 'end': 21}, {'word': '##ing', 'score': 0.7195519208908081, 'entity': 'I-PER', 'index': 6, 'start': 21, 'end': 24}]\n",
      "elem aile\n",
      "elem ##en\n",
      "elem mann\n",
      "elem ##ing\n",
      "NER over initial question: \n",
      " where was [START] aileen manning [END] given birth ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what country is bill fisk from\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 144/400 [00:17<00:14, 17.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " what country is bill fisk from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is the name of a politician\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What is the name of a politician ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what city and state did robert e. rodes die in\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what city and state did robert e. rodes die in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What kind of film is pray for japan?\n",
      "ner_results [{'word': 'jap', 'score': 0.9857989549636841, 'entity': 'B-MISC', 'index': 9, 'start': 30, 'end': 33}, {'word': '##an', 'score': 0.434604674577713, 'entity': 'I-MISC', 'index': 10, 'start': 33, 'end': 35}]\n",
      "elem jap\n",
      "elem ##an\n",
      "NER over initial question: \n",
      " What kind of film is pray for [START] japan [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " in which country was dick barton strikes back filmed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 150/400 [00:17<00:11, 22.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " in which country was dick barton strikes back filmed ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what city was başar sabuncu born in\n",
      "ner_results [{'word': 'başa', 'score': 0.6934021711349487, 'entity': 'B-LOC', 'index': 4, 'start': 14, 'end': 18}, {'word': '##bun', 'score': 0.4658283293247223, 'entity': 'I-LOC', 'index': 7, 'start': 22, 'end': 25}, {'word': '##cu', 'score': 0.4121622145175934, 'entity': 'I-LOC', 'index': 8, 'start': 25, 'end': 27}]\n",
      "elem başa\n",
      "elem ##bun\n",
      "elem ##cu\n",
      "NER over initial question: \n",
      " what city was başar sabuncu born in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where did antónio granjo die\n",
      "ner_results [{'word': 'ant', 'score': 0.600277304649353, 'entity': 'B-MISC', 'index': 3, 'start': 10, 'end': 13}, {'word': '##ón', 'score': 0.3521748483181, 'entity': 'I-MISC', 'index': 4, 'start': 13, 'end': 15}]\n",
      "elem ant\n",
      "elem ##ón\n",
      "NER over initial question: \n",
      " where did antónio granjo die ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which major city was steve drake born in\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " which major city was steve drake born in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what type of music is on the album the new black\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what type of music is on the album the new black ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which country gives gabriela bustelo her nationality\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " which country gives gabriela bustelo her nationality ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is stephen rebello's country of origin?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 153/400 [00:18<00:10, 23.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'step', 'score': 0.35130321979522705, 'entity': 'B-MISC', 'index': 3, 'start': 8, 'end': 12}]\n",
      "elem step\n",
      "NER over initial question: \n",
      " what is stephen rebello's country of origin ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is a male jewish person\n",
      "ner_results [{'word': 'je', 'score': 0.9938233494758606, 'entity': 'B-MISC', 'index': 5, 'start': 14, 'end': 16}, {'word': '##wis', 'score': 0.8367620706558228, 'entity': 'B-MISC', 'index': 6, 'start': 16, 'end': 19}, {'word': '##h', 'score': 0.8430870175361633, 'entity': 'B-MISC', 'index': 7, 'start': 19, 'end': 20}]\n",
      "elem je\n",
      "elem ##wis\n",
      "elem ##h\n",
      "NER over initial question: \n",
      " who is a male [START] jewish [END] person ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is the birth place of hannah murray?\n",
      "ner_results [{'word': 'hann', 'score': 0.5198717713356018, 'entity': 'B-PER', 'index': 7, 'start': 27, 'end': 31}]\n",
      "elem hann\n",
      "NER over initial question: \n",
      " What is the birth place of hannah murray ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which language is the film thelma in\n",
      "ner_results [{'word': 'W', 'score': 0.9268766045570374, 'entity': 'B-MISC', 'index': 1, 'start': 0, 'end': 1}, {'word': '##hic', 'score': 0.9551447629928589, 'entity': 'I-MISC', 'index': 2, 'start': 1, 'end': 4}, {'word': '##h', 'score': 0.9797670841217041, 'entity': 'I-MISC', 'index': 3, 'start': 4, 'end': 5}, {'word': 'language', 'score': 0.9775635600090027, 'entity': 'I-MISC', 'index': 4, 'start': 6, 'end': 14}]\n",
      "elem W\n",
      "elem ##hic\n",
      "elem ##h\n",
      "elem language\n",
      "NER over initial question: \n",
      " [START] Which language [END] is the film thelma in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is the name of an album that is considered to be ambient music\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 158/400 [00:18<00:18, 12.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " What is the name of an album that is considered to be ambient music ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where was craig gillespie born?\n",
      "ner_results [{'word': 'c', 'score': 0.6702593564987183, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 11}, {'word': '##rai', 'score': 0.49424901604652405, 'entity': 'I-PER', 'index': 4, 'start': 11, 'end': 14}, {'word': '##g', 'score': 0.6217032670974731, 'entity': 'I-PER', 'index': 5, 'start': 14, 'end': 15}]\n",
      "elem c\n",
      "elem ##rai\n",
      "elem ##g\n",
      "NER over initial question: \n",
      " where was [START] craig [END] gillespie born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " state mahamadou diarra 's gender\n",
      "ner_results [{'word': 'maha', 'score': 0.4405977129936218, 'entity': 'B-MISC', 'index': 2, 'start': 6, 'end': 10}]\n",
      "elem maha\n",
      "NER over initial question: \n",
      " state mahamadou diarra 's gender ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " This city, with a population of 1,881 as of 2010 is located in raleigh county, west virginia.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 160/400 [00:19<00:42,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'ra', 'score': 0.975440502166748, 'entity': 'B-LOC', 'index': 17, 'start': 63, 'end': 65}, {'word': '##leigh', 'score': 0.8831830620765686, 'entity': 'I-LOC', 'index': 18, 'start': 65, 'end': 70}, {'word': 'vir', 'score': 0.9074180722236633, 'entity': 'B-LOC', 'index': 22, 'start': 84, 'end': 87}, {'word': '##gini', 'score': 0.58225017786026, 'entity': 'I-LOC', 'index': 23, 'start': 87, 'end': 91}, {'word': '##a', 'score': 0.7590346336364746, 'entity': 'I-LOC', 'index': 24, 'start': 91, 'end': 92}]\n",
      "elem ra\n",
      "elem ##leigh\n",
      "elem vir\n",
      "elem ##gini\n",
      "elem ##a\n",
      "NER over initial question: \n",
      " This city with a population of 1881 as of 2010 is located in [START] raleigh [END] county west virginia. ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what position does osama elsamni play \n",
      "ner_results [{'word': 'osam', 'score': 0.7389712333679199, 'entity': 'B-MISC', 'index': 4, 'start': 19, 'end': 23}, {'word': '##a', 'score': 0.462712824344635, 'entity': 'I-MISC', 'index': 5, 'start': 23, 'end': 24}]\n",
      "elem osam\n",
      "elem ##a\n",
      "NER over initial question: \n",
      " what position does [START] osama [END] elsamni play  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is colettetrudeau's genre\n",
      "ner_results [{'word': 'col', 'score': 0.4717745780944824, 'entity': 'B-PER', 'index': 3, 'start': 8, 'end': 11}, {'word': '##tru', 'score': 0.3861615061759949, 'entity': 'I-PER', 'index': 5, 'start': 15, 'end': 18}, {'word': '##dea', 'score': 0.5414507389068604, 'entity': 'I-PER', 'index': 6, 'start': 18, 'end': 21}, {'word': '##u', 'score': 0.7652961611747742, 'entity': 'I-PER', 'index': 7, 'start': 21, 'end': 22}]\n",
      "elem col\n",
      "elem ##tru\n",
      "elem ##dea\n",
      "elem ##u\n",
      "NER over initial question: \n",
      " what is colettetrudeau's genre ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where did kwan shan die?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 162/400 [00:19<00:36,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'kwa', 'score': 0.5236412882804871, 'entity': 'B-MISC', 'index': 3, 'start': 10, 'end': 13}, {'word': '##n', 'score': 0.31476840376853943, 'entity': 'I-MISC', 'index': 4, 'start': 13, 'end': 14}]\n",
      "elem kwa\n",
      "elem ##n\n",
      "NER over initial question: \n",
      " where did [START] kwan [END] shan die ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is michael noble, baron glenkinglas's profession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 167/400 [00:21<00:45,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'mich', 'score': 0.6576108932495117, 'entity': 'B-PER', 'index': 3, 'start': 8, 'end': 12}, {'word': '##king', 'score': 0.45406436920166016, 'entity': 'I-PER', 'index': 10, 'start': 33, 'end': 37}]\n",
      "elem mich\n",
      "elem ##king\n",
      "NER over initial question: \n",
      " what is michael noble baron glenkinglas's profession ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which town is located in kennebec county Maine \n",
      "ner_results [{'word': 'Maine', 'score': 0.9994755983352661, 'entity': 'B-LOC', 'index': 10, 'start': 41, 'end': 46}]\n",
      "elem Maine\n",
      "NER over initial question: \n",
      " which town is located in kennebec county [START] Maine [END]  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Name someone who was born in mytilene\n",
      "ner_results [{'word': 'my', 'score': 0.9665231704711914, 'entity': 'B-MISC', 'index': 7, 'start': 29, 'end': 31}, {'word': '##tile', 'score': 0.7852271199226379, 'entity': 'B-MISC', 'index': 8, 'start': 31, 'end': 35}, {'word': '##ne', 'score': 0.6941872239112854, 'entity': 'B-MISC', 'index': 9, 'start': 35, 'end': 37}]\n",
      "elem my\n",
      "elem ##tile\n",
      "elem ##ne\n",
      "NER over initial question: \n",
      " Name someone who was born in [START] mytilene [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what sex does decimus burton identify with\n",
      "ner_results [{'word': 'de', 'score': 0.6203651428222656, 'entity': 'B-PER', 'index': 4, 'start': 14, 'end': 16}, {'word': '##us', 'score': 0.42865124344825745, 'entity': 'I-PER', 'index': 6, 'start': 19, 'end': 21}]\n",
      "elem de\n",
      "elem ##us\n",
      "NER over initial question: \n",
      " what sex does decimus burton identify with ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What British writer/editor was born in london?\n",
      "ner_results [{'word': 'British', 'score': 0.9887063503265381, 'entity': 'B-MISC', 'index': 2, 'start': 5, 'end': 12}]\n",
      "elem British\n",
      "NER over initial question: \n",
      " What [START] British [END] writer/editor was born in london ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " in what language was number 17 filmed in \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 172/400 [00:21<00:27,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " in what language was number 17 filmed in  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is the category of the object 9764 morgenstern?\n",
      "ner_results [{'word': '976', 'score': 0.49285468459129333, 'entity': 'B-LOC', 'index': 8, 'start': 35, 'end': 38}]\n",
      "elem 976\n",
      "NER over initial question: \n",
      " what is the category of the object 9764 morgenstern ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what language is the great indian butterfly in\n",
      "ner_results [{'word': 'india', 'score': 0.8235267996788025, 'entity': 'B-MISC', 'index': 6, 'start': 27, 'end': 32}]\n",
      "elem india\n",
      "NER over initial question: \n",
      " what language is the great indian butterfly in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which nation is madhur jaffrey from?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Which nation is madhur jaffrey from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What made the music for the jacket\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What made the music for the jacket ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which position does craig cacek play?\n",
      "ner_results [{'word': 'c', 'score': 0.33732956647872925, 'entity': 'B-MISC', 'index': 6, 'start': 20, 'end': 21}]\n",
      "elem c\n",
      "NER over initial question: \n",
      " Which position does craig cacek play ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what label is grandpuba signed under?\n",
      "ner_results [{'word': 'grand', 'score': 0.8136820197105408, 'entity': 'B-ORG', 'index': 4, 'start': 14, 'end': 19}, {'word': '##pu', 'score': 0.8230289816856384, 'entity': 'I-ORG', 'index': 5, 'start': 19, 'end': 21}, {'word': '##ba', 'score': 0.8730204701423645, 'entity': 'I-ORG', 'index': 6, 'start': 21, 'end': 23}]\n",
      "elem grand\n",
      "elem ##pu\n",
      "elem ##ba\n",
      "NER over initial question: \n",
      " what label is [START] grandpuba [END] signed under ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who directed music and lyrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 175/400 [00:21<00:25,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " who directed music and lyrics ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what type of film is what time is it there?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what type of film is what time is it there ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what country is hello! hum lallan bol rahe hain from\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 177/400 [00:22<00:34,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " what country is hello hum lallan bol rahe hain from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which language is spoken in the film trópico de sangre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 181/400 [00:23<00:32,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'tr', 'score': 0.9188259840011597, 'entity': 'B-MISC', 'index': 10, 'start': 37, 'end': 39}, {'word': '##ó', 'score': 0.9563632011413574, 'entity': 'I-MISC', 'index': 11, 'start': 39, 'end': 40}, {'word': '##pico', 'score': 0.9755746126174927, 'entity': 'I-MISC', 'index': 12, 'start': 40, 'end': 44}, {'word': 'de', 'score': 0.9914866089820862, 'entity': 'I-MISC', 'index': 13, 'start': 45, 'end': 47}, {'word': 'sangre', 'score': 0.9897558689117432, 'entity': 'I-MISC', 'index': 14, 'start': 48, 'end': 54}]\n",
      "elem tr\n",
      "elem ##ó\n",
      "elem ##pico\n",
      "elem de\n",
      "elem sangre\n",
      "NER over initial question: \n",
      " Which language is spoken in the film [START] trópico de sangre [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which country is the film bittersweet memories from\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Which country is the film bittersweet memories from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is chong-nee's profession?\n",
      "ner_results [{'word': 'cho', 'score': 0.5529198050498962, 'entity': 'B-PER', 'index': 3, 'start': 8, 'end': 11}, {'word': '##ng', 'score': 0.2900243401527405, 'entity': 'B-PER', 'index': 4, 'start': 11, 'end': 13}, {'word': '-', 'score': 0.4802631735801697, 'entity': 'I-PER', 'index': 5, 'start': 13, 'end': 14}, {'word': 'ne', 'score': 0.545316755771637, 'entity': 'I-PER', 'index': 6, 'start': 14, 'end': 16}, {'word': '##e', 'score': 0.4205951392650604, 'entity': 'I-PER', 'index': 7, 'start': 16, 'end': 17}]\n",
      "elem cho\n",
      "elem ##ng\n",
      "elem -\n",
      "elem ne\n",
      "elem ##e\n",
      "NER over initial question: \n",
      " What is chong-nee's profession ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what books have algis budrys written?\n",
      "ner_results [{'word': 'al', 'score': 0.5158088803291321, 'entity': 'B-MISC', 'index': 4, 'start': 16, 'end': 18}]\n",
      "elem al\n",
      "NER over initial question: \n",
      " what books have algis budrys written ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who wrote un chien andalou\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 183/400 [00:23<00:30,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'un', 'score': 0.9989933371543884, 'entity': 'B-MISC', 'index': 3, 'start': 10, 'end': 12}, {'word': 'chien', 'score': 0.9980441331863403, 'entity': 'I-MISC', 'index': 4, 'start': 13, 'end': 18}, {'word': 'anda', 'score': 0.9931212663650513, 'entity': 'I-MISC', 'index': 5, 'start': 19, 'end': 23}, {'word': '##lou', 'score': 0.9912106394767761, 'entity': 'I-MISC', 'index': 6, 'start': 23, 'end': 26}]\n",
      "elem un\n",
      "elem chien\n",
      "elem anda\n",
      "elem ##lou\n",
      "NER over initial question: \n",
      " who wrote [START] un chien andalou [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where was oded fehr born \n",
      "ner_results [{'word': 'od', 'score': 0.5021912455558777, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 12}]\n",
      "elem od\n",
      "NER over initial question: \n",
      " where was oded fehr born  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is the position that  mike twellman plays\n",
      "ner_results [{'word': 'mi', 'score': 0.42665189504623413, 'entity': 'B-MISC', 'index': 6, 'start': 27, 'end': 29}]\n",
      "elem mi\n",
      "NER over initial question: \n",
      " what is the position that  mike twellman plays ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what was jacques mehler's place of birth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 185/400 [00:23<00:25,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'ja', 'score': 0.9957315921783447, 'entity': 'B-PER', 'index': 3, 'start': 9, 'end': 11}, {'word': '##cque', 'score': 0.6807682514190674, 'entity': 'I-PER', 'index': 4, 'start': 11, 'end': 15}, {'word': '##s', 'score': 0.985944926738739, 'entity': 'I-PER', 'index': 5, 'start': 15, 'end': 16}, {'word': 'me', 'score': 0.9927644729614258, 'entity': 'I-PER', 'index': 6, 'start': 17, 'end': 19}, {'word': '##hler', 'score': 0.9771450757980347, 'entity': 'I-PER', 'index': 7, 'start': 19, 'end': 23}]\n",
      "elem ja\n",
      "elem ##cque\n",
      "elem ##s\n",
      "elem me\n",
      "elem ##hler\n",
      "NER over initial question: \n",
      " what was [START] jacques [END] mehler's place of birth ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who are the parents of leopold de rothschild\n",
      "ner_results [{'word': 'leo', 'score': 0.5521742105484009, 'entity': 'B-MISC', 'index': 6, 'start': 23, 'end': 26}]\n",
      "elem leo\n",
      "NER over initial question: \n",
      " who are the parents of leopold de rothschild ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which genre is whos afraid of virginia woolf? under\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 187/400 [00:24<00:38,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " Which genre is whos afraid of virginia woolf under ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Name a military personnel involved in world war ii\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Name a military personnel involved in world war ii ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who directed enemy mine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 192/400 [00:24<00:33,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " who directed enemy mine ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " whats the name of a female comic book character\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " whats the name of a female comic book character ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what country is lawrence lipton originally from\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what country is lawrence lipton originally from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What position does footballer jamie ness play?\n",
      "ner_results [{'word': 'jam', 'score': 0.8241810202598572, 'entity': 'B-PER', 'index': 5, 'start': 30, 'end': 33}, {'word': '##ie', 'score': 0.5677288174629211, 'entity': 'I-PER', 'index': 6, 'start': 33, 'end': 35}]\n",
      "elem jam\n",
      "elem ##ie\n",
      "NER over initial question: \n",
      " What position does footballer [START] jamie [END] ness play ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " how can the game ridge racer 6 be played\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " how can the game ridge racer 6 be played ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What artist produced the album monster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 194/400 [00:25<00:27,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " What artist produced the album monster ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What gaming company published pac-man world 3\n",
      "ner_results [{'word': 'pa', 'score': 0.931614875793457, 'entity': 'B-MISC', 'index': 6, 'start': 30, 'end': 32}, {'word': '##c', 'score': 0.7706493735313416, 'entity': 'I-MISC', 'index': 7, 'start': 32, 'end': 33}, {'word': '-', 'score': 0.7291232943534851, 'entity': 'I-MISC', 'index': 8, 'start': 33, 'end': 34}, {'word': 'man', 'score': 0.7458305358886719, 'entity': 'I-MISC', 'index': 9, 'start': 34, 'end': 37}]\n",
      "elem pa\n",
      "elem ##c\n",
      "elem -\n",
      "elem man\n",
      "NER over initial question: \n",
      " What gaming company published pac-man world 3 ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which county is saline located in?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " which county is saline located in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who directed aaina?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 197/400 [00:25<00:24,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'aa', 'score': 0.44476082921028137, 'entity': 'B-PER', 'index': 3, 'start': 13, 'end': 15}, {'word': '##ina', 'score': 0.508774995803833, 'entity': 'I-MISC', 'index': 4, 'start': 15, 'end': 18}]\n",
      "elem aa\n",
      "elem ##ina\n",
      "NER over initial question: \n",
      " who directed [START] aaina [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which asian nation is lanao del norte an administrative division of?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 199/400 [00:26<00:38,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'asi', 'score': 0.8876060247421265, 'entity': 'B-MISC', 'index': 2, 'start': 6, 'end': 9}]\n",
      "elem asi\n",
      "NER over initial question: \n",
      " which asian nation is lanao del norte an administrative division of ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what continent is robert helpmann from\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what continent is robert helpmann from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is the label for  mucc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 204/400 [00:26<00:22,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'mu', 'score': 0.9041418433189392, 'entity': 'B-ORG', 'index': 6, 'start': 22, 'end': 24}, {'word': '##cc', 'score': 0.8165481686592102, 'entity': 'I-ORG', 'index': 7, 'start': 24, 'end': 26}]\n",
      "elem mu\n",
      "elem ##cc\n",
      "NER over initial question: \n",
      " who is the label for  [START] mucc [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who composed music for nothing like the holidays?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " who composed music for nothing like the holidays ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " is abraham cohen de herrera male or female\n",
      "ner_results [{'word': 'ab', 'score': 0.47976547479629517, 'entity': 'B-MISC', 'index': 2, 'start': 3, 'end': 5}]\n",
      "elem ab\n",
      "NER over initial question: \n",
      " is abraham cohen de herrera male or female ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which label signed b-projekt?\n",
      "ner_results [{'word': 'b', 'score': 0.6669031977653503, 'entity': 'B-ORG', 'index': 6, 'start': 19, 'end': 20}]\n",
      "elem b\n",
      "NER over initial question: \n",
      " Which label signed b-projekt ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what language is spoken in candleshoe\n",
      "ner_results [{'word': 'cand', 'score': 0.5395224094390869, 'entity': 'B-MISC', 'index': 6, 'start': 27, 'end': 31}]\n",
      "elem cand\n",
      "NER over initial question: \n",
      " what language is spoken in candleshoe ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is a rock music album by Australian singer John Farnham\n",
      "ner_results [{'word': 'Australian', 'score': 0.9552663564682007, 'entity': 'B-MISC', 'index': 8, 'start': 30, 'end': 40}, {'word': 'John', 'score': 0.9998754262924194, 'entity': 'B-PER', 'index': 10, 'start': 48, 'end': 52}, {'word': 'Far', 'score': 0.9999478459358215, 'entity': 'I-PER', 'index': 11, 'start': 53, 'end': 56}, {'word': '##nham', 'score': 0.9999064207077026, 'entity': 'I-PER', 'index': 12, 'start': 56, 'end': 60}]\n",
      "elem Australian\n",
      "elem John\n",
      "elem Far\n",
      "elem ##nham\n",
      "NER over initial question: \n",
      " what is a rock music album by [START] Australian [END] singer [START] John Farnham [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what sex is rafael carrera\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 210/400 [00:26<00:13, 14.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " what sex is rafael carrera ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who wrote the music for the film silambattam?\n",
      "ner_results [{'word': 'sila', 'score': 0.9874122142791748, 'entity': 'B-MISC', 'index': 8, 'start': 33, 'end': 37}, {'word': '##mbat', 'score': 0.9857508540153503, 'entity': 'I-MISC', 'index': 9, 'start': 37, 'end': 41}, {'word': '##tam', 'score': 0.9937795996665955, 'entity': 'I-MISC', 'index': 10, 'start': 41, 'end': 44}]\n",
      "elem sila\n",
      "elem ##mbat\n",
      "elem ##tam\n",
      "NER over initial question: \n",
      " Who wrote the music for the film [START] silambattam [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what kind of music is meteora (album)\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what kind of music is meteora (album) ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is the gender of monique leyrac?\n",
      "ner_results [{'word': 'mon', 'score': 0.5026077032089233, 'entity': 'B-MISC', 'index': 6, 'start': 22, 'end': 25}, {'word': '##ique', 'score': 0.32117384672164917, 'entity': 'I-ORG', 'index': 7, 'start': 25, 'end': 29}, {'word': '##rac', 'score': 0.35303664207458496, 'entity': 'I-ORG', 'index': 9, 'start': 33, 'end': 36}]\n",
      "elem mon\n",
      "elem ##ique\n",
      "elem ##rac\n",
      "NER over initial question: \n",
      " what is the gender of monique leyrac ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what position does hassan shirmohammadi play\n",
      "ner_results [{'word': 'has', 'score': 0.36952877044677734, 'entity': 'B-MISC', 'index': 4, 'start': 19, 'end': 22}]\n",
      "elem has\n",
      "NER over initial question: \n",
      " what position does hassan shirmohammadi play ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what film is under tristar pictures?\n",
      "ner_results [{'word': 'tri', 'score': 0.6369421482086182, 'entity': 'B-MISC', 'index': 5, 'start': 19, 'end': 22}]\n",
      "elem tri\n",
      "NER over initial question: \n",
      " what film is under tristar pictures ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " whats the gender of andy mccollum\n",
      "ner_results [{'word': 'and', 'score': 0.45513424277305603, 'entity': 'B-MISC', 'index': 6, 'start': 20, 'end': 23}]\n",
      "elem and\n",
      "NER over initial question: \n",
      " whats the gender of andy mccollum ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " whats the name of a person born in  loboc, bohol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 213/400 [00:27<00:22,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': '##hol', 'score': 0.7019297480583191, 'entity': 'B-MISC', 'index': 15, 'start': 45, 'end': 48}]\n",
      "elem ##hol\n",
      "NER over initial question: \n",
      " whats the name of a person born in  loboc bohol ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " In which conflict did co-rux-te-chod-ish participate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 215/400 [00:28<00:30,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'co', 'score': 0.6121746897697449, 'entity': 'B-ORG', 'index': 5, 'start': 22, 'end': 24}, {'word': 'ru', 'score': 0.4250151515007019, 'entity': 'I-ORG', 'index': 7, 'start': 25, 'end': 27}, {'word': '##x', 'score': 0.4349890351295471, 'entity': 'I-ORG', 'index': 8, 'start': 27, 'end': 28}, {'word': '##d', 'score': 0.4607264995574951, 'entity': 'I-ORG', 'index': 13, 'start': 35, 'end': 36}, {'word': 'ish', 'score': 0.5363644957542419, 'entity': 'I-ORG', 'index': 15, 'start': 37, 'end': 40}]\n",
      "elem co\n",
      "elem ru\n",
      "elem ##x\n",
      "elem ##d\n",
      "elem ish\n",
      "NER over initial question: \n",
      " In which conflict did co-rux-te-chod-ish participate ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Whats the name of a musical film\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Whats the name of a musical film ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what language is used in suspense\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what language is used in suspense ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who wrote not in the flesh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 220/400 [00:28<00:24,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " who wrote not in the flesh ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is aaron miles (basketball)'s profession \n",
      "ner_results [{'word': 'aa', 'score': 0.42586252093315125, 'entity': 'B-PER', 'index': 3, 'start': 8, 'end': 10}, {'word': '##ron', 'score': 0.32925960421562195, 'entity': 'I-PER', 'index': 4, 'start': 10, 'end': 13}]\n",
      "elem aa\n",
      "elem ##ron\n",
      "NER over initial question: \n",
      " what is [START] aaron [END] miles (basketball)'s profession  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is the artist of carrots / kkkkk\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " who is the artist of carrots / kkkkk ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which poet was born in glencoe\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " which poet was born in glencoe ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what kind of music album is table for one?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 225/400 [00:28<00:16, 10.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " what kind of music album is table for one ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which language was the film state of the union in\n",
      "ner_results [{'word': 'W', 'score': 0.8031348586082458, 'entity': 'B-MISC', 'index': 1, 'start': 0, 'end': 1}, {'word': '##h', 'score': 0.6026211977005005, 'entity': 'I-MISC', 'index': 3, 'start': 4, 'end': 5}, {'word': 'language', 'score': 0.8026172518730164, 'entity': 'I-MISC', 'index': 4, 'start': 6, 'end': 14}]\n",
      "elem W\n",
      "elem ##h\n",
      "elem language\n",
      "NER over initial question: \n",
      " Which [START] language [END] was the film state of the union in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What was roy rogers' nationality?\n",
      "ner_results [{'word': 'ro', 'score': 0.4317428171634674, 'entity': 'B-MISC', 'index': 3, 'start': 9, 'end': 11}]\n",
      "elem ro\n",
      "NER over initial question: \n",
      " What was roy rogers' nationality ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where did kuno fischer spend his last day alive\n",
      "ner_results [{'word': 'kuno', 'score': 0.7476139068603516, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 14}, {'word': 'fi', 'score': 0.5501946806907654, 'entity': 'I-PER', 'index': 4, 'start': 15, 'end': 17}, {'word': '##scher', 'score': 0.7403134107589722, 'entity': 'I-PER', 'index': 5, 'start': 17, 'end': 22}]\n",
      "elem kuno\n",
      "elem fi\n",
      "elem ##scher\n",
      "NER over initial question: \n",
      " where did [START] kuno fischer [END] spend his last day alive ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is myles dillon's gender?\n",
      "ner_results [{'word': 'my', 'score': 0.4867270290851593, 'entity': 'B-PER', 'index': 3, 'start': 8, 'end': 10}, {'word': '##les', 'score': 0.4458354711532593, 'entity': 'I-PER', 'index': 4, 'start': 10, 'end': 13}]\n",
      "elem my\n",
      "elem ##les\n",
      "NER over initial question: \n",
      " What is [START] myles [END] dillon's gender ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is the language of the composition hurricane?\n",
      "ner_results [{'word': 'hurricane', 'score': 0.9160903096199036, 'entity': 'B-MISC', 'index': 8, 'start': 40, 'end': 49}]\n",
      "elem hurricane\n",
      "NER over initial question: \n",
      " what is the language of the composition [START] hurricane [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where is zenon grocholewski from?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 228/400 [00:29<00:13, 12.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'zen', 'score': 0.38526973128318787, 'entity': 'B-LOC', 'index': 3, 'start': 9, 'end': 12}]\n",
      "elem zen\n",
      "NER over initial question: \n",
      " Where is zenon grocholewski from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is the name of a 2004 horror movie\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what is the name of a 2004 horror movie ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what type of videogame is jade dynasty\n",
      "ner_results [{'word': 'ja', 'score': 0.8859080672264099, 'entity': 'B-MISC', 'index': 7, 'start': 26, 'end': 28}, {'word': '##de', 'score': 0.7339016199111938, 'entity': 'I-MISC', 'index': 8, 'start': 28, 'end': 30}]\n",
      "elem ja\n",
      "elem ##de\n",
      "NER over initial question: \n",
      " what type of videogame is [START] jade [END] dynasty ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is the profession of michael dudikoff?\n",
      "ner_results [{'word': 'mich', 'score': 0.8623847961425781, 'entity': 'B-PER', 'index': 6, 'start': 26, 'end': 30}, {'word': '##ael', 'score': 0.8422821164131165, 'entity': 'I-PER', 'index': 7, 'start': 30, 'end': 33}]\n",
      "elem mich\n",
      "elem ##ael\n",
      "NER over initial question: \n",
      " What is the profession of [START] michael [END] dudikoff ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is the language flying blind is in?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What is the language flying blind is in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What kind of music does sylvan richardson make\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 234/400 [00:29<00:11, 14.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'sy', 'score': 0.4623192846775055, 'entity': 'B-LOC', 'index': 6, 'start': 24, 'end': 26}, {'word': '##lva', 'score': 0.3730614483356476, 'entity': 'B-MISC', 'index': 7, 'start': 26, 'end': 29}]\n",
      "elem sy\n",
      "elem ##lva\n",
      "NER over initial question: \n",
      " What kind of music does sylvan richardson make ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who recorded maple leaves (ep)\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " who recorded maple leaves (ep) ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is the title of a film in the drama genre?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What is the title of a film in the drama genre ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What company published the game brothers: a tale of two sons\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What company published the game brothers: a tale of two sons ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is the country origin of the film the end of violence?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 236/400 [00:29<00:11, 14.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " What is the country origin of the film the end of violence ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What was william b. hawks's place of birth?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 238/400 [00:30<00:35,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'will', 'score': 0.9661014080047607, 'entity': 'B-PER', 'index': 3, 'start': 9, 'end': 13}, {'word': '##iam', 'score': 0.8614923357963562, 'entity': 'I-PER', 'index': 4, 'start': 13, 'end': 16}, {'word': 'b', 'score': 0.5661325454711914, 'entity': 'I-PER', 'index': 5, 'start': 17, 'end': 18}, {'word': '##w', 'score': 0.5565399527549744, 'entity': 'I-PER', 'index': 8, 'start': 22, 'end': 23}, {'word': '##ks', 'score': 0.5719032287597656, 'entity': 'I-PER', 'index': 9, 'start': 23, 'end': 25}]\n",
      "elem will\n",
      "elem ##iam\n",
      "elem b\n",
      "elem ##w\n",
      "elem ##ks\n",
      "NER over initial question: \n",
      " What was [START] william [END] b. hawks's place of birth ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what position does baseball player steve bellán play\n",
      "ner_results [{'word': 'baseball', 'score': 0.9235239624977112, 'entity': 'B-ORG', 'index': 4, 'start': 19, 'end': 27}]\n",
      "elem baseball\n",
      "NER over initial question: \n",
      " what position does [START] baseball [END] player steve bellán play ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Name an event that occurred in laconia.\n",
      "ner_results [{'word': 'lac', 'score': 0.663482666015625, 'entity': 'B-MISC', 'index': 7, 'start': 31, 'end': 34}, {'word': '##onia', 'score': 0.45773202180862427, 'entity': 'I-LOC', 'index': 8, 'start': 34, 'end': 38}]\n",
      "elem lac\n",
      "elem ##onia\n",
      "NER over initial question: \n",
      " Name an event that occurred in laconia. ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which country contains taylor county\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 242/400 [00:31<00:25,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'tay', 'score': 0.575762927532196, 'entity': 'B-LOC', 'index': 4, 'start': 23, 'end': 26}]\n",
      "elem tay\n",
      "NER over initial question: \n",
      " which country contains taylor county ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is john landis' religion\n",
      "ner_results [{'word': 'jo', 'score': 0.37641194462776184, 'entity': 'B-MISC', 'index': 3, 'start': 8, 'end': 10}]\n",
      "elem jo\n",
      "NER over initial question: \n",
      " what is john landis' religion ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which type of music is sportin' life associated with\n",
      "ner_results [{'word': 'sport', 'score': 0.38289380073547363, 'entity': 'B-PER', 'index': 6, 'start': 23, 'end': 28}]\n",
      "elem sport\n",
      "NER over initial question: \n",
      " which type of music is sportin' life associated with ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is Mithoon's profession?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 244/400 [00:31<00:22,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'Mit', 'score': 0.9968095421791077, 'entity': 'B-PER', 'index': 3, 'start': 8, 'end': 11}, {'word': '##ho', 'score': 0.823874831199646, 'entity': 'I-PER', 'index': 4, 'start': 11, 'end': 13}, {'word': '##on', 'score': 0.9824770092964172, 'entity': 'I-PER', 'index': 5, 'start': 13, 'end': 15}]\n",
      "elem Mit\n",
      "elem ##ho\n",
      "elem ##on\n",
      "NER over initial question: \n",
      " What is Mithoon's profession ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what language is country in?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what language is country in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " effie gray was born in this country\n",
      "ner_results [{'word': 'ef', 'score': 0.5577040910720825, 'entity': 'B-MISC', 'index': 1, 'start': 0, 'end': 2}]\n",
      "elem ef\n",
      "NER over initial question: \n",
      " effie gray was born in this country ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " In which conflict did chesley g. peterson participate in?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 247/400 [00:31<00:15,  9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " In which conflict did chesley g. peterson participate in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what position did cary williams play\n",
      "ner_results [{'word': 'car', 'score': 0.661247730255127, 'entity': 'B-PER', 'index': 4, 'start': 18, 'end': 21}, {'word': '##iam', 'score': 0.4909294545650482, 'entity': 'I-PER', 'index': 7, 'start': 27, 'end': 30}]\n",
      "elem car\n",
      "elem ##iam\n",
      "NER over initial question: \n",
      " what position did cary williams play ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is kim hiorthøy's career?\n",
      "ner_results [{'word': 'kim', 'score': 0.8315355777740479, 'entity': 'B-PER', 'index': 3, 'start': 8, 'end': 11}, {'word': 'hi', 'score': 0.7268798351287842, 'entity': 'I-PER', 'index': 4, 'start': 12, 'end': 14}, {'word': '##ort', 'score': 0.7151828408241272, 'entity': 'I-PER', 'index': 5, 'start': 14, 'end': 17}, {'word': '##h', 'score': 0.7637692093849182, 'entity': 'I-PER', 'index': 6, 'start': 17, 'end': 18}, {'word': '##øy', 'score': 0.8609783053398132, 'entity': 'I-PER', 'index': 7, 'start': 18, 'end': 20}]\n",
      "elem kim\n",
      "elem hi\n",
      "elem ##ort\n",
      "elem ##h\n",
      "elem ##øy\n",
      "NER over initial question: \n",
      " what is [START] kim [END] hiorthøy's career ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What type of asteroid group is 3775 ellenbeth in\n",
      "ner_results [{'word': '377', 'score': 0.9713433980941772, 'entity': 'B-LOC', 'index': 7, 'start': 31, 'end': 34}, {'word': '##5', 'score': 0.9374805092811584, 'entity': 'I-LOC', 'index': 8, 'start': 34, 'end': 35}, {'word': 'ellen', 'score': 0.952582597732544, 'entity': 'I-LOC', 'index': 9, 'start': 36, 'end': 41}, {'word': '##beth', 'score': 0.9610465168952942, 'entity': 'I-LOC', 'index': 10, 'start': 41, 'end': 45}]\n",
      "elem 377\n",
      "elem ##5\n",
      "elem ellen\n",
      "elem ##beth\n",
      "NER over initial question: \n",
      " What type of asteroid group is [START] 3775 ellenbeth [END] in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is a shooting guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 250/400 [00:31<00:13, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " who is a shooting guard ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is a parent of  christian vi of denmark\n",
      "ner_results [{'word': 'ch', 'score': 0.9802320003509521, 'entity': 'B-MISC', 'index': 6, 'start': 20, 'end': 22}, {'word': '##risti', 'score': 0.4879285991191864, 'entity': 'B-MISC', 'index': 7, 'start': 22, 'end': 27}]\n",
      "elem ch\n",
      "elem ##risti\n",
      "NER over initial question: \n",
      " who is a parent of  christian vi of denmark ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What gender is Ernie Russell?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 255/400 [00:32<00:10, 13.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'Ernie', 'score': 0.9996469020843506, 'entity': 'B-PER', 'index': 4, 'start': 15, 'end': 20}, {'word': 'Russell', 'score': 0.9993387460708618, 'entity': 'I-PER', 'index': 5, 'start': 21, 'end': 28}]\n",
      "elem Ernie\n",
      "elem Russell\n",
      "NER over initial question: \n",
      " What gender is [START] Ernie Russell [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What kind of album is the ugly duckling\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What kind of album is the ugly duckling ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which city did terence cooke pass away in\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " which city did terence cooke pass away in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who was the film producer for anatole dauman\n",
      "ner_results [{'word': 'ana', 'score': 0.671550452709198, 'entity': 'B-MISC', 'index': 7, 'start': 30, 'end': 33}, {'word': '##tol', 'score': 0.28760480880737305, 'entity': 'B-MISC', 'index': 8, 'start': 33, 'end': 36}]\n",
      "elem ana\n",
      "elem ##tol\n",
      "NER over initial question: \n",
      " who was the film producer for anatole dauman ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what film did lou scheimer produce?\n",
      "ner_results [{'word': 'lo', 'score': 0.602271556854248, 'entity': 'B-PER', 'index': 4, 'start': 14, 'end': 16}, {'word': 's', 'score': 0.6030445694923401, 'entity': 'I-PER', 'index': 6, 'start': 18, 'end': 19}, {'word': '##che', 'score': 0.6692713499069214, 'entity': 'I-PER', 'index': 7, 'start': 19, 'end': 22}, {'word': '##ime', 'score': 0.6827836036682129, 'entity': 'I-PER', 'index': 8, 'start': 22, 'end': 25}, {'word': '##r', 'score': 0.6663778424263, 'entity': 'I-PER', 'index': 9, 'start': 25, 'end': 26}]\n",
      "elem lo\n",
      "elem s\n",
      "elem ##che\n",
      "elem ##ime\n",
      "elem ##r\n",
      "NER over initial question: \n",
      " what film did lou [START] scheimer [END] produce ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " is howard d. white male or female\n",
      "ner_results [{'word': 'how', 'score': 0.5269909501075745, 'entity': 'B-MISC', 'index': 2, 'start': 3, 'end': 6}]\n",
      "elem how\n",
      "NER over initial question: \n",
      " is howard d. white male or female ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is  a person that was born in  heligoland\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 261/400 [00:32<00:07, 18.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'hel', 'score': 0.6247831583023071, 'entity': 'B-MISC', 'index': 9, 'start': 35, 'end': 38}]\n",
      "elem hel\n",
      "NER over initial question: \n",
      " who is  a person that was born in  heligoland ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where did neal barrett die\n",
      "ner_results [{'word': 'ne', 'score': 0.8504214882850647, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 12}, {'word': '##al', 'score': 0.4910966157913208, 'entity': 'I-PER', 'index': 4, 'start': 12, 'end': 14}]\n",
      "elem ne\n",
      "elem ##al\n",
      "NER over initial question: \n",
      " where did [START] neal [END] barrett die ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which animated films did deborah lurie contribute music to?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " which animated films did deborah lurie contribute music to ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which country is david j. bodycombe from \n",
      "ner_results []\n",
      "NER over initial question: \n",
      " which country is david j. bodycombe from  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which war did william buckingham participate in \n",
      "ner_results [{'word': 'will', 'score': 0.406879186630249, 'entity': 'B-PER', 'index': 4, 'start': 14, 'end': 18}]\n",
      "elem will\n",
      "NER over initial question: \n",
      " which war did william buckingham participate in  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where was oviedo born?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 268/400 [00:32<00:06, 21.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'ov', 'score': 0.5436587929725647, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 12}, {'word': '##ied', 'score': 0.47273576259613037, 'entity': 'I-PER', 'index': 4, 'start': 12, 'end': 15}, {'word': '##o', 'score': 0.7427330017089844, 'entity': 'I-PER', 'index': 5, 'start': 15, 'end': 16}]\n",
      "elem ov\n",
      "elem ##ied\n",
      "elem ##o\n",
      "NER over initial question: \n",
      " Where was [START] oviedo [END] born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what kind of movie is we want a child!\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what kind of movie is we want a child ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what country is the movie go go tales from?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what country is the movie go go tales from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who is a retired Spanish footballer who played midfielder?\n",
      "ner_results [{'word': 'Spanish', 'score': 0.995329737663269, 'entity': 'B-MISC', 'index': 5, 'start': 17, 'end': 24}]\n",
      "elem Spanish\n",
      "NER over initial question: \n",
      " Who is a retired [START] Spanish [END] footballer who played midfielder ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What position does football player rory patterson play?\n",
      "ner_results [{'word': '##on', 'score': 0.30843040347099304, 'entity': 'I-ORG', 'index': 10, 'start': 47, 'end': 49}]\n",
      "elem ##on\n",
      "NER over initial question: \n",
      " What position does football player rory patterson play ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what war was lancelot curran involved in \n",
      "ner_results [{'word': 'lance', 'score': 0.5259454250335693, 'entity': 'B-MISC', 'index': 4, 'start': 13, 'end': 18}]\n",
      "elem lance\n",
      "NER over initial question: \n",
      " what war was lancelot curran involved in  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where did eiler larsen end his life?\n",
      "ner_results [{'word': 'eile', 'score': 0.5580756068229675, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 14}, {'word': '##r', 'score': 0.6010677814483643, 'entity': 'I-PER', 'index': 4, 'start': 14, 'end': 15}, {'word': 'lar', 'score': 0.5585691928863525, 'entity': 'I-PER', 'index': 5, 'start': 16, 'end': 19}, {'word': '##sen', 'score': 0.6909109354019165, 'entity': 'I-PER', 'index': 6, 'start': 19, 'end': 22}]\n",
      "elem eile\n",
      "elem "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 271/400 [00:32<00:05, 22.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##r\n",
      "elem lar\n",
      "elem ##sen\n",
      "NER over initial question: \n",
      " Where did [START] eiler larsen [END] end his life ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what kind of celestial body is bd+60 2522\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what kind of celestial body is bd+60 2522 ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what type of music is played in the balladeering album\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what type of music is played in the balladeering album ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who was the artist that recorded the album destruction by definition\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " who was the artist that recorded the album destruction by definition ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What country is cornelia parker from?\n",
      "ner_results [{'word': 'corn', 'score': 0.7071775197982788, 'entity': 'B-LOC', 'index': 4, 'start': 16, 'end': 20}]\n",
      "elem corn\n",
      "NER over initial question: \n",
      " What country is cornelia parker from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What's a football team that plays in the EPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 277/400 [00:33<00:05, 21.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'EP', 'score': 0.7433998584747314, 'entity': 'B-ORG', 'index': 11, 'start': 41, 'end': 43}, {'word': '##L', 'score': 0.872963547706604, 'entity': 'I-ORG', 'index': 12, 'start': 43, 'end': 44}]\n",
      "elem EP\n",
      "elem ##L\n",
      "NER over initial question: \n",
      " What's a football team that plays in the [START] EPL [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what category does 3314 beals belong to\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what category does 3314 beals belong to ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which position in football did danny whitaker play in?\n",
      "ner_results [{'word': 'dann', 'score': 0.5065616965293884, 'entity': 'B-PER', 'index': 6, 'start': 31, 'end': 35}]\n",
      "elem dann\n",
      "NER over initial question: \n",
      " which position in football did danny whitaker play in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what was ed van der elsken's place of death\n",
      "ner_results [{'word': 'ed', 'score': 0.9841587543487549, 'entity': 'B-PER', 'index': 3, 'start': 9, 'end': 11}, {'word': 'van', 'score': 0.9953559041023254, 'entity': 'I-PER', 'index': 4, 'start': 12, 'end': 15}, {'word': 'der', 'score': 0.9920550584793091, 'entity': 'I-PER', 'index': 5, 'start': 16, 'end': 19}, {'word': 'els', 'score': 0.9815483093261719, 'entity': 'I-PER', 'index': 6, 'start': 20, 'end': 23}, {'word': '##ken', 'score': 0.9452244639396667, 'entity': 'I-PER', 'index': 7, 'start': 23, 'end': 26}]\n",
      "elem ed\n",
      "elem van\n",
      "elem der\n",
      "elem els\n",
      "elem ##ken\n",
      "NER over initial question: \n",
      " what was [START] ed van der [END] elsken's place of death ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is the gender of joe nanini\n",
      "ner_results [{'word': 'jo', 'score': 0.35356295108795166, 'entity': 'B-MISC', 'index': 6, 'start': 22, 'end': 24}, {'word': '##e', 'score': 0.35631123185157776, 'entity': 'I-MISC', 'index': 7, 'start': 24, 'end': 25}, {'word': 'nan', 'score': 0.3426685333251953, 'entity': 'I-PER', 'index': 8, 'start': 26, 'end': 29}]\n",
      "elem jo\n",
      "elem ##e\n",
      "elem nan\n",
      "NER over initial question: \n",
      " what is the gender of [START] joe [END] nanini ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where did archduchess barbara of austria perish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 280/400 [00:33<00:05, 22.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'aust', 'score': 0.66673344373703, 'entity': 'B-LOC', 'index': 10, 'start': 33, 'end': 37}]\n",
      "elem aust\n",
      "NER over initial question: \n",
      " Where did archduchess barbara of austria perish ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which films did charlie chaplin direct\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " which films did charlie chaplin direct ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who is someone born in sydney\n",
      "ner_results [{'word': 'syd', 'score': 0.9770616292953491, 'entity': 'B-LOC', 'index': 6, 'start': 23, 'end': 26}, {'word': '##ney', 'score': 0.8140395283699036, 'entity': 'I-LOC', 'index': 7, 'start': 26, 'end': 29}]\n",
      "elem syd\n",
      "elem ##ney\n",
      "NER over initial question: \n",
      " Who is someone born in [START] sydney [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What baseball position does jack ness play?\n",
      "ner_results [{'word': 'baseball', 'score': 0.6012169718742371, 'entity': 'B-ORG', 'index': 2, 'start': 5, 'end': 13}]\n",
      "elem baseball\n",
      "NER over initial question: \n",
      " What [START] baseball [END] position does jack ness play ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where is daniel petrov from\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 286/400 [00:33<00:04, 23.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'dan', 'score': 0.5127832293510437, 'entity': 'B-MISC', 'index': 3, 'start': 9, 'end': 12}]\n",
      "elem dan\n",
      "NER over initial question: \n",
      " where is daniel petrov from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which war was hermann stuckmann involved in\n",
      "ner_results [{'word': 'her', 'score': 0.4214238226413727, 'entity': 'B-PER', 'index': 6, 'start': 14, 'end': 17}, {'word': 'st', 'score': 0.273569256067276, 'entity': 'B-PER', 'index': 8, 'start': 22, 'end': 24}, {'word': '##uck', 'score': 0.4824509918689728, 'entity': 'I-PER', 'index': 9, 'start': 24, 'end': 27}]\n",
      "elem her\n",
      "elem st\n",
      "elem ##uck\n",
      "NER over initial question: \n",
      " Which war was hermann stuckmann involved in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is the gender of  gil brewer\n",
      "ner_results [{'word': 'gi', 'score': 0.31491619348526, 'entity': 'B-ORG', 'index': 6, 'start': 23, 'end': 25}]\n",
      "elem gi\n",
      "NER over initial question: \n",
      " what is the gender of  gil brewer ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what language is in the movie nightmare?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what language is in the movie nightmare ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " whose life began in celle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 289/400 [00:33<00:05, 21.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'celle', 'score': 0.5943911075592041, 'entity': 'B-LOC', 'index': 5, 'start': 20, 'end': 25}]\n",
      "elem celle\n",
      "NER over initial question: \n",
      " whose life began in [START] celle [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is the gender of nate bussey\n",
      "ner_results [{'word': 'nat', 'score': 0.4586944282054901, 'entity': 'B-MISC', 'index': 6, 'start': 22, 'end': 25}]\n",
      "elem nat\n",
      "NER over initial question: \n",
      " what is the gender of nate bussey ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " russell wade died in this southern California city. \n",
      "ner_results [{'word': 'California', 'score': 0.9995295405387878, 'entity': 'B-LOC', 'index': 9, 'start': 35, 'end': 45}]\n",
      "elem California\n",
      "NER over initial question: \n",
      " russell wade died in this southern [START] California [END] city.  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What's the title of a netflix blaxploitation film?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 294/400 [00:34<00:07, 13.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'net', 'score': 0.6284115314483643, 'entity': 'B-MISC', 'index': 8, 'start': 22, 'end': 25}, {'word': '##f', 'score': 0.23798896372318268, 'entity': 'B-MISC', 'index': 9, 'start': 25, 'end': 26}]\n",
      "elem net\n",
      "elem ##f\n",
      "NER over initial question: \n",
      " What's the title of a netflix blaxploitation film ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is the job title of jennifer byrne\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what is the job title of jennifer byrne ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What type of celestial object is 3587 descartes\n",
      "ner_results [{'word': '358', 'score': 0.49565285444259644, 'entity': 'B-LOC', 'index': 8, 'start': 33, 'end': 36}]\n",
      "elem 358\n",
      "NER over initial question: \n",
      " What type of celestial object is 3587 descartes ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which time zones is majs located in?\n",
      "ner_results [{'word': 'maj', 'score': 0.7085765600204468, 'entity': 'B-LOC', 'index': 5, 'start': 20, 'end': 23}]\n",
      "elem maj\n",
      "NER over initial question: \n",
      " which time zones is majs located in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " whats the language that was used in  lovely, still\n",
      "ner_results [{'word': 'what', 'score': 0.9210500717163086, 'entity': 'B-MISC', 'index': 1, 'start': 0, 'end': 4}]\n",
      "elem what\n",
      "NER over initial question: \n",
      " whats the language that was used in  lovely still ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which netflix genre is dinosaurs: giants of patagonia under\n",
      "ner_results [{'word': 'patag', 'score': 0.9886922240257263, 'entity': 'B-LOC', 'index': 17, 'start': 44, 'end': 49}, {'word': '##onia', 'score': 0.9402351379394531, 'entity': 'I-LOC', 'index': 18, 'start': 49, 'end': 53}]\n",
      "elem patag\n",
      "elem ##onia\n",
      "NER over initial question: \n",
      " Which netflix genre is dinosaurs: giants of [START] patagonia [END] under ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which is the main ideology of the communist party of britain?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 296/400 [00:35<00:18,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'brit', 'score': 0.4772436022758484, 'entity': 'B-ORG', 'index': 12, 'start': 53, 'end': 57}]\n",
      "elem brit\n",
      "NER over initial question: \n",
      " which is the main ideology of the communist party of britain ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where was the last place samuel marx resided before he died\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 300/400 [00:35<00:15,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'sam', 'score': 0.8285022974014282, 'entity': 'B-PER', 'index': 6, 'start': 25, 'end': 28}]\n",
      "elem sam\n",
      "NER over initial question: \n",
      " where was the last place samuel marx resided before he died ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is a location in united states\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what is a location in united states ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which label is osker signed to\n",
      "ner_results [{'word': 'os', 'score': 0.8805356025695801, 'entity': 'B-ORG', 'index': 4, 'start': 15, 'end': 17}, {'word': '##ker', 'score': 0.6847453713417053, 'entity': 'I-ORG', 'index': 5, 'start': 17, 'end': 20}]\n",
      "elem os\n",
      "elem ##ker\n",
      "NER over initial question: \n",
      " which label is [START] osker [END] signed to ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is oday taleb's nationality\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What is oday taleb's nationality ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which country is urban ghost story from?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " which country is urban ghost story from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who was the main person at the james k. polk 1845 presidential inauguration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 304/400 [00:36<00:14,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'jam', 'score': 0.46985241770744324, 'entity': 'B-LOC', 'index': 8, 'start': 31, 'end': 34}, {'word': '##es', 'score': 0.35018688440322876, 'entity': 'I-LOC', 'index': 9, 'start': 34, 'end': 36}]\n",
      "elem jam\n",
      "elem ##es\n",
      "NER over initial question: \n",
      " Who was the main person at the [START] james [END] k. polk 1845 presidential inauguration ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which village is located in allegany county\n",
      "ner_results [{'word': 'alle', 'score': 0.867059588432312, 'entity': 'B-LOC', 'index': 6, 'start': 28, 'end': 32}, {'word': '##gan', 'score': 0.3990307152271271, 'entity': 'I-LOC', 'index': 7, 'start': 32, 'end': 35}, {'word': '##y', 'score': 0.5150995254516602, 'entity': 'I-LOC', 'index': 8, 'start': 35, 'end': 36}]\n",
      "elem alle\n",
      "elem ##gan\n",
      "elem ##y\n",
      "NER over initial question: \n",
      " which village is located in [START] allegany [END] county ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is dildar ali naseerabadi's religion?\n",
      "ner_results [{'word': 'nas', 'score': 0.8776716589927673, 'entity': 'B-PER', 'index': 6, 'start': 19, 'end': 22}, {'word': '##eer', 'score': 0.6955591440200806, 'entity': 'B-PER', 'index': 7, 'start': 22, 'end': 25}, {'word': '##abad', 'score': 0.8006221055984497, 'entity': 'I-PER', 'index': 8, 'start': 25, 'end': 29}, {'word': '##i', 'score': 0.9519912600517273, 'entity': 'I-PER', 'index': 9, 'start': 29, 'end': 30}]\n",
      "elem nas\n",
      "elem ##eer\n",
      "elem ##abad\n",
      "elem ##i\n",
      "NER over initial question: \n",
      " what is dildar ali naseerabadi's religion ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what types of music is sub rosa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 306/400 [00:36<00:11,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'sub', 'score': 0.8958925604820251, 'entity': 'B-MISC', 'index': 6, 'start': 23, 'end': 26}, {'word': 'rosa', 'score': 0.7240636348724365, 'entity': 'I-MISC', 'index': 7, 'start': 27, 'end': 31}]\n",
      "elem sub\n",
      "elem rosa\n",
      "NER over initial question: \n",
      " what types of music is [START] sub rosa [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Name a tv program in the comedy-drama genre\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Name a tv program in the comedy-drama genre ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who was the author of war of the twins?\n",
      "ner_results [{'word': 'war', 'score': 0.9606212973594666, 'entity': 'B-MISC', 'index': 6, 'start': 22, 'end': 25}, {'word': 'of', 'score': 0.9072137475013733, 'entity': 'I-MISC', 'index': 7, 'start': 26, 'end': 28}, {'word': 'the', 'score': 0.8434174060821533, 'entity': 'I-MISC', 'index': 8, 'start': 29, 'end': 32}, {'word': 'twin', 'score': 0.75789475440979, 'entity': 'I-MISC', 'index': 9, 'start': 33, 'end': 37}, {'word': '##s', 'score': 0.6897526979446411, 'entity': 'I-MISC', 'index': 10, 'start': 37, 'end': 38}]\n",
      "elem war\n",
      "elem of\n",
      "elem the\n",
      "elem twin\n",
      "elem ##s\n",
      "NER over initial question: \n",
      " who was [START] the [END] author [START] of war of the twins [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what was the name of chihiro onitsuka's debut j-pop album\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 308/400 [00:36<00:13,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'chi', 'score': 0.5198078751564026, 'entity': 'B-PER', 'index': 6, 'start': 21, 'end': 24}, {'word': 'oni', 'score': 0.6093325614929199, 'entity': 'I-PER', 'index': 8, 'start': 29, 'end': 32}, {'word': '##tsu', 'score': 0.7571744918823242, 'entity': 'I-PER', 'index': 9, 'start': 32, 'end': 35}, {'word': '##ka', 'score': 0.8612977266311646, 'entity': 'I-PER', 'index': 10, 'start': 35, 'end': 37}]\n",
      "elem chi\n",
      "elem oni\n",
      "elem ##tsu\n",
      "elem ##ka\n",
      "NER over initial question: \n",
      " what was the name of chihiro onitsuka's debut j-pop album ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who was born in ogden\n",
      "ner_results [{'word': 'og', 'score': 0.9942036867141724, 'entity': 'B-LOC', 'index': 5, 'start': 16, 'end': 18}, {'word': '##den', 'score': 0.9522879719734192, 'entity': 'I-LOC', 'index': 6, 'start': 18, 'end': 21}]\n",
      "elem og\n",
      "elem ##den\n",
      "NER over initial question: \n",
      " who was born in [START] ogden [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What gender is doctor strange\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 312/400 [00:37<00:11,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " What gender is doctor strange ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where is giorgos kimoulis's place of birth\n",
      "ner_results [{'word': 'gi', 'score': 0.8584398031234741, 'entity': 'B-PER', 'index': 3, 'start': 9, 'end': 11}, {'word': '##org', 'score': 0.6307823061943054, 'entity': 'I-PER', 'index': 4, 'start': 11, 'end': 14}, {'word': '##os', 'score': 0.6880746483802795, 'entity': 'I-PER', 'index': 5, 'start': 14, 'end': 16}, {'word': 'kim', 'score': 0.6979876160621643, 'entity': 'I-PER', 'index': 6, 'start': 17, 'end': 20}, {'word': '##oul', 'score': 0.9444862604141235, 'entity': 'I-PER', 'index': 7, 'start': 20, 'end': 23}, {'word': '##is', 'score': 0.91413813829422, 'entity': 'I-PER', 'index': 8, 'start': 23, 'end': 25}]\n",
      "elem gi\n",
      "elem ##org\n",
      "elem ##os\n",
      "elem kim\n",
      "elem ##oul\n",
      "elem ##is\n",
      "NER over initial question: \n",
      " where is [START] giorgos [END] kimoulis's place of birth ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is maripol's gender\n",
      "ner_results [{'word': 'mari', 'score': 0.9113945364952087, 'entity': 'B-ORG', 'index': 3, 'start': 8, 'end': 12}, {'word': '##pol', 'score': 0.8893129229545593, 'entity': 'I-ORG', 'index': 4, 'start': 12, 'end': 15}]\n",
      "elem mari\n",
      "elem ##pol\n",
      "NER over initial question: \n",
      " What is maripol's gender ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where did ignacy potocki die\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 315/400 [00:37<00:08,  9.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'ig', 'score': 0.597157895565033, 'entity': 'B-MISC', 'index': 3, 'start': 10, 'end': 12}, {'word': 'pot', 'score': 0.49994349479675293, 'entity': 'B-MISC', 'index': 6, 'start': 17, 'end': 20}, {'word': '##ock', 'score': 0.6040316224098206, 'entity': 'I-MISC', 'index': 7, 'start': 20, 'end': 23}, {'word': '##i', 'score': 0.44252726435661316, 'entity': 'I-MISC', 'index': 8, 'start': 23, 'end': 24}]\n",
      "elem ig\n",
      "elem pot\n",
      "elem ##ock\n",
      "elem ##i\n",
      "NER over initial question: \n",
      " where did ignacy [START] potocki [END] die ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is ellen swallow richards's nationality?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what is ellen swallow richards's nationality ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what harry was born in wellington\n",
      "ner_results [{'word': 'well', 'score': 0.9116827249526978, 'entity': 'B-LOC', 'index': 7, 'start': 23, 'end': 27}, {'word': '##ington', 'score': 0.6686500906944275, 'entity': 'I-LOC', 'index': 8, 'start': 27, 'end': 33}]\n",
      "elem well\n",
      "elem ##ington\n",
      "NER over initial question: \n",
      " what harry was born in [START] wellington [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which football position does nizami hajiyev play\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 317/400 [00:37<00:07, 10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'niz', 'score': 0.4670949876308441, 'entity': 'B-ORG', 'index': 7, 'start': 29, 'end': 32}]\n",
      "elem niz\n",
      "NER over initial question: \n",
      " Which football position does nizami hajiyev play ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what album is in the genre of surf music?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what album is in the genre of surf music ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where was gladys guevarra born\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " where was gladys guevarra born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " in what language was hemlock society filmed in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 319/400 [00:37<00:07, 10.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'hem', 'score': 0.5761705636978149, 'entity': 'B-MISC', 'index': 5, 'start': 21, 'end': 24}, {'word': '##lock', 'score': 0.3946034610271454, 'entity': 'B-MISC', 'index': 6, 'start': 24, 'end': 28}]\n",
      "elem hem\n",
      "elem ##lock\n",
      "NER over initial question: \n",
      " in what language was [START] hemlock [END] society filmed in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " whats theodore s. westhusing's gender, male or female\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 324/400 [00:39<00:12,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " whats theodore s. westhusing's gender male or female ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what american football position does michael doyle play \n",
      "ner_results [{'word': 'mich', 'score': 0.9898915886878967, 'entity': 'B-PER', 'index': 6, 'start': 37, 'end': 41}, {'word': '##ael', 'score': 0.8587133288383484, 'entity': 'I-PER', 'index': 7, 'start': 41, 'end': 44}, {'word': 'do', 'score': 0.7754899859428406, 'entity': 'I-PER', 'index': 8, 'start': 45, 'end': 47}, {'word': '##yle', 'score': 0.5322471857070923, 'entity': 'I-PER', 'index': 9, 'start': 47, 'end': 50}]\n",
      "elem mich\n",
      "elem ##ael\n",
      "elem do\n",
      "elem ##yle\n",
      "NER over initial question: \n",
      " what american football position does [START] michael doyle [END] play  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which label is sean kingston signed to\n",
      "ner_results [{'word': 'sean', 'score': 0.5125130414962769, 'entity': 'B-PER', 'index': 6, 'start': 15, 'end': 19}]\n",
      "elem sean\n",
      "NER over initial question: \n",
      " Which label is [START] sean [END] kingston signed to ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where was peter yates given birth at?\n",
      "ner_results [{'word': 'pet', 'score': 0.5967643857002258, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 13}, {'word': '##er', 'score': 0.39506053924560547, 'entity': 'I-PER', 'index': 4, 'start': 13, 'end': 15}]\n",
      "elem pet\n",
      "elem ##er\n",
      "NER over initial question: \n",
      " where was [START] peter [END] yates given birth at ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what's a title of a lewis milestone film\n",
      "ner_results [{'word': 'le', 'score': 0.6944609880447388, 'entity': 'B-MISC', 'index': 8, 'start': 20, 'end': 22}]\n",
      "elem le\n",
      "NER over initial question: \n",
      " what's a title of a lewis milestone film ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where was clete boyer born?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 329/400 [00:39<00:07,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'c', 'score': 0.3895503580570221, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 11}]\n",
      "elem c\n",
      "NER over initial question: \n",
      " where was clete boyer born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is the birthplace of derek parlane\n",
      "ner_results [{'word': 'der', 'score': 0.7868996858596802, 'entity': 'B-MISC', 'index': 7, 'start': 26, 'end': 29}, {'word': '##ek', 'score': 0.5172988176345825, 'entity': 'I-MISC', 'index': 8, 'start': 29, 'end': 31}]\n",
      "elem der\n",
      "elem ##ek\n",
      "NER over initial question: \n",
      " What is the birthplace of [START] derek [END] parlane ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is the producer for bonnie scotland?\n",
      "ner_results [{'word': 'bon', 'score': 0.380317360162735, 'entity': 'B-MISC', 'index': 6, 'start': 24, 'end': 27}, {'word': '##land', 'score': 0.45309585332870483, 'entity': 'I-LOC', 'index': 10, 'start': 35, 'end': 39}]\n",
      "elem bon\n",
      "elem ##land\n",
      "NER over initial question: \n",
      " who is the producer for bonnie scotland ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who was the artist for theogonia\n",
      "ner_results [{'word': 'theo', 'score': 0.805991530418396, 'entity': 'B-MISC', 'index': 6, 'start': 23, 'end': 27}, {'word': '##gonia', 'score': 0.5810880064964294, 'entity': 'B-MISC', 'index': 7, 'start': 27, 'end': 32}]\n",
      "elem theo\n",
      "elem ##gonia\n",
      "NER over initial question: \n",
      " who was the artist for [START] theogonia [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is sophie treadwell's job?\n",
      "ner_results [{'word': 'so', 'score': 0.851310133934021, 'entity': 'B-PER', 'index': 3, 'start': 8, 'end': 10}, {'word': '##phie', 'score': 0.5982432961463928, 'entity': 'B-PER', 'index': 4, 'start': 10, 'end': 14}, {'word': 'tre', 'score': 0.8760639429092407, 'entity': 'I-PER', 'index': 5, 'start': 15, 'end': 18}, {'word': '##ad', 'score': 0.910426914691925, 'entity': 'I-PER', 'index': 6, 'start': 18, 'end': 20}, {'word': '##well', 'score': 0.8510355949401855, 'entity': 'I-PER', 'index': 7, 'start': 20, 'end': 24}]\n",
      "elem so\n",
      "elem ##phie\n",
      "elem tre\n",
      "elem ##ad\n",
      "elem ##well\n",
      "NER over initial question: \n",
      " What is [START] sophie [END] treadwell's job ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Name a musician in the j-pop genre.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 332/400 [00:39<00:05, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'j', 'score': 0.8652098774909973, 'entity': 'B-MISC', 'index': 6, 'start': 23, 'end': 24}, {'word': '-', 'score': 0.5140042901039124, 'entity': 'I-MISC', 'index': 7, 'start': 24, 'end': 25}, {'word': 'pop', 'score': 0.4476887881755829, 'entity': 'I-MISC', 'index': 8, 'start': 25, 'end': 28}]\n",
      "elem j\n",
      "elem -\n",
      "elem pop\n",
      "NER over initial question: \n",
      " Name a musician in the j-pop genre. ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which country released chocolate: deep dark secrets\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " which country released chocolate: deep dark secrets ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What country made the film nebeští jezdci\n",
      "ner_results [{'word': 'ne', 'score': 0.45188477635383606, 'entity': 'B-MISC', 'index': 6, 'start': 27, 'end': 29}, {'word': '##be', 'score': 0.3445037603378296, 'entity': 'I-LOC', 'index': 7, 'start': 29, 'end': 31}, {'word': '##ští', 'score': 0.3300865590572357, 'entity': 'I-ORG', 'index': 8, 'start': 31, 'end': 34}, {'word': 'je', 'score': 0.3019975423812866, 'entity': 'I-LOC', 'index': 9, 'start': 35, 'end': 37}, {'word': '##zd', 'score': 0.38079923391342163, 'entity': 'I-MISC', 'index': 10, 'start': 37, 'end': 39}, {'word': '##ci', 'score': 0.27961164712905884, 'entity': 'I-MISC', 'index': 11, 'start': 39, 'end': 41}]\n",
      "elem ne\n",
      "elem ##be\n",
      "elem ##ští\n",
      "elem je\n",
      "elem ##zd\n",
      "elem ##ci\n",
      "NER over initial question: \n",
      " What country made the film [START] nebeští jezdci [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where was atanas chipilov born\n",
      "ner_results [{'word': 'ata', 'score': 0.9632896184921265, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 13}, {'word': '##nas', 'score': 0.7455505132675171, 'entity': 'I-PER', 'index': 4, 'start': 13, 'end': 16}, {'word': 'chip', 'score': 0.48042893409729004, 'entity': 'I-PER', 'index': 5, 'start': 17, 'end': 21}, {'word': '##ilo', 'score': 0.6102108359336853, 'entity': 'I-PER', 'index': 6, 'start': 21, 'end': 24}, {'word': '##v', 'score': 0.8246822953224182, 'entity': 'I-PER', 'index': 7, 'start': 24, 'end': 25}]\n",
      "elem ata\n",
      "elem ##nas\n",
      "elem chip\n",
      "elem ##ilo\n",
      "elem ##v\n",
      "NER over initial question: \n",
      " Where was [START] atanas chipilov [END] born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who directed the film the dangerous flirt?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Who directed the film the dangerous flirt ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which musician was born in elkhart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 337/400 [00:39<00:04, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'elk', 'score': 0.3434371054172516, 'entity': 'B-LOC', 'index': 6, 'start': 27, 'end': 30}]\n",
      "elem elk\n",
      "NER over initial question: \n",
      " which musician was born in elkhart ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is london keys's ethnicity?\n",
      "ner_results [{'word': 'lo', 'score': 0.9935789108276367, 'entity': 'B-MISC', 'index': 3, 'start': 8, 'end': 10}, {'word': '##ndo', 'score': 0.9365686774253845, 'entity': 'B-MISC', 'index': 4, 'start': 10, 'end': 13}]\n",
      "elem lo\n",
      "elem ##ndo\n",
      "NER over initial question: \n",
      " What is london keys's ethnicity ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What gender was philip sendak born as?\n",
      "ner_results [{'word': 'phi', 'score': 0.6339198350906372, 'entity': 'B-MISC', 'index': 4, 'start': 16, 'end': 19}]\n",
      "elem phi\n",
      "NER over initial question: \n",
      " What gender was philip sendak born as ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what's the name of a midfielder from spain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 339/400 [00:40<00:05, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " what's the name of a midfielder from spain ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what type of bridge is bear mountain bridge\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what type of bridge is bear mountain bridge ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what ethnicity is linda w. cropp\n",
      "ner_results [{'word': 'li', 'score': 0.4431682527065277, 'entity': 'B-MISC', 'index': 5, 'start': 18, 'end': 20}]\n",
      "elem li\n",
      "NER over initial question: \n",
      " what ethnicity is linda w. cropp ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who created the program birds in the bush \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 344/400 [00:40<00:03, 15.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'birds', 'score': 0.4679976999759674, 'entity': 'B-MISC', 'index': 5, 'start': 24, 'end': 29}]\n",
      "elem birds\n",
      "NER over initial question: \n",
      " who created the program [START] birds [END] in the bush  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which position in the infield did cory bailey play\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " which position in the infield did cory bailey play ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who is a notable defender in football\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Who is a notable defender in football ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where is roscoe h. hillenkoetter buried \n",
      "ner_results [{'word': 'ros', 'score': 0.5485619902610779, 'entity': 'B-LOC', 'index': 3, 'start': 9, 'end': 12}]\n",
      "elem ros\n",
      "NER over initial question: \n",
      " where is roscoe h. hillenkoetter buried  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what ethnicity is rose jackson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 348/400 [00:40<00:03, 14.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " what ethnicity is rose jackson ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where was rowland hill (postal reformer) buried?\n",
      "ner_results [{'word': 'row', 'score': 0.7333205342292786, 'entity': 'B-LOC', 'index': 3, 'start': 10, 'end': 13}]\n",
      "elem row\n",
      "NER over initial question: \n",
      " Where was rowland hill (postal reformer) buried ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " This is the city where steve sarkisian was born\n",
      "ner_results [{'word': 'sa', 'score': 0.9096394777297974, 'entity': 'B-PER', 'index': 8, 'start': 29, 'end': 31}, {'word': '##rki', 'score': 0.7825726866722107, 'entity': 'I-PER', 'index': 9, 'start': 31, 'end': 34}, {'word': '##sian', 'score': 0.8939347863197327, 'entity': 'I-PER', 'index': 10, 'start': 34, 'end': 38}]\n",
      "elem sa\n",
      "elem ##rki\n",
      "elem ##sian\n",
      "NER over initial question: \n",
      " This is the city where steve [START] sarkisian [END] was born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where was fred bongusto born \n",
      "ner_results [{'word': 'fred', 'score': 0.8422307968139648, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 14}]\n",
      "elem fred\n",
      "NER over initial question: \n",
      " where was [START] fred [END] bongusto born  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is a film in the genre documentary film?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 350/400 [00:40<00:03, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " What is a film in the genre documentary film ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who composed believe\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " who composed believe ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " in what country was a switchback railway filmed in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 354/400 [00:41<00:02, 17.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " in what country was a switchback railway filmed in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " whats the gender of  nuno santos\n",
      "ner_results [{'word': 'nun', 'score': 0.48069679737091064, 'entity': 'B-PER', 'index': 6, 'start': 21, 'end': 24}, {'word': '##o', 'score': 0.5177805423736572, 'entity': 'I-PER', 'index': 7, 'start': 24, 'end': 25}]\n",
      "elem nun\n",
      "elem ##o\n",
      "NER over initial question: \n",
      " whats the gender of  [START] nuno [END] santos ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who created the the character edward cullen\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Who created the the character edward cullen ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is a netflix horror film\n",
      "ner_results [{'word': 'net', 'score': 0.8639613389968872, 'entity': 'B-MISC', 'index': 4, 'start': 10, 'end': 13}, {'word': '##f', 'score': 0.4134286046028137, 'entity': 'I-MISC', 'index': 5, 'start': 13, 'end': 14}, {'word': '##li', 'score': 0.5120739340782166, 'entity': 'I-MISC', 'index': 6, 'start': 14, 'end': 16}, {'word': '##x', 'score': 0.8132520914077759, 'entity': 'I-MISC', 'index': 7, 'start': 16, 'end': 17}]\n",
      "elem net\n",
      "elem ##f\n",
      "elem ##li\n",
      "elem ##x\n",
      "NER over initial question: \n",
      " what is a [START] netflix [END] horror film ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what film is directed by richard fleischer?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what film is directed by richard fleischer ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which country is pascale arbillot from\n",
      "ner_results [{'word': 'pas', 'score': 0.5555003881454468, 'entity': 'B-LOC', 'index': 4, 'start': 17, 'end': 20}]\n",
      "elem pas\n",
      "NER over initial question: \n",
      " which country is pascale arbillot from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What is donald crews's gender\n",
      "ner_results [{'word': 'dona', 'score': 0.5253748893737793, 'entity': 'B-ORG', 'index': 3, 'start': 8, 'end': 12}]\n",
      "elem dona\n",
      "NER over initial question: \n",
      " What is donald crews's gender ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where was g. stanley hall born\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 361/400 [00:41<00:01, 20.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'stan', 'score': 0.9227929711341858, 'entity': 'B-LOC', 'index': 5, 'start': 13, 'end': 17}]\n",
      "elem stan\n",
      "NER over initial question: \n",
      " Where was g. stanley hall born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where did léon kauffman die\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Where did léon kauffman die ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what country gives nationality to ibrahim mahlab\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what country gives nationality to ibrahim mahlab ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is jim matt's genre of choice?\n",
      "ner_results [{'word': 'jim', 'score': 0.4659993648529053, 'entity': 'B-MISC', 'index': 3, 'start': 8, 'end': 11}]\n",
      "elem jim\n",
      "NER over initial question: \n",
      " what is [START] jim [END] matt's genre of choice ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Name a drama film found on Netflix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 367/400 [00:41<00:01, 21.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'Netflix', 'score': 0.9945391416549683, 'entity': 'B-ORG', 'index': 7, 'start': 27, 'end': 34}]\n",
      "elem Netflix\n",
      "NER over initial question: \n",
      " Name a drama film found on [START] Netflix [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is kennedy space center named after?\n",
      "ner_results [{'word': 'ken', 'score': 0.7743901014328003, 'entity': 'B-LOC', 'index': 3, 'start': 7, 'end': 10}, {'word': '##ned', 'score': 0.45777788758277893, 'entity': 'I-LOC', 'index': 4, 'start': 10, 'end': 13}, {'word': '##y', 'score': 0.46428176760673523, 'entity': 'I-LOC', 'index': 5, 'start': 13, 'end': 14}]\n",
      "elem ken\n",
      "elem ##ned\n",
      "elem ##y\n",
      "NER over initial question: \n",
      " who is [START] kennedy [END] space center named after ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which position in basketball does tyler zeller play?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " which position in basketball does tyler zeller play ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what language is cold fever filmed in?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what language is cold fever filmed in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where was tetsuzô Ôsawa born?\n",
      "ner_results [{'word': 'te', 'score': 0.9980255961418152, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 12}, {'word': '##tsu', 'score': 0.7280230522155762, 'entity': 'B-PER', 'index': 4, 'start': 12, 'end': 15}, {'word': '##z', 'score': 0.9250954389572144, 'entity': 'I-PER', 'index': 5, 'start': 15, 'end': 16}, {'word': '##ô', 'score': 0.9861956238746643, 'entity': 'I-PER', 'index': 6, 'start': 16, 'end': 17}, {'word': 'Ô', 'score': 0.9994287490844727, 'entity': 'I-PER', 'index': 7, 'start': 18, 'end': 19}, {'word': '##saw', 'score': 0.9997416138648987, 'entity': 'I-PER', 'index': 8, 'start': 19, 'end': 22}, {'word': '##a', 'score': 0.999600350856781, 'entity': 'I-PER', 'index': 9, 'start': 22, 'end': 23}]\n",
      "elem te\n",
      "elem ##tsu\n",
      "elem ##z\n",
      "elem ##ô\n",
      "elem Ô\n",
      "elem ##saw\n",
      "elem ##a\n",
      "NER over initial question: \n",
      " Where was [START] tetsuzô Ôsawa [END] born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which country released the movie once upon a time\n",
      "ner_results [{'word': 'which', 'score': 0.3056555986404419, 'entity': 'B-LOC', 'index': 1, 'start': 0, 'end': 5}]\n",
      "elem which\n",
      "NER over initial question: \n",
      " [START] which [END] country released the movie once upon a time ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Name a notable thrash metal artist\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " Name a notable thrash metal artist ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " fabric 14: stacey pullen is what type of music\n",
      "ner_results [{'word': 'sta', 'score': 0.6661722660064697, 'entity': 'B-MISC', 'index': 5, 'start': 11, 'end': 14}, {'word': '##cey', 'score': 0.6219815015792847, 'entity': 'I-MISC', 'index': 6, 'start': 14, 'end': 17}]\n",
      "elem sta\n",
      "elem ##cey\n",
      "NER over initial question: \n",
      " fabric 14: [START] stacey [END] pullen is what type of music ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who's the directed credited for anna and the king of siam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 373/400 [00:42<00:02, 10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " Who's the directed credited for anna and the king of siam ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who is a person born in breda\n",
      "ner_results [{'word': 'bred', 'score': 0.37964439392089844, 'entity': 'B-MISC', 'index': 7, 'start': 24, 'end': 28}, {'word': '##a', 'score': 0.2564886808395386, 'entity': 'B-MISC', 'index': 8, 'start': 28, 'end': 29}]\n",
      "elem bred\n",
      "elem ##a\n",
      "NER over initial question: \n",
      " who is a person born in [START] breda [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " is mike burgoyne male or female\n",
      "ner_results [{'word': 'bu', 'score': 0.820504367351532, 'entity': 'B-MISC', 'index': 4, 'start': 8, 'end': 10}, {'word': '##rgo', 'score': 0.4373452365398407, 'entity': 'B-MISC', 'index': 5, 'start': 10, 'end': 13}]\n",
      "elem bu\n",
      "elem ##rgo\n",
      "NER over initial question: \n",
      " is mike burgoyne male or female ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What country was the film tales that witness madness?\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " What country was the film tales that witness madness ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Who's a pianist born in philadelphia\n",
      "ner_results [{'word': 'phi', 'score': 0.8154208660125732, 'entity': 'B-LOC', 'index': 8, 'start': 24, 'end': 27}]\n",
      "elem phi\n",
      "NER over initial question: \n",
      " Who's a pianist born in philadelphia ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What's an adventure game made by telltale games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 379/400 [00:42<00:01, 15.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'tell', 'score': 0.7400925159454346, 'entity': 'B-MISC', 'index': 9, 'start': 33, 'end': 37}]\n",
      "elem tell\n",
      "NER over initial question: \n",
      " What's an adventure game made by telltale games ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Which country created the film thunichal\n",
      "ner_results [{'word': 'thu', 'score': 0.5032739639282227, 'entity': 'B-MISC', 'index': 8, 'start': 31, 'end': 34}]\n",
      "elem thu\n",
      "NER over initial question: \n",
      " Which country created the film thunichal ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " Where was the american actor paul america born?\n",
      "ner_results [{'word': 'american', 'score': 0.7626768946647644, 'entity': 'B-MISC', 'index': 4, 'start': 14, 'end': 22}]\n",
      "elem american\n",
      "NER over initial question: \n",
      " Where was the [START] american [END] actor paul america born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which production company made piglet's big movie\n",
      "ner_results [{'word': 'pi', 'score': 0.39014527201652527, 'entity': 'B-PER', 'index': 5, 'start': 30, 'end': 32}]\n",
      "elem pi\n",
      "NER over initial question: \n",
      " which production company made piglet's big movie ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what gameplay mode does feel the magic: xy/xx have\n",
      "ner_results [{'word': 'x', 'score': 0.7850238680839539, 'entity': 'B-MISC', 'index': 9, 'start': 40, 'end': 41}]\n",
      "elem x\n",
      "NER over initial question: \n",
      " what gameplay mode does feel the magic: xy/xx have ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where was john mckelvey born \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 382/400 [00:42<00:01, 16.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'jo', 'score': 0.9759240746498108, 'entity': 'B-PER', 'index': 3, 'start': 10, 'end': 12}, {'word': '##hn', 'score': 0.7851604223251343, 'entity': 'I-PER', 'index': 4, 'start': 12, 'end': 14}, {'word': 'm', 'score': 0.9388988614082336, 'entity': 'I-PER', 'index': 5, 'start': 15, 'end': 16}, {'word': '##ckel', 'score': 0.9205625057220459, 'entity': 'I-PER', 'index': 6, 'start': 16, 'end': 20}, {'word': '##vey', 'score': 0.9537380337715149, 'entity': 'I-PER', 'index': 7, 'start': 20, 'end': 23}]\n",
      "elem jo\n",
      "elem ##hn\n",
      "elem m\n",
      "elem ##ckel\n",
      "elem ##vey\n",
      "NER over initial question: \n",
      " where was [START] john mckelvey [END] born  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what gymnast is the daughter of terry yorath \n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what gymnast is the daughter of terry yorath  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is mike mcgee most known for\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what is mike mcgee most known for ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what type of film is lips of blood\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what type of film is lips of blood ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where in britain was sarah badel born?\n",
      "ner_results [{'word': 'brit', 'score': 0.6837582588195801, 'entity': 'B-MISC', 'index': 3, 'start': 9, 'end': 13}]\n",
      "elem brit\n",
      "NER over initial question: \n",
      " where in britain was sarah badel born ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " The album titled profile ii: the best of emmylou harris is from which artist?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 388/400 [00:44<00:01,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'em', 'score': 0.6562450528144836, 'entity': 'B-PER', 'index': 10, 'start': 41, 'end': 43}, {'word': '##my', 'score': 0.45618554949760437, 'entity': 'I-PER', 'index': 11, 'start': 43, 'end': 45}, {'word': '##lou', 'score': 0.4335818886756897, 'entity': 'I-PER', 'index': 12, 'start': 45, 'end': 48}]\n",
      "elem em\n",
      "elem ##my\n",
      "elem ##lou\n",
      "NER over initial question: \n",
      " The album titled profile ii: the best of [START] emmylou [END] harris is from which artist ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " of which language is cash mccall in\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " of which language is cash mccall in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what instrument does víctor espínola play \n",
      "ner_results [{'word': 'ví', 'score': 0.7170430421829224, 'entity': 'B-MISC', 'index': 4, 'start': 21, 'end': 23}, {'word': '##ctor', 'score': 0.4038808345794678, 'entity': 'I-MISC', 'index': 5, 'start': 23, 'end': 27}, {'word': 'es', 'score': 0.3556306064128876, 'entity': 'I-MISC', 'index': 6, 'start': 28, 'end': 30}, {'word': '##pí', 'score': 0.5247237682342529, 'entity': 'I-MISC', 'index': 7, 'start': 30, 'end': 32}, {'word': '##nol', 'score': 0.6705613732337952, 'entity': 'I-MISC', 'index': 8, 'start': 32, 'end': 35}]\n",
      "elem ví\n",
      "elem ##ctor\n",
      "elem es\n",
      "elem ##pí\n",
      "elem ##nol\n",
      "NER over initial question: \n",
      " what instrument does [START] víctor [END] espínola play  ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " made from technetium could be best described as what?\n",
      "ner_results [{'word': 'tech', 'score': 0.5973988771438599, 'entity': 'B-MISC', 'index': 3, 'start': 10, 'end': 14}]\n",
      "elem tech\n",
      "NER over initial question: \n",
      " made from technetium could be best described as what ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what county is gettysburg from?\n",
      "ner_results [{'word': 'get', 'score': 0.9608449935913086, 'entity': 'B-LOC', 'index': 4, 'start': 15, 'end': 18}, {'word': '##tys', 'score': 0.8874747157096863, 'entity': 'I-LOC', 'index': 5, 'start': 18, 'end': 21}, {'word': '##burg', 'score': 0.9941928386688232, 'entity': 'I-LOC', 'index': 6, 'start': 21, 'end': 25}]\n",
      "elem get\n",
      "elem ##tys\n",
      "elem ##burg\n",
      "NER over initial question: \n",
      " what county is [START] gettysburg [END] from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What conflict did friedrich jeckeln partake in?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 393/400 [00:44<00:00,  9.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'fri', 'score': 0.6010605692863464, 'entity': 'B-MISC', 'index': 4, 'start': 18, 'end': 21}]\n",
      "elem fri\n",
      "NER over initial question: \n",
      " What conflict did friedrich jeckeln partake in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which country is mackenzie davis from\n",
      "ner_results [{'word': 'mac', 'score': 0.790265679359436, 'entity': 'B-LOC', 'index': 4, 'start': 17, 'end': 20}, {'word': '##ken', 'score': 0.4484028220176697, 'entity': 'I-LOC', 'index': 5, 'start': 20, 'end': 23}, {'word': '##zie', 'score': 0.47748467326164246, 'entity': 'I-LOC', 'index': 6, 'start': 23, 'end': 26}]\n",
      "elem mac\n",
      "elem ##ken\n",
      "elem ##zie\n",
      "NER over initial question: \n",
      " which country is [START] mackenzie [END] davis from ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what country was breaking the waves released in\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " what country was breaking the waves released in ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " who was born in teddington\n",
      "ner_results [{'word': 'te', 'score': 0.9984838962554932, 'entity': 'B-LOC', 'index': 5, 'start': 16, 'end': 18}, {'word': '##ddin', 'score': 0.9855822324752808, 'entity': 'I-LOC', 'index': 6, 'start': 18, 'end': 22}, {'word': '##gton', 'score': 0.9795512557029724, 'entity': 'I-LOC', 'index': 7, 'start': 22, 'end': 26}]\n",
      "elem te\n",
      "elem ##ddin\n",
      "elem ##gton\n",
      "NER over initial question: \n",
      " who was born in [START] teddington [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What genre of tv program is grass?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 396/400 [00:44<00:00, 11.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results []\n",
      "NER over initial question: \n",
      " What genre of tv program is grass ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " which language is spoken in the show noah\n",
      "ner_results [{'word': 'no', 'score': 0.9754449725151062, 'entity': 'B-MISC', 'index': 8, 'start': 37, 'end': 39}, {'word': '##ah', 'score': 0.9096447229385376, 'entity': 'I-MISC', 'index': 9, 'start': 39, 'end': 41}]\n",
      "elem no\n",
      "elem ##ah\n",
      "NER over initial question: \n",
      " which language is spoken in the show [START] noah [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " in which country was knife edge filmed\n",
      "ner_results []\n",
      "NER over initial question: \n",
      " in which country was knife edge filmed ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " where was jean-christophe mitterrand's place of birth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:45<00:00,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_results [{'word': 'mitte', 'score': 0.6500059366226196, 'entity': 'B-PER', 'index': 9, 'start': 26, 'end': 31}, {'word': '##rran', 'score': 0.5632789731025696, 'entity': 'I-PER', 'index': 10, 'start': 31, 'end': 35}, {'word': '##d', 'score': 0.601681113243103, 'entity': 'I-PER', 'index': 11, 'start': 35, 'end': 36}]\n",
      "elem mitte\n",
      "elem ##rran\n",
      "elem ##d\n",
      "NER over initial question: \n",
      " where was jean-christophe mitterrand's place of birth ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " What category of celestial object is 15412 schaefer?\n",
      "ner_results [{'word': '1541', 'score': 0.826678991317749, 'entity': 'B-LOC', 'index': 8, 'start': 37, 'end': 41}, {'word': '##2', 'score': 0.48941630125045776, 'entity': 'I-LOC', 'index': 9, 'start': 41, 'end': 42}, {'word': 's', 'score': 0.4831588566303253, 'entity': 'I-LOC', 'index': 10, 'start': 43, 'end': 44}, {'word': '##cha', 'score': 0.7365809679031372, 'entity': 'I-LOC', 'index': 11, 'start': 44, 'end': 47}, {'word': '##efer', 'score': 0.7689307928085327, 'entity': 'I-LOC', 'index': 12, 'start': 47, 'end': 51}]\n",
      "elem 1541\n",
      "elem ##2\n",
      "elem s\n",
      "elem ##cha\n",
      "elem ##efer\n",
      "NER over initial question: \n",
      " What category of celestial object is [START] 15412 schaefer [END] ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what kind of music does jim conway play\n",
      "ner_results [{'word': 'jim', 'score': 0.3688376247882843, 'entity': 'B-ORG', 'index': 6, 'start': 24, 'end': 27}]\n",
      "elem jim\n",
      "NER over initial question: \n",
      " what kind of music does [START] jim [END] conway play ?\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Initial question: \n",
      " what is mballa zambo's gender?\n",
      "ner_results [{'word': 'm', 'score': 0.4417704641819, 'entity': 'B-MISC', 'index': 3, 'start': 8, 'end': 9}, {'word': '##ball', 'score': 0.3328281342983246, 'entity': 'I-PER', 'index': 4, 'start': 9, 'end': 13}, {'word': '##a', 'score': 0.5876283049583435, 'entity': 'I-PER', 'index': 5, 'start': 13, 'end': 14}, {'word': 'za', 'score': 0.5087464451789856, 'entity': 'I-PER', 'index': 6, 'start': 15, 'end': 17}, {'word': '##mbo', 'score': 0.48250100016593933, 'entity': 'I-PER', 'index': 7, 'start': 17, 'end': 20}]\n",
      "elem m\n",
      "elem ##ball\n",
      "elem ##a\n",
      "elem za\n",
      "elem ##mbo\n",
      "NER over initial question: \n",
      " what is [START] mballa [END] zambo's gender ?\n",
      "\n",
      "\n",
      "####################################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# NER\n",
    "changed_texts = []\n",
    "for question in tqdm(texts):\n",
    "    print(\"Initial question: \\n\", question)\n",
    "    \n",
    "    # load text\n",
    "    ner.receive_text(text = question)\n",
    "    \n",
    "    # receive text with marked entities\n",
    "    new_text = ner.text_with_marked_entities()\n",
    "    changed_texts.append(new_text)\n",
    "    print(\"NER over initial question: \\n\", new_text)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = data.sample(n = n, replace = False, random_state=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'changed_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-b1049f0a8404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_with_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchanged_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'changed_texts' is not defined"
     ]
    }
   ],
   "source": [
    "df[\"question_with_tokens\"] = changed_texts\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:03:22.459928\n"
     ]
    }
   ],
   "source": [
    "n = 400\n",
    "start_time = datetime.now()\n",
    "\n",
    "\n",
    "mGENRE_results = model_mGENRE.sample(\n",
    "                                    list(df[\"question_with_tokens\"]),\n",
    "                                    beam = 3,\n",
    "                                    prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
    "                                        e for e in trie.get(sent.tolist())\n",
    "                                        if e < len(model_mGENRE.task.target_dictionary)\n",
    "                                    ],\n",
    "                                    text_to_id=lambda x: max(lang_title2wikidataID[tuple(reversed(x.split(\" >> \")))], key=lambda y: int(y[1:])),\n",
    "                                    marginalize=True,\n",
    "                                    verbose = True\n",
    "                                )\n",
    "\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold =  -10.00 \t accuracy =  21.00 %\t number of observations =  400 \t share of observations =  100.00 %\n",
      "threshold =  -3.00 \t accuracy =  21.71 %\t number of observations =  387 \t share of observations =  96.75 %\n",
      "threshold =  -2.00 \t accuracy =  24.17 %\t number of observations =  331 \t share of observations =  82.75 %\n",
      "threshold =  -1.50 \t accuracy =  27.14 %\t number of observations =  269 \t share of observations =  67.25 %\n",
      "threshold =  -1.00 \t accuracy =  34.09 %\t number of observations =  176 \t share of observations =  44.00 %\n",
      "threshold =  -0.75 \t accuracy =  34.75 %\t number of observations =  141 \t share of observations =  35.25 %\n",
      "threshold =  -0.60 \t accuracy =  38.26 %\t number of observations =  115 \t share of observations =  28.75 %\n",
      "threshold =  -0.40 \t accuracy =  44.59 %\t number of observations =  74 \t share of observations =  18.50 %\n",
      "threshold =  -0.20 \t accuracy =  57.14 %\t number of observations =  35 \t share of observations =  8.75 %\n",
      "threshold =  -0.10 \t accuracy =  90.91 %\t number of observations =  11 \t share of observations =  2.75 %\n"
     ]
    }
   ],
   "source": [
    "thresholds = [-10, -3, -2, -1.5, -1, -0.75, -0.6, -0.4, -0.2, -0.1]\n",
    "accuracy_400_ner_top_1 = []\n",
    "share_of_observations_400_ner_top_1 = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    certain_out = [i[0]['id'] for i in mGENRE_results if i[0]['score'] > threshold]\n",
    "    indexes = [1 if i[0]['score'] > threshold else 0 for i in mGENRE_results]\n",
    "    y_true = list(compress(list(df.loc[:,\"object\"]), indexes))\n",
    "    \n",
    "    result = [x in certain_out for x in y_true] \n",
    "    accuracy = np.round(sum(result)/len(result), 4)*100\n",
    "    accuracy_400_ner_top_1.append(accuracy)\n",
    "    share = np.round(len(result)/n*100, 4)\n",
    "    share_of_observations_400_ner_top_1.append(share)\n",
    "    \n",
    "    print(\"threshold = \", format(threshold, '.2f'), \"\\t\",\n",
    "          \"accuracy = \", format(accuracy, '.2f'), \"%\\t\", \n",
    "          \"number of observations = \", len(result), '\\t',\n",
    "          \"share of observations = \",  format(share, '.2f'), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtYAAAH5CAYAAAClAnm4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABxr0lEQVR4nO3deXhU5fn/8fcdZFUQZBNZRCq4r6BGBETBfd/FDbWttdW61bW2RWrrigtq9Vf9asUVcKs7qAiySFRQVNQKLggJq+wCAUKe3x/3CZmESUjIJGcy+byuK9dkzjlz5p4M0c88uc/zWAgBERERERGpmqy4CxARERERyQQK1iIiIiIiKaBgLSIiIiKSAgrWIiIiIiIpoGAtIiIiIpICCtYiIiIiIimgYC0idZqZvW1mA2vouYKZ7Zzic/7ZzP4vleeUspnZLWb2TNx11JTq+DcrkskUrEUyjJk1MLO/mdm3ZrbKzPKi8HhkwjGzzGyNmf2S8PVQtO/C6H+m15c6b66Z9Y2+v8XM1kePW2ZmH5rZwQnH9jWzwlLn/yXxmGp8/ZUKAiGEY0IIwyp47nFm9pstry71Qgi3hRDSqqbyRP82g5n1T9jW0MyeMLMVZjbfzK4p9Zh+ZvY/M1ttZmPNbMear1xEZPMUrEUyz4vAScAFQAtgJ2AocFyp404IIWyT8HV5wr4lwPVm1rSc5xkRQtgGaAWMBV4otX9uqfNvE0KYXJUXVh4z26q6zi2pYWa/As4A5pXadQvQFdgROAz/t3d09JhWwMvAX4HtgCnAiAo+n/5NiEiNUrAWSXPR6PJ1ZvZFNAL9uJm1jUahV5rZe2bWIjq2P3AEcFII4aMQwrroa1QI4cpKPO03wGTgms0dGEIoAJ4F2ptZ6y14iZhZRzN72cwWmdniotHzaN/FZvaNmS01s9GJo5XRyOdlZjYTmGlm46Ndn0cj5GeZWQszeyM699Lo+w4J59g4Ch2N1k80syHRsT+a2THRvn8CvYGHikb4zexfZnZPqdfympldXc7LPdbMfjCzn83sbjPLih73KzN7P3r9P5vZs2bWPOG8N0R/fVgZ/TWiX7R9Y2uCmXWOfiYDzWx2dJ6bE87R2MyGRa/tGzO73sxyy3lfeprZJ2a2PLrtWerndquZTYpqeicKweX5F3ADsK7U9oHArSGEpSGEb4DHgAujfacCX4UQXggh5OMhfB8z27WMmmdFP6svgFVmtpWZ3Whm30d1fm1mpyQcX+Z7Hu3fycw+iB77Lv5BMvH5TjSzr8z/cjPOzHYrVUuFfneTvI5W0b/VZWa2xMwmJPxb2dzrmWRm90WP/SF6Hy80szlmttASWp/M7Ekz+39m9m50vg+sjL8ImP9lYUj0b2tB9LjGm6tXpC7RP3qR2uE0PDB3A04A3gb+DLTGf4+viI7rD3wUQigzLFXCX4GrzGy78g4yswb46PhiYGlln8TM6gFvAD8BnYH2wPBo30n46zwVf60TgOdLneJk4CBg9xBCn2jbPtEI+Qj85/MffDS0E7AGeIiyHQR8iweou4DHzcxCCDdHz395wgj/MGBAQuBphb8Hz5Vz/lOAHsD++F8WLi76UQC3AzsAuwEd8RCJme0CXA4cEEJoChwFzCrnOXoBuwD9gL8lhL1B+M+4C/7v6byyThC9728CDwAtgXuBN82sZcJh5wAXAW2ABsC15ZzvDGBtCOGtUttbAO2AzxM2fw7sEX2/R+K+EMIq4PuE/ckMwP9C0zz64Pc9/qFoW2Aw8IyZtUs4Pul7Hu17Dpga7bsV/xBQVHs3/N/jVfi/z7eA16PfiSIV/d0t7U9AbnRc2+gxIdpXkdfzBf6+PYf/Ph0A7Iy/5w+Z2TYJx58bvbZWwDT8g3Iyd0SvY9/oXO2Bv1WgXpE6Q8FapHZ4MISwIISQh4e7j0IIn0UjeK8A+0XHtQLmFz3IzLaLRpCWm1l+qXP+N9pX9PXbxJ0hhGnAu/gIYzJnmtkyPKj+Fjg9CjFFdih1/mVmtnWS8xyIh8nrQgirQgj5IYSJ0b5LgdtDCN9E574N2LfUiNrtIYQlIYQ1yYoMISwOIbwUQlgdQlgJ/BM4tIzXBPBTCOGxEMIGPDi3w4NCsnN/DCzHAyzA2cC4EMKCcs5/Z1TvbOB+PAQSQvguhPBuCGFtCGERHmSL6twANAR2N7P6IYRZIYTvy3mOwSGENSGEz/FQuk+0/UzgtmhkOBcPzWU5DpgZQng6hFAQQnge+B8eDov8J4QwI/rZj8QD1ybMW4puA5L91aQo4C1P2LYcaJqwfzklJe5P5oEQwpyifxPRaPfcEEJh9GFrJv7vrkjS99zMOuGB9K/R+zIeeD3hcWcBb0bv23pgCNAY6JlwTEV/d0tbH9WxYwhhfQhhQgghVPD1/BhC+E/0ekbgH9L+Hr2Gd/C/GCReh/BmCGF8CGEtcDNwsJl1TCwm+qBxCXB19O93Jf6enr25ekXqEgVrkdohMaitSXK/KJwsxv/nBkD0P8DmQHc8mCU6OYTQPOHrsSTP+zfg92aWLFiOjM7dFpgePUeiuaXO3zwabSytIx5sCpLs2xEYWhTM8d5vw0fKisxJ8riNzKyJmf3bzH4ysxXAeKB5NFKezMYPJiGE1dG325RxLHgQKxr5PQ94urx6StX7E/6hgqhFYLh5u8cK4BmitoMQwnf4qOgtwMLouB3KeY75Cd+vTqh/h1LPX97PboeovkQ/UfJnX9bzlHYL8HQIYVaSfb9Et80StjUDVibsb0ZJifuTKfG6zOwCM5uW8O9oT0q2dJT1nu8ALC317zbxZ1LiZxRCKIyeO/FnVNHf3dLuBr4D3onaOW6sxOsp/RyU+rBX+nk3/rxCCL/gv2el/321BpoAUxOed1S0vdx6ReoSBWuRzDIGOMASeoirIoTwP/zCsZvLOeZnfCTrllJ/jq6oOUAnS36h2Rzgd6XCeeMQwoeJJWzm/H/C2yIOCiE0A4raRazsh5Qp2XM9A5xkZvvgLRz/3cw5EkcCOwFzo+9vi86/V1TneYk1hhCeCyH0wj9sBODOLah/HpD4b6NjWQdGdZXute0E5G3B8/YDrjCf8WN+9LwjzeyGEMLSqK59Eo7fB/gq+v6rxH3RXz1+lbA/mY3vU/TXjcfwVpqW0YfB6VTs/Z8HtCj1l5ZOCd+X+BlFo7od2bKfUQkhhJUhhD+FELoAJwLXmM+OUpXXU5aN/w6iFpHtKP53WeRnPJDvkfC7uG3wC5jLrLcKNYnUSgrWIhkk+jPvWLzN4yDzqffqA9lVOO1gvI+2eTnP+y0wGri+rGPK8TEeYO4ws63NrJGZHRLt+3/ATWa2B4CZbRv16pZnAd5DXKQpHgiWRX3Dg7agxrLOTdRS8Qk+Uv1SWS0pCa4zv6CyI94aUTTDRVN8dHa5mbUHrit6gJntYmaHm1lDID96PYVbUP9I/OfZInqOy8s59i2gm5mdY34B4FnA7ng/fGX1w0dV942+5gK/wy9mBHgK+EtU1654a9GT0b5XgD3N7DQza4T/FeWL6ENfRWyNB+1FAGZ2UVTLZoUQfsJnIRkc/S71omQrzEjguCjw1sc/xK0FPtz0bJVjZseb2c5RWF+OtwMVVuX1lONYM+sV9YbfCuSEEEqM+kej8Y8B95lZm+i525vZUZupV6ROUbAWyTyn4OHnGWAZ8CN+cdJRpY573UrOMf1KspOFEH7EQ2Oy/uhEdwOXFP1PF++xLj2P9WlJzr8BDys7A7PxC6DOiva9go/MDo/aI6YDx5Q+Rym3AMOiP1efifcxN8ZH3HLwP19vqaHA6eazRyT2Jw8D9mLzbSAAr+IXw03DLw58PNo+GL+gcXm0/eWExzTELxz7GW9baAPctAX1/x3/+f4IvIdPzbg22YEhhMXA8XhYXIx/aDo++gtFpUR97vOLvvDQtTRqOwD/sPM93lbxAXB3CGFU9NhF+AWA/8Qvjj2I4r7eijz318A9+Cw3C/D3aVIlyj8nes4lUZ1PJZz7W/wvCw/i780J+DSWpWc92RJd8ffol6j2h0MIY1PwepJ5Dn9tS/CWrrIuar0Bb/fIiX4f38P/GlRmvVWsS6TWMV1bICJSNWbWB/8gs2NtumDLzH4PnB1CKO9iTslgZvYkkBtC+EvctYhkAo1Yi4hUQdQCcCXwf+keqs2snZkdYmZZ5lP4/QlvtRARkRRQsBYR2ULm80Mvw2diuT/WYiqmAfBvfEaN9/G2lIdjrUhEJIOoFUREREREJAU0Yi0iIiIikgIK1iIiIiIiKZBsQYZaqVWrVqFz585xlyEiIiIiGWzq1Kk/hxBaJ9uXMcG6c+fOTJkyJe4yRERERCSDmdlPZe1TK4iIiIiISAooWIuIiIiIpICCtYiIiIhICihYi4iIiIikgIK1iIiIiEgKZMysIJuzYsUKFi5cyPr16+MuRbZQ/fr1adOmDc2aNYu7FBEREZFN1IlgvWLFChYsWED79u1p3LgxZhZ3SVJJIQTWrFlDXl4egMK1iIiIpJ060QqycOFC2rdvT5MmTRSqaykzo0mTJrRv356FCxfGXY6IiIjIJupEsF6/fj2NGzeOuwxJgcaNG6udR0RERNJSnQjWgEaqM4TeRxEREUlXdSZYi4iIiIhUJwVrEREREZEUULDOcAsWLCArK4uvv/467lJEREREMpqCdYZ766236Ny5M7vvvnvcpYiIiIhkNAXrysrJgQEDoHt3v83Jibuicr355pscd9xxcZdRphAC+fn5cZchIiIitUQ6RzEF68oYNAj69YMRI+DTT2HkSL8/aFC1Pu3YsWMxM+bOnbtx28EHH0y9evVYtmzZxm177bUXN99888b769ev59133y0RrKdNm0a/fv1o0qQJLVq04Nxzz2XBggXlPv+TTz6JmfHll19yxBFHsPXWW7Prrrvy8ssvb3Lsq6++So8ePWjUqBHbb789119/fYnp8W655RZatWrFxIkTOeCAA2jUqBEvvPDClvxYREREpI6JKYpVmIJ1ReXkwJAhsHo1hODbCgv9/pAh1fpx6aCDDqJ+/fpMmDABgNWrVzN16lQaNGjApEmTAFiyZAlfffUVvXv33vi4CRMmUFBQQN++fQFYtGgRffv2ZfXq1Tz33HM8+OCDfPDBBxxxxBGsW7dus3Wcc845nHjiibzyyit07dqVs88+m9zc3I37R44cyamnnsqBBx7Ia6+9xqBBg3j00Ue56aabSpxn9erVDBw4kN/85jeMGjWKAw88sKo/IhEREclwMUaxCqsTS5onddVVMG1axY//+mt/55JZvRpOPBEq2se8775w//0VfuomTZrQvXt3JkyYwFlnnUVOTg7bbrst/fr1Y8KECRx33HFMnDgRM6Nnz54bH/fmm2/Sr18/GjVqBMA999wDwOjRozcuCd61a1eys7N56aWXGDBgQLl1XH311Vx88cUAdO/enbZt2/LGG29w6aWXEkLguuuu44ILLuDhhx/e+JiGDRty2WWXcdNNN9GyZUsA1qxZw7333stJJ51U4Z+BiIiI1G1Dh8KaNcn35ef7/uzsmq2pNI1YV1RZ72RF91dRnz59No5Yjx8/nl69enHooYeW2LbPPvtsDMywaX/1xx9/zJFHHlnimIMOOojOnTszceLEzdZw5JFHbvy+ZcuWtGnTZuOI9YwZM5g9ezZnnnkmBQUFG78OP/xw8vPzmT59+sbHmhnHHHPMFv4kREREpC6aMaN4pLq0wkKYObNm60km9hFrM7sS+C1gwGMhhPvNbDtgBNAZmAWcGUJYmtInrsSIMeDd8SNH+jtXWlYWnHACPPdcSkpLpnfv3gwZMoRly5ZtHKXu3bs3V111Ffn5+UyYMKFEG8j333/Pt99+y7HHHrtx27x589hjjz02OXfbtm1ZsmTJZmto3rx5ifsNGjTYeOHhzz//DFDi+RLNmTNn4/ctWrSgQYMGm30+ERERkSLdunmzQVlRrFu3Gi9p0zrifHIz2xMP1QcC+wDHm9nOwI3AmBBCV2BMdD9eV14JUUvFJho1giuuqNanP+SQQwAYN24cOTk59OnThz322INtttmGMWPG8Omnn5YI1m+++SZ77703HTt23LitXbt2LFy4cJNzL1iwgO22265K9RU9/tFHH+WTTz7Z5CtxhFrLkouIiEhlxRzFKiTuVpDdgI9CCKtDCAXAB8CpwEnAsOiYYcDJ8ZSXIDsbrr0WmjTxj0Xgt02a+PZqbupp0aIFe+65J/fddx/16tVjv/32w8zo1asXd911FwUFBZsE69LT7B100EGMHj2alStXbtz2ySefMGvWLHr16lWl+nbZZRfat2/PrFmz6NGjxyZfRf3VIiIiIluiKIolhusajGIVEnewng70NrOWZtYEOBboCLQNIcyLjpkPtI2rwBIGD4YxY+DMM33yxDPP9PuDB9fI0/fu3Zvx48fTs2dP6tWrV2Jb165dadvWf0yrVq3igw8+2CRYX3PNNQAcddRRvPrqqzz77LOceuqp7LXXXpx22mlVqi0rK4t77rmHu+++mz/+8Y+89dZbvPfeezz66KMce+yxrC7rwk8RERGRCho8GG67zb/fZZcaj2KbFWuPdQjhGzO7E3gHWAVMAzaUOiaYWdJWdTO7BLgEoFOnTtVbbJHs7Ng+EvXu3Zt//etf9OnTp8Q2oMSI83vvvcfWW29Ndqk6W7duzdixY/nTn/7EgAEDaNCgAcceeyz33XdfSnqezzrrLJo1a8Ztt93GE088Qb169ejSpQvHH3+8eqpFREQkJZo29dvRo2HHHeOtpTQLZV1eGQMzuw3IBa4E+oYQ5plZO2BcCGGX8h7bo0ePMGXKlKT7vvnmG3bbbbeU15uuLrnkElatWsWzzz4bdynVoq69nyIiIlJs8GC45RZYuxbiGLczs6khhB7J9qXDrCBtQggLzawT3l+dDewEDATuiG5fjbHEWufRRx+NuwQRERGRapGXB23axBOqNyf2YA28ZGYtgfXAZSGEZWZ2BzDSzH4N/AScGWuFIiIiIpIW8vKgffu4q0gu9mAdQuidZNtioF8M5YiIiIhIGsvNTb/e6iJxzwoiIiIiIlJh6TxirWAtIiIiIrVCfj4sXqxgLSIiIiJSJXPn+q2CtYiIiIhIFeTm+m2HDvHWURYFaxERERGpFfLy/FYj1iIiIiIiVaBgLbF48sknMTN++eUXABYuXMgtt9zCrFmzShw3btw4zIzp06fHUKWIiIhIxeXlwdZbQ7NmcVeSnIJ1hjruuOOYPHkyTZo0ATxYDx48eJNgLSIiIlJb5OV5f7VZ3JUkF/sCMVI9WrduTevWreMuY4utWbOGxo0bx12GiIiIpJHc3PRtAwGNWFdaTg4MGADdu/ttTk71P+fYsWMxM+YWzTEDHHzwwdSrV49ly5Zt3LbXXntx8803AyVbQWbNmsVee+0FwGGHHYaZYaU+6v3888+cccYZbLPNNnTp0oWHH354s3V17tyZa6+9lvvuu48OHTrQokULzj777BI1ASxZsoRLLrmEtm3b0qhRI3r27MlHH31U4hgz49577+Wqq66idevWG+sVERERKZLOi8OAgnWlDBoE/frBiBHw6acwcqTfHzSoep/3oIMOon79+kyYMAGA1atXM3XqVBo0aMCkSZMAD69fffUVvXtvskI87dq149lnnwXgX//6F5MnT2by5Mkljvntb3/LPvvswyuvvELfvn257LLL+Pjjjzdb28iRIxkzZgyPPvood955J2+88QZ//vOfN+5fu3Yt/fv357333uPuu+/mv//9L61bt6Z///7Mnz+/xLnuvvtu5s2bx9NPP80DDzxQuR+SiIiIZLTCQp/HOp2DtVpBKignB4YMgdWri7cVFvr9IUPgmGMgO7t6nrtJkyZ0796dCRMmcNZZZ5GTk8O2225Lv379mDBhAscddxwTJ07EzOjZs+cmj2/YsCF77703ALvvvjvZSQodMGAAf/nLXwDo27cvr7/+Oi+//DIHHnhgubXVr1+f//73v2y1lf9T+vrrrxk+fPjGEe9nnnmG6dOn89VXX9G1a1cA+vfvzy677MI999zD3XffvfFc7dq1Y8SIEVvwExIREZFMt2gRFBQoWKelq66CadMqfvzXX5cM1YlWr4YTT4Tdd6/YufbdF+6/v+LPDdCnTx9GjRoFwPjx4+nVqxeHHnoozzzzzMZt++yzD8228DLZI488cuP39evXp2vXruQWzcJejsMOO2xjqAYP7gsXLmT9+vXUr1+f9957j+7du7PTTjtRUFCw8bhDDz2UKVOmlDjXscceu0W1i4iISOZL98VhQK0gFbZmTdX2V1Xv3r2ZPn06y5YtY8KECfTu3ZvevXszZcoU8vPzN27bUs2bNy9xv0GDBuTn52/R40IIrF27FvDe7ZycHOrXr1/i6z//+Q9z5swp8di2bdtucf0iIiKS2dJ9DmuowyPWlR0xHjDAe6oLCzfdl5UFJ5wAzz2XktKSOuSQQwCfdzonJ4c777yTPfbYg2222YYxY8bw6aefct1111VfAVtou+22o0ePHjzyyCOb7GvYsGGJ+6UvqBQREREpomCdQa68El57LXk7SKNGcMUV1fv8LVq0YM899+S+++6jXr167LfffpgZvXr14q677qKgoKDcEesGDRoAVGgUOpX69evHO++8Q6dOnWjTpk2NPreIiIhkjrw8qFcP0vkP3GoFqaDsbLj2WmjSxEeowW+bNPHt1XXhYqLevXszfvx4evbsSb169Ups69q1a7mtFJ06daJx48YMGzaMyZMnb9LfXF0uuOACdtppJ/r27csTTzzBuHHjeOmll7jhhhu47777aqQGERERqf1yc6FdOw/X6UrBuhIGD4YxY+DMM30e6zPP9PuDB9fM8xeNSPfp02eTbb169Sr3sY0aNeKxxx5j6tSpHHrooRxwwAHVV2ip5x07dixHHHEEgwYN4sgjj+TKK69k5syZm51xRERERKRIus9hDWAhhLhrSIkePXqEskZhv/nmG3bbbbcarkiqi95PERGRumf33WG33eCll+Ktw8ymhhB6JNunEWsRERERSXu1YcRawVpERERE0trKlbBiRXrPYQ0K1iIiIiKS5mrDVHugYC0iIiIiaU7BWkREREQkBRSs00ymzH5S1+l9FBERqXtyc/1WwToN1K9fnzVr1sRdhqTAmjVrqF+/ftxliIiISA3Ky4MWLXxhvnRWJ4J1mzZtyMvLY/Xq1RrxrKVCCKxevZq8vDwtjS4iIlLH1Iap9gC2iruAmtCsWTMA5s6dy/r162OuRrZU/fr1adu27cb3U0REROoGBes006xZMwUyERERkVooLw/22SfuKjavTrSCiIiIiEjttH49zJ9fO0asFaxFREREJG3Nnw8hKFiLiIiIiFRJbZnDGhSsRURERCSNFQXrDh3iraMiFKxFREREJG3VlsVhQMFaRERERNJYXh40bAgtW8ZdyeYpWIuIiIhI2srLgx12ALO4K9k8BWsRERERSVu1ZXEYULAWERERkTSWm1s7LlwEBWsRERERSVMhaMRaRERERKTKli6F/HwFaxERERGRKqlNi8OAgrWIiIiIpKmiOazVYy0iIiIiUgUasRYRERERSYGiYN2uXbx1VJSCtYiIiIikpbw8aNMGGjSIu5KKUbAWERERkbSUl1d7+qtBwVpERERE0lRubu3pr4Y0CNZmdrWZfWVm083seTNrZGY7mdlHZvadmY0ws1ryBwARERERSZWki8Pk5MCAAdC9u9/m5MRSWzKxBmszaw9cAfQIIewJ1APOBu4E7gsh7AwsBX4dX5UiIiIiUtPy82Hx4lLBetAg6NcPRoyATz+FkSP9/qBBsdWZKPYRa2AroLGZbQU0AeYBhwMvRvuHASfHU5qIiIiIxGHuXL/d2GOdkwNDhsDq1b7WOUBhod8fMiQtRq5jDdYhhDxgCDAbD9TLganAshBCQXRYLlCLumtEREREpKqKFofZOGI9dCisWZP84Px83x+zuFtBWgAnATsBOwBbA0dX4vGXmNkUM5uyaNGiaqpSRERERGraJovDzJhRPFJdWmEhzJxZI3WVJ+5WkP7AjyGERSGE9cDLwCFA86g1BKADkJfswSGER0MIPUIIPVq3bl0zFYuIiIhItdskWHfrBlllRNesLN8fs7iD9Wwg28yamJkB/YCvgbHA6dExA4FXY6pPRERERGKQlwfbbAPNmkUbrrwSGjVKfnCjRnDFFTVWW1ni7rH+CL9I8VPgy6ieR4EbgGvM7DugJfB4bEWKiIiISI0rmsPaLNqQnQ3XXpuwAR+pbtLEt2dnx1JnIgtl9arUMj169AhTpkyJuwwRERERSYGePaFxYxgzJmHj9Omw115w4IGwYQN07eoj2TUYqs1sagihR7J9WyXbKCIiIiISp7w8OPTQUhtHj/bbF1+Ejh1rvKbNibvHWkRERESkhMJCn8d6k1UXR4+G3XZLy1ANCtYiIiIikmYWLoSCgoTFYcDnsB4/Ho46Kra6NkfBWkRERETSyiZT7YGH6rVrFaxFRERERCoqabAePRoaNoQ+fWKpqSIUrEVEREQkrZQZrHv39un10pSCtYiIiIikldxcqFcP2rZN2PD112ndBgIK1iIiIiKSZvLyoF07D9cAvPOO3ypYi4iIiIhUXF5ekjaQHXaAPfeMraaKULAWERERkbRSIlhv2ADvvgtHHllyOfM0pGAtIiIiImklLy9hDuspU2DpUg/WaU7BWkRERETSxsqVsGJFwoj1O+/4SPURR8RaV0UoWIuIiIhI2thkqr3Ro6F7d2jVKraaKkrBWkRERETSRolgvXw55OSk/WwgRRSsRURERCRtFAXrDh2AMWP84sVa0F8NCtYiIiIikkZyc/22fXu8v7ppUzj44FhrqigFaxERERFJG3l50KIFNG4UvL/68MOhfv24y6oQBWsRERERSRsb57CeORNmzao1/dWgYC0iIiIiaWTjHNajR/uGWtJfDQrWIiIiIpJGcnMT+qt/9Sv/qiUUrEVEREQkLaxfDwsWQPvtN8DYsbWqDQQUrEVEREQkTcyfDyFA+zUzYdUqBWsRERERkS2xcQ7r2ZNhq62gb99Y66ksBWsRERERSQsb57D+chT07AnNmsVbUCUpWIuIiIhIWti4nPm3Y2pdGwgoWIuIiIhImsjLg4b1N9CSxbUyWG8VdwEiIiIiIuDBeoeGi7FtW8F++8VdTqVpxFpERERE0kJeXqDD2u/hiCMgq/bF1NpXsYiIiIhkpNwf1tF+/axa2QYCCtYiIiIikgZCgLx5WbQnr1YtY55IwVpEREREYrd0KeQX1Kd9uwDt2sVdzhZRsBYRERGR2OXNXA1Ahx7bx1zJllOwFhEREZHY5Y6aDkD7frvGXMmWU7AWERERkdjljf8egPZH7xVzJVtOwVpEREREYpf3+SIA2u3UKOZKtpyCtYiIiIjEa9Ys8hY3om3TVTRoEHcxW07BWkRERETi9c475NKB9h1rdzSt3dWLiIiISO03ejR59TvT/le1tw0EFKxFREREJE4FBTBmDHlZHWnf3uKupkq2irsAEREREanDPvqI/OX5LKYpHTrEXUzVaMRaREREROLzzjvkWUcA2rePuZYqUrAWERERkfiMHk3ebv0BBWsRERERkS2zZAl88gl5u/YDFKxFRERERLbMe+9BYSF5O/QAUI+1iIiIiMgWGT0att2WXOvENttAs2ZxF1Q1CtYiIiIiUvNCgHfegf79yZuXVevbQEDBWkRERETi8M03kJsLRx1FXl7t76+GmIO1me1iZtMSvlaY2VVmtp2ZvWtmM6PbFnHWKSIiIiIpNnq03x55JHl5tb+/GmIO1iGEb0MI+4YQ9gW6A6uBV4AbgTEhhK7AmOi+iIiIiGSK0aNhl10o7Lgjc+dqxDrV+gHfhxB+Ak4ChkXbhwEnx1WUiIiIiKRYfj588AEcdRQLF/qq5grWqXU28Hz0fdsQwrzo+/lA22QPMLNLzGyKmU1ZtGhRTdQoIiIiIlU1YYKH66i/GhSsU8bMGgAnAi+U3hdCCEBI9rgQwqMhhB4hhB6tW7eu5ipFREREJCVGj4YGDeDQQxWsq8ExwKchhAXR/QVm1g4gul0YW2UiIiIiklqjR0OvXrD11huDtS5eTJ0BFLeBALwGDIy+Hwi8WuMViYiIiEjqzZ0L06fDUUcBPuNevXrQpk3MdaVA7MHazLYGjgBeTth8B3CEmc0E+kf3RURERKS2e+cdv42CdV4etGvn4bq22yruAkIIq4CWpbYtxmcJEREREZFMMno0bL897L03QMYsDgNpMGItIiIiInXEhg3w7rtw5JFgBpAxi8OAgrWIiIiI1JTPPoPFiz1YR3JzNWItIiIiIlI5RcuYH3EEACtX+peCtYiIiIhIZYweDfvvv3EKkEyawxoUrEVERESkJqxYAZMnb5wNBMioOaxBwVpEREREasL770NBwSb91aARaxERERGRinvnHdhmG+jZc+MmtYKIiIiIiFRUTg4MGACPPw7NmsGnn27clZcHLVpA48Yx1pdCCtYiIiIiUj0GDYJ+/WDECFi3DubN8/uDBgGZNYc1KFiLiIiISHXIyYEhQ2D1agjBt4Xg94cMgZycjFp1ERSsRURERKQ6DB0Ka9Yk35efD0OHZtTiMKBgLSIiIiLVYcaM4pHq0goLWT/jBxYsULAWERERESnfjjuWvS8ri/kdDiAE9ViLiIiIiJTtu+/g44/L3t+oEXknXApoxFpEREREJLmPP/a5qvPz4eKLoUkTyIoiZ1aW37/2WnKb7wlkVrDeKu4CRERERCRDvPkmnHkmtG0Lo0ZBt27w29/6hYwzZ0LXrnDllZCdTd5Qf4iCdcTM+gJ7RHe/CiGMq2I9IiIiIlIb/d//waWXwj77eMDefnvfnp3tX6Xk5UHDhtCyZQ3XWY22KFib2Q7AS8CBgEWbg5l9BJwWQpiXovpEREREJJ2FAH//O9xyCxx1FLzwAjRtutmHFc1hbbbZQ2uNLe2xfgToAAzER6y7A38HDgAeSk1pIiIiIpLWCgrgkks8VF9wAbz+eoVCNZBxc1jDZkaszaxdGaPPRwJnhRBeS9j2mZl1BM5KZYEiIiIikoZWrYKzzvK2j5tvhltvrdTwc14eHHBANdYXg82NWH9lZhcl2b4eSPZxpGm0T0REREQy1aJFcNhh8Pbb8Mgj8I9/VCpUh0DGLWcOm++xfhj4t5mdBfw2hDAn2v4a8KCZdQI+AxoCJwCnA09VV7EiIiIiErPvv4ejj/ZejpdfhpNOqvQpli712fgyaXEY2EywDiH8xcxeBP4DTDezG0MIjwCXA08C/wQCxRcwvgJcWX3lioiIiEhsPvkEjjsONmyAMWN8vuotkJvrt3VtxJoQwjQz6wHcBNxnZmcCvw4hnGxmOwO7RYd+HUL4vhprFREREZG4vPUWnHEGtGnjc1TvsssWnyovz28zLVhXaFaQEMKGEMI/gP2BxsAXZnY18H0I4fXoS6FaREREJBM98QSceKKH6cmTqxSqoY4H6yIhhK+BnsAtwD+ASWZWtZ+siIiIiKSnEHy2j1//Gg4/HD74oHjhlyrIy/NrHdu1S0GNaaRCwdrMepjZaWbWI4RQGEIYAuwLFADTzOwmM9vSObFFREREJN0UFMDvfgd/+xucfz688UaF56jenNxc7yhp0CAlp0sb5YZhM2ttZh8CHwEvAB+Z2WQzaxNCmBlC6ANch/dff2xme1d/ySIiIiJSrVatglNOgcceg5tugmHDUpqCM3GqPdj8iPW9+GqKg4Fj8RaQ7tF2AEIIDwF7A0uBT8xscLVUKiIiIiLVb9Ei6NfPF37517/gtttSvu54pgbrzc0KcgTwdAjh79H9UWbWBTgm8aAQwizgCDP7DXAXMCjVhYqIiIhINfvhB5+jes4ceOklH7WuBnl5cMgh1XLqWG0uWBuwutS2VRTPW11CCOH/zOytVBQmIiIiIjVo6lQ49lhYvx7ee6/akm9+PixeXDdHrMcAF5rZZOATvA1kIPBGWQ8IIcxNXXkiIiIiUu1GjYLTT4dWrWDcONhtt80+ZEtl6lR7sPlgfTXQFXia4hUWP422i4iIiEht9+ST8JvfwF57+SIw1TwHXp0N1iGEBWZ2IH4B447AbOCTEEJhTRQnIiIiItUkBL8w8S9/gf79vae6WbNqf9qiYN2hQ7U/VY2ryJLmAfg4+hIRERGR2q6gAC6/HP79bzjvPHj88RqbVDqTR6y1qIuIiIhIXbJ6NZx2mofqG25I+RzVm5ObC9tsUyOD4zVusyPWIiIiIpIhfv4ZTjgBPvoIHnzQR61rWKbOYQ0K1iIiIiJ1w48/+hzVP/0EL74Ip54aSxl5eZnZXw1qBRERERHJfJ9+Cgcf7KsqvvdebKEaMnvEWsFaREREJJONHg2HHgoNG8KkSdCrV2ylFBbC3LkK1iIiIiJS2zz1FBx/PHTpApMnV+vCLxWxcKFPSKJgLSIiIiK1Q9Ec1QMH+mj1+PGwww5xV5XRU+1BCoO1mXUys/jfMREREZG6bMMGuOwyuPlmOPdcX01x223jrgrI7MVhILUj1rOAOWb2gZkdlcLzioiIiEhFrFkDp58OjzwC11/vrSA1OEf15uTm+m2mjlincrq92YABhwBvmdlnIYQeKTy/iIiIiJRl8WKfozonB4YOhSuuiLuiTeTlQb160KZN3JVUj5QF6xBCZwAzaw70ib5EREREpLrNmuVzVM+aBS+84CsrpqG8PGjXzsN1Jkr5AjEhhGXAa9HXZkVB/P+APYEAXAx8C4wAOuMtJmeGEJamulYRERGRWu+zz+DYYyE/H959F3r3jruiMmXy4jBQiR5rM6tfTTUMBUaFEHYF9gG+AW4ExoQQugJjovsiIiIikujdd6FPH6hf3+eoTuNQDZm9OAxU7uLFPDO708x2TtWTm9m2eMvI4wAhhHXRiPdJwLDosGHAyal6ThEREZGM8PTTPlK9004+R/Xuu8dd0Wbl5ipYJx57HfCtmb1rZqeZWVU7ZHYCFgH/MbPPzOz/zGxroG0IYV50zHygbRWfR0RERCQzhAB33AEXXOAj1BMm1Iq0unKlf9WCUrdYZYL1DsB5wASgHzASyDWzf5pZ5y18/q2A/YFHQgj7Aaso1fYRQgh47/UmzOwSM5tiZlMWLVq0hSWIiIiI1BIbNsAf/wg33QQDBsDbb6fNHNWbk+lzWEMlgnXUpvFcCKEvsCtwPx6MbwK+M7O3zOwkM6tMWM8FckMIH0X3X8SD9gIzawcQ3S4so6ZHQwg9Qgg9WrduXYmnFREREall1qyBM86Af/0LrrsOnnkGGjaMu6oKy/RVF2ELF4gJIcwIIfwJaE/xKPbRwMvAbDO7pSKrMIYQ5uOLyuwSbeoHfI3PKDIw2jYQeHVL6hQRERHJCEuWQP/+8N//wv33w113QVYq1/mrfpm+OAxUcbq9EMI6M3sTaAV0xdtFdgD+BtxkZo8AN4QQ1pZzmj8Cz5pZA+AH4CI88I80s18DPwFnVqVOERERkVrrp598juoffoARI3zUuhaqCyPWWxyszSwb+B0eehsBK4AHgCfwdo5r8NDcEPh9WecJIUwDkq3Q2G9LaxMRERHJCNOm+cwfa9bAO+/AoYfGXdEWy8uD7baDxo3jrqT6VCpYm1lT4Hw8UO+JL2H+GfAw8FwIYU106Bdm9jQwCjidcoK1iIiIiCQxZgyccopfnDhxIuyxR9wVVUmmz2ENlQjWZvY4PjrdBFgLPA08HEL4ONnxIYQNZjYOODwFdYqIiIjUHc8+CxddBLvs4jN/ZMBUGpk+hzVU7uLFi/A5pa8HOoQQLiwrVCcYB/x9C2sTERERqVtC8AsTzzsPDjnE56jOgFANGrEu7egQwjuVOXkIYRIwqXIliYiIiNRBGzbA1VfDgw/CWWfBsGG1ajq98qxfDwsWZMxnhDJVZh7rSoVqEREREamg/HwP0w8+CH/6Ezz3XMaEaoD5830wPtNHrCscrM2sn5k9Udb81Ga2Q7S/b6qKExEREcl4S5bAkUfCSy/BvffCkCG1bo7qzakLc1hD5VpB/gjsGkKYm2xnCGGumR0MbIv3VouIiIhIeWbP9jmqv/8ehg/3UesMVBfmsIbKXby4P/DhZo6ZSPI5qUVEREQk0eefw8EHw9y5MHp0xobqnBy47Tb/fvBgv5+pKhOs2wBJR6sTLIiOExEREZGyvP8+9O4NZj5Hdd++cVdULQYNgn794LPP/P6rr/r9QYPirau6VCZYLwc6buaYjsCqLS9HREREJMM9/7y3f+y4ow/f7rln3BVVi5wcbxdfvbp4W2Gh3x8yJDNHrisTrD8GTjaz7ZPtjC5qPDk6TkREREQSheCJ8pxzoGfPjJqjOpmhQ30l9mTy831/pqlMsH4QaApMMLMTzawhgJk1NLOTgPHANsADqS9TREREpBbbsAGuugquuw7OPNN7qps3j7uqajVjhn+WSKawEGbOrNl6akKFZwUJIbxjZrcCfwVeAYKZLQVaABZ93RpCGFUtlYqIiIjURvn5cP758OKLvgBMBk6nl8x225W9LysLunWruVpqSmWm2yOEMMjMJuFT7x0ENAeWADnAgyGEd1NeoYiIiEhttXQpnHwyjB8P99wD11wTd0U14rHHYOxYvzYz2ah1o0ZwxRU1X1d1q1Swho0rMGoVRhEREZHyzJnjFyl+951fsHj22XFXVO3Wr/eOl4cfhqOOgr33hn/9ywftCwt9pLpRI7j2WsjOjrva1Kt0sBYRERGRzfjiCzjmGPjlFxg1Cg47LO6Kqt3ChXDGGT44f911cPvtUK8enHqqX6g4cyZ07QpXXpmZoRoUrEVERERSa+xYb/9o2tTnqN5rr7grqnaffeYveeFCePZZn/ikSHZ25gbp0irVOW9m7czsX2b2nZmtMbMNSb4KqqtYERERkbQ2fLi3f3TsCJMn14lQPWIEHHKIt3pMnFgyVNc1FQ7WZtYemAL8Dl8EpiEwG5gJbMBnBfkcmJD6MkVERETS3L33woABPjw7YYKH6wy2YQPcdJO3jnfvDlOm+G1dVpkR678B2wNHhxD2ibb9J4SwK9AFGA00Bk5NbYkiIiIiaayw0KfR+9Of4PTTfY7qFi3irqpaLVsGJ5wAd9wBv/sdjBkDbdvGXVX8KhOsjwJGhRDeK70jhJALnIEH68Epqk1EREQkveXn+yj1/ff7VXkjRvi0Fxnsf/+Dgw6Cd9+FRx6B//f/oEGDuKtKD5UJ1tsDXyXc34AHaQBCCL8A7wInpaY0ERERkTS2bJn3U48c6Yu+3Hdfxi/88uabHqqXLoX334dLL427ovRSmXd/BZD4eWQp0L7UMcuB1lUtSkRERCStzZkDvXrBhx/Cc895G4hZ3FVVmxDgttu8/WPnnb2funfvuKtKP5WZbu8nILEL/3PgcDNrEkJYbWZZwJFAbioLFBEREUkrX37pc1SvXOlzVB9+eNwVVatVq+Dii31g/pxzfFXFJk3irio9VWbEegxwmJnVj+4PA3YAPjSzu4FJwB7AiNSWKCIiIpImxo3zodoQfOaPDA/Vs2b5VHovvAB33QXPPKNQXZ7KjFg/jrd/tALmhRCeMbPuwB+BvaNjhgP/TG2JIiIiImlg5Eg4/3z41a98pLpTp7grqlbjxvlKiuvXw1tveTu5lK/CI9YhhJkhhDtDCPMStl0NtAMOBtqFEM4JIeRXQ50iIiIi8bnvPjjrLL9yb+LEjA7VIcBDD0H//tC6NXzyiUJ1RVVmgZgLzOyo0ttDCItCCB+FEBaktjQRERGRmBUW+oWJ11wDp50G77wD220Xd1XVZu1a+O1v4Y9/hGOPhZwc6No17qpqj8r0WD8B6POKiIiI1A1r1/rVevfe60kzw+eonjcPDjsMHn8c/vIX+O9/oVmzuKuqXSrTYz2fygVxERERkdpp2TI45RRvNL7rLrj22oyeTu/jj/3lLlvmFyqefnrcFdVOlQnWo/BZQbJCCIXVVZCIiIhIrHJzfTq9b7+FZ5/1UesM9tRTcMkl0K4dTJ4Me++9+cdIcpUZgb4ZaAo8bmatqqkeERERkfh89RUcfDD89BO8/XZGh+qCArj6ahg4EHr29IsUFaqrpjIj1s/jKyteAJxtZrPw9pBQ6rgQQuiXmvJEREREasj48XDSSdC4sX+/775xV1RtFi/2SU7GjIErrvAV2evX3/zjpHyVCdZ9E75vCOwSfZVWOmiLiIiIpLcXXoDzzoMuXXyO6h13jLuiajN9un9+yM2FJ56Aiy6Ku6LMUZl5rLMq+FWvOgsWERERSamhQ3349oADYNKkjA7VL78M2dmwZg188IFCdapplg8RERGpmwoL4brr4KqrfEqMd9/N2DmqCwth0CCfinvPPWHKFA/YklqVaQURERERqb1ycnx0esYM2HlnWLrUw/Rll/n2epn5R/eVK30l9ldfhQsvhEceyejpuGNV4WBtZn0qemwIYfyWlSMiIiJSDQYN8iv01qzxNbs//dS39+sHDz6YsXNUf/cdnHwy/O9//tnhj3/M2JeaFiozYj2Oil+YmJkf+URERKT2ycnxUL169ab7Jk+Gjz7KyL6Id97x1vGsLP/+8MPjrijzVSZY/53kwbo5cADQE3gd+LTqZYmIiIikyNChPlKdTH6+78+gYB2Cr8J+/fWwxx6+NHmXLnFXVTdUOFiHEG4pb7+ZXQg8iC8kIyIiIhK/L7/04dpQxh/dCwth5syarakarVnjqyg+84xfqPjkk7DNNnFXVXekbFaQEMKTQA5wW6rOKSIiIlJpIcDYsXDssb6U4PLlZTcWZ2VBt241W181yc2F3r09VN96q0/NrVBds1I93d40oMIXOYqIiIikTEEBjBjh81EffjhMnQr/+Ae8+aavpphMo0a+9GAtN2kS9OjhE568+ir85S+6SDEOqQ7WHdEUfiIiIlKTVq3ymT26doWzz/b55R59FH76CW6+GY46Cq69Fpo08RFq8NsmTXx7Le+vfuwxOOwwaNrUr9M88cS4K6q7UhKCzawecBFwOjAxFecUERERKdfChR6oH34YliyBnj3hvvs8WWaVGjscPBiOOcYvVJw500P4lVfW6lC9bp2vbfPII/7Z4fnnoUWLuKuq2yozj/UP5ZyjbXS7DvhzCuoSERERSW7GDLjnHhg2zNPlSSf5Coo9e5b/uOzsWh2kEy1cCGecAePH+0u//faMXd+mVqnMiHUWyafbWw98CXwMPBhC+CYVhYmIiIiU8OGHcPfd3kTcoAEMHAjXXAO77BJ3ZTXqs8980ZeFC+HZZ+Gcc+KuSIpUZrq9ztVRgJnNAlYCG4CCEEIPM9sOGAF0BmYBZ4YQllbH84uIiEgaKyyE11/3QD1pkvc63HwzXH45tG0bd3U1bvhwuPhiaNkSJk6E7t3jrkgSpfrixS11WAhh3xBCj+j+jcCYEEJXYEx0X0REROqK/Hy/Km/33X14Ni/P+6Nnz/a55OpYqN6wAW68EQYM8DA9ZYpCdTqqTI91Y6A1MD+EsC7J/oZ4r/XCEEJ+Fes6CegbfT8MX079hiqeU0RERNLdkiV+Nd6DD8KCBbD//n5V3umnw1Z1c+KxZcu83ePtt+HSS/3zRYMGcVclyVRmxPpvwLdAWVONbw38j8pfvBiAd8xsqpldEm1rG0KYF30/Hw/sIiIikqlmzfJZOjp18kmY99sPxozxodmzz66zofp//4ODDoJ33/XPG488olCdzirzr/QY4L0QwpJkO0MIS8zsPeB4PIRXVK8QQp6ZtQHeNbP/lTpvMLOk65BGQfwSgE6dOlXiKUVERCQtfPaZ90+PHOkrmpxzjs8tvddecVcWuzfegHPPhYYN4f33fVVFSW+VGbHuDMzYzDEzouMqLISQF90uBF4BDgQWmFk7gOh2YRmPfTSE0COE0KN169aVeVoRERGJSwgwejT07++tHm+8AVdfDT/+6FPo1fFQHQLcdptPx73zzj5or1BdO1QmWNcHCjdzTAAaVfSEZra1mTUt+h44EpgOvAYMjA4bCLxaiTpFREQkHa1fD08/DfvuC0cfDd98A3feCXPm+Kh1hw5xVxi7VavgrLN84pMBA2DCBO+OkdqhMq0gPwCHbuaYvsBPlThnW+AV88XstwKeCyGMMrNPgJFm9uvofGdW4pwiIiKSTlas8Bk+7r8fcnNhjz3gP//xtg81DG80a5ZPgPLFF3DXXd4R4xFJaovKBOvXgBvN7PoQwl2ld5rZjcD+wCb7yhJC+AHYJ8n2xUC/StQmIiIi6WbuXJ/C4t//huXLoW9f//6YY5QYSxk3zic+KSiAt97yAX2pfSoTrIcA5wK3m9mZwDtAHtAeOArYF5hNJYK1iIiIZKCvv4YhQ+CZZ3wC5tNP93W3e/TY/GPrmBDgoYe8xbxbN19UsmvXuKuSLVWZlReXmllf4DkgGx+dDkDRR84PgfO0QqKIiEgdFAKMH++90m++CY0bwyWX+JLjXbrEXV1aWrsW/vAHeOIJOOEE/xzSrFncVUlVVGpSyBDCLKCnme2Ph+vmwDIgJ4TwaaqLExERkTS3YQO8/LIH6k8+gdatYfBgT4ytWsVdXdqaNw9OOw0mT/ZpuwcPhqx0WQ9bttgWzbYehWgFaRERkbpq9Wq/APHee+GHH3xeuEcegYEDfbRayvTxx3DKKb6i4gsveKeMZIYKfzYys8Zm1snMkl6+a2YNo/0Vnm5PREREaplFi2DQIJ8D7vLLoU0beOklXyLw0ksVqjdj2DDo08cnQ5k8WaE606TDkuYiIiKS7r77zts7OnWCv/8dDjnEJ1n+8EM49VSoVy/uCtNaQYFfoHjhhdCzp3fN7L133FVJqlUmWG92SXOgaElzERERyQQffeTDqt26weOP+xrbX3/t01f06qVp8ypg8WKfPu/+++GKK3zRSbWfZ6bK9Fh3BsZs5pgZQK8trkZERETiV1joM3vcfbePSjdvDjfeCH/8I7RrF3d1tcqXX8JJJ0Fens/+cdFFcVck1akywTrlS5qLiIhIGlm71ud8u+ceX268Uye47z749a+hadO4q6t1Xn4ZLrjAp9D74APIzo67IqlulWkFqY4lzUVERCRuS5fC7bdD587wm99Aw4bw7LPeV33VVQrVlVRYCH/7m0+nt+eeMGWKQnVdUZlg/RrQ3cyuT7YzYUnz/6agLhEREalus2f7Ai6dOsGf/wx77QXvvAOffgrnnAP168ddYa2zYoVPpXfrrX6h4rhxsMMOcVclNUVLmouIiNQ1n3/u/dPDh/v9s8+Ga6+FffeNtaza7rvvvJ/6229h6FBvSde1nXWLljQXERGpC0KA997zQP3uu7DNNj5FxVVX+Yi1VMno0f75JCvLB/0PPzzuiiQOWtJcREQkk61fDyNHwpAhMG0abL+991P/7nfQokXc1dV6Ifi1njfcAHvs4bMQ7rRT3FVJXLSkuYiISCZauRL+7/988uTZs2HXXYvnoW7YMO7qMsKaNfDb3/p1nqedBk8+6X8IkLpri4K1iIiIpKl58+DBB+GRR2DZMujdGx56CI47zvsUJCXmzPGLFKdO9QsVb75Z/dSyBcHazNoB/fCLFpN95A0hhFurWpiIiIhUwv/+5+0eTz/t7R+nngrXXQcHHRR3ZRln4kQfoV6zxls/Tjwx7ookXVQqWJvZYODGUo8z/CLGxO8VrEVERKpbCJ7y7r4bXn8dGjXyxVyuuQZ23jnu6jLSo4/C5ZfDjjv6VHq77RZ3RZJOKvw3ITM7F/grMAE4HQ/Rw4BzgMfwVRmHA7oOVkREpDpt2AAvvQQHHwx9+sCHH8KgQd5L/fDDCtXVYN06+MMf/JrPww+Hjz9WqJZNVWbE+vdALnB0CKHAvJFoVghhODDczF4B3gSeT32ZIiIiwpo1foXcvff6pMldunj/9EUXQZMmcVeXsRYuhDPOgPHjvbvm9tuhXr24q5J0VJlgvRfwfAihIGHbxn9WIYTRZjYauA54PUX1iYiIyM8/+0j0gw/69wcc4FPonXqqEl41+/RTOPlkWLTIZ/8455y4K5J0VplgXR9YnHB/DbBtqWOmA5dWtSgREREBfvjBR6efeMJHq487zodM+/TRFBTVICfHV0ycMQO6dfNWjzvugJYtvZW9e/e4K5R0V5lgPQ9ol3B/NrB3qWN2AAoQERGRLffJJ35B4ksv+Yj0eefBn/7kK5BItRg0yCdVWbPGrwn97DO/7djR3462beOuUGqDykxo+RmwZ8L994HeZna+mW1tZsfhFzV+lsoCRURE6oTCQnjzTejbFw480NfIvu46mDXLR6wVqqtNTo6H6tWrPUxD8e3ixfDjj/HVJrVLZYL1G8CeZla0UOcdwHLgSWAF8Bo+U8hfUlmgiIhIRlu7Fv7zH9hrLzj+ePj+e095c+Z4H8IOO8RdYcYbOtRHqpPJz/f9IhVR4VaQEMKTeIguuj/HzA4A/gT8CpgFPBxC+DK1JYqIiGSg5cvh3//21DZ3rgfrp56Cs8+G+vXjrq5OyM+H116Dt94qHqEurbAQZs6s2bqk9qrSkuYhhB+By1NUi4iISObLzYX77/eVRlauhH79vNXjyCN1QWINCAEmT4Zhw2DECP9807ix/+iTheusLL+QUaQiqhSsRUREpIK++MJbPJ5/3hPcmWfCtdfC/vvHXVmdMGuWr/b+1FM+BXjjxr4s+QUX+BTgRx7pPdalNWoEV1xR4+VKLaVgLSIiUl1CgPff9xk+Ro+GrbeGyy6Dq66Czp3jri7jrVwJL77oo9MffODb+vaFP/8ZTj8dmjYtPvbaa/1zT36+t39kZXmovvZayM6OpXyphRSsRUREUq2gwBPd3Xf7CiNt2sA//gG//z1st13c1WW0DRv8s8ywYfDyy35R4s47w623+qyFZX2eGTwYjjnGW95nzoSuXeHKKxWqpXIUrEVERFLll1+8X/q++7z3oFs376U+/3wf/pRq8/XX3ubxzDOQlwfNm3ubx8CBHo4r0r6ena0gLVWjYC0iIlJVCxb4cuMPPwxLl8Ihh/gFiiec4D0FUi1+/tlb1p96CqZM8bV0jj7aP9eccII+y0jNU7AWERHZUt9+C/fc48lu3To4+WRf1OXgg+OuLGOtW+fr6Awb5rcFBbDvvr7y+znnaIVEiZeCtYiISGVNmuT906+9Bg0aeL/Bn/6kedmqSQg+Ij1smI9QL1niAfrKK73dY++9465QxClYi4iIVERhIbz6qgfqyZP9IsSbb4bLL9cwaTXJzfWe6aeegm++gYYN/Y8CAwfCEUfAVkoxkmb0T1JERKQ8+fme7O65B2bM8GklHngALr7Yp8+TlFq1Cl55xUenx4zx0epDDvFrQM84wy9KFElXCtYiIiLJLFniFyM++CAsXAjdu8Pw4b6qiIZKU6qw0OeZfuopn6Xwl1/888tf/+qtHr/6VdwVilSM/ssgIiKS6McffVqJxx/3pfiOOcYvSOzbV0uOp9iMGR6mn34aZs/2BVvOOsvDdK9emlBFah8FaxEREYCpU71/+oUXPNGdc44vu7fXXnFXllGWLoURIzxQT57sP+ojjoDbb/f+6SZN4q5QZMspWIuISN0Vgi81ftddMHasD5lec41PN9GhQ9zVZYz16/3HPGyYT6Sybh3ssYf/2M89F3bYIe4KRVJDwVpEROqedeu8X3rIEPjyS092d90Fl1wC224bd3UZIQT4/HMP0889523qrVrBpZf6rB777afOGsk8CtYiIpJ5cnJg6FBv4u3WzUegs7NhxQqfXuL++33d6z32gCefhAEDfD5qqbJ58zxIDxvmn1kaNPBVEC+4wNvV69ePu0KR6qNgLSIimWXQIB+JXrPGh02nTfP5p/fdF776ysN1374esI85RsOmKbBmjf+In3rKWz4KC+Ggg3xSlbPO8im/ReoCBWsREckcOTkeqlevLt5WWOjJb/Jk6NcP7rgDevSIr8YMEYIvQPnUUzByJCxfDh07wo03wvnnw667xl2hSM1TsBYRkdpt1Sqfq+2nn3zi48RQnSgrC1q3Vqiuoh9/9DD91FPwww++Rs5pp3nfdN++miJP6jYFaxERSV8hwOLFxcE52dfixRU7V2EhzJxZvfVmqBUrfBbCYcNgwgTvnjnsMO+6OfVU2GabuCsUSQ8K1iIiEp8NG2Du3LKD8+zZPiKdqEkT2HFH/zrgAL/t1Mlv774b3njDQ3RpWVl+IaNUyIYN8N57HqZfecVXdu/WDf75TzjvPP+Ri0hJaRGszaweMAXICyEcb2Y7AcOBlsBU4PwQwro4axQRkS2Qnw9z5pQ92pybCwUFJR/TsqWH5F13haOOKg7NRV8tW5Z9weFWW3kaTNYO0qgRXHFF6l9jhpk+3ds8nnnGZ/ho0QIuvthn9TjwQF3rKVKetAjWwJXAN0Cz6P6dwH0hhOFm9v+AXwOPxFWciIiUYdmy4pHlZMF5wYKSx2dl+ZzRO+4IPXuWHG0u+r4qfQXZ2b5a4pAhHuoLC/05GzXy7dnZVXq5mWrRInj+eR+d/vRT/3xy7LHeN33ccdCwYdwVitQOFkKItwCzDsAw4J/ANcAJwCJg+xBCgZkdDNwSQjiqvPP06NEjTJkypdrrFRGpMwoLPRiX19+8YkXJxzRsuOkIc+L9Dh1qZiLjonmsZ86Erl2L57GWjdau9a6Zp56Ct97yPxx07+4j0wMG+HWeIrIpM5saQkh6FXQ6jFjfD1wPNI3utwSWhRCK/jaYC7SPoS4Rkcy2bp23YpQVnOfM8fSVaNttPSB37gyHHrppcG7TJj2mhcjOVpBOIgT4+GMfmR4+HJYuhXbt4OqrPVDvuWfcFYrUbrEGazM7HlgYQphqZn234PGXAJcAdNJVFCIiJf3yS/ltGnPnetJKtP32HpD33x9OOWXT4Kzlvmul2bO9Z/qpp+Dbb6FxY397L7gA+veHevXirlAkM8Q9Yn0IcKKZHQs0wnushwLNzWyraNS6A5CX7MEhhEeBR8FbQWqmZBGRNBAC/Pxz+cF5yZKSj9lqK1/BY8cdPU2VbtXo2NF7kSUj/PILvPSSh+mxY/2fTJ8+cP31cPrp0KzZ5s8hIpUTa7AOIdwE3AQQjVhfG0I418xeAE7HZwYZCLwaV40iIrEoKPAR5bKC8+zZm858sfXWxWH5oIM2Dc7t2mloMsMVFnqIfuopD9WrVsGvfgW33OJT5HXpEneFIpkt7hHrstwADDezfwCfAY/HXI+ISGqtWVMcmJMF59xcn0g4UatWHpB33x2OOWbT4LzddpoLrY769lvvm37mGW+Nb9YMzjnHZ/Xo2VP/LERqStoE6xDCOGBc9P0PwIFx1iMissVCKJ6GrqzgvHBhycdkZUH79h6Qe/XadFaNTp18RFoksmSJX4A4bJhfkFivnk/7fffdcOKJ3kctIjUrbYK1iEitUVgI8+eXH5xXriz5mEaNikeW99ln09Hm9u1rZho6SWtFswTOmOGrHJaeJXD9enj7bQ/Tr7/u9/feG+65x0eot98+vtpFRMFaRGRT69YVrxaYLDTPmePHJGre3ANyly5w2GGbjji3aaO/x0u5Bg3ydW3WrPE/ekybBq+95uvanHii900/95xfs9qmDVx+ubd67LNP3JWLSJHYF4hJFS0QI1IHbG44r6JWrix/tHnevE2noWvXrmRYLh2cNcWCVEFODvTrl3wldjP/59igAZx0kofpI4/UHzhE4pLuC8SIiGxeecN5gwcXHxeCr89cXnBeurTkuevXL56G7sgjNw3NHTtqTWepVvfc4/+0kwkBevSAd96BFi1qti4RqRwFaxFJfzk5HqoTh/MKC/3+7bd7yM7PLw7SpRPKNtsUB+WDD950xHn77dNjtUDJWEWf977/Hn74wW8Tv+bP3/zjFapF0p+CtYikv6FDyx7OW78eRo3yK7j23BOOO27TEecWLdTfLNWuoMDb70uH5qIwnXg9qxl06OBzTB93HHzyCXz55aYdSOCf+bp1q7nXISJbTsFaRNLX11/7fGKvvJI8cRTZay9PJiLVbPXq5CPO338Ps2Z5uC7SsCHstJOH50MP9duir86dSy5yWV6PdaNGcMUV1f3KRCQVFKxFJL388AOMGOGB+osvfGivdWv/O7qG86SaFa0UnzjSnBie580reXzz5h6U998fzjijZHhu377iHUbZ2X65wJAh3tVUWOiPbdTIt2/JNboiUvMUrEUkfnl5MHKkh+mPP/ZtPXvCAw/A6ad777SG8yRFNmwov2VjxYqSxxe1bBx9dMng/Ktf+WKXqTJ4sC+oOXQozJwJXbtu+cQ3IhIPBWsRiceiRfDSSx6mx4/3ocL99oM774Qzz/S/lRdp107DeVIpa9aU37Kxfn3xsQ0aFLds9O5dMjjvtFPJlo3qlp2tf84itZnmsRaRmrN8Ofz3v/D88/Deez50uOuuMGAAnHUW7LJL+Y8vmsdaw3l1XgiwePGmo81F38+dW/L4bbfddLQ5sWWjXr14XoeI1D6ax1pE4rNqFbzxho9Mv/WWr1jYuTNcdx2cfbbP5lHRGTs0nFenbNgAubnJR52//37Tlo0ddvCgfOSRyVs2NDGMiFQ3BWsRSb21a2H0aA/Tr73m4bpdO/j97310+sADlXIE8JaNH38su2UjceX4+vWLWzYOOWTTlo3GjWN7GSIigIK1iKRKQQG8/76H6Zdf9raPli3hvPN8ZLp3b/29vQ4KAZYsKXthlLy8ksc3a+ZBee+94ZRTSobnDh30T0hE0puCtYhsucJCmDTJw/QLL/gFiU2beiI6+2zo39+HGSWjFRaW37KxfHnJ49u186Dcv/+mLRstW+qPGSJSeylYi0jlhABTp3qYHjHCE1XjxnDCCR6mjzmmZqdRkBqRn192y8aPP27astG5swflgw8uDs1duvhXkyaxvQwRkWqlYC0iFTN9uofp4cM9TdWv7xP73nmnh+qmTeOuUKqoqGUjWdtGXl7J9XmaNvWwvOeecNJJJUedO3ZUy4aI1E0K1iJStu++K14Fcfp0nzv68MPhz3/2do8WLeKuUCqhsNADclktG8uWlTx+++09KB9++KYtG61aqWVDRKQ0BWsRKSk3tzhMF80N36sXPPSQr4LYtm289Um51q4tv2Vj7driY7faqrhl46CDvE0jsW1j661jexkiIrWSgrWIwMKF8OKLHqYnTPBt3bvD3Xf7KoidOsVbn5SwdGnZC6Pk5pZs2dhmGw/Ku+/uHTulWza20v8FRERSRv9JFamrli2DV17xVRDHjPE+gd13h1tv9VUQu3aNu8Jaq2iByBkzoFu3yi8QWVjoKweW1bKxdGnJ49u29aDct++mLRutW6tlQ0SkpihYi9Qlv/wCr7/uI9OjRvlUDl26wI03+owee+6pFFZFgwbBkCG+8EkIMG2ar5Fz7bUweHDxcWvX+gIoZbVs5OcXH1uvnrdsdOnin3kSg3OXLj4qLSIi8bOQ+DfDWqxHjx5hSlE/qIgUy8/3ED18uIfq1at97eezzvJVEHv0UJhOkZwc6NfPf8Sl1a/vMxGuXOnhec6cki0bW2+96Whz0VenTmrZEBFJF2Y2NYTQI9k+/adaJBOtX+/tHcOHe7vHihU+jcPAgT4y3auXz/ARs6q2TNSEEHygf8kSb8Eouk38vuh2/PjkoRr8LRk9GvbfH/r02TQ8t2mjzzciIrWdgrVIbbG5FFpY6BceDh/uFyL+/LOvD33qqR6m+/VLq2HPirZMpEp+ftmBuLzQvHSpr9Zelvr1YbvtfObBX34pv4Y994QPP0zt6xIRkfSRPv+XFZGylZdCjzvOL0AcOdKveGvSBE480cP0UUel5SqIOTn+chJHdwsL/f6QId4ykWzkesMGv+ayvFHjsm7XrCm7HjNo3tzDcYsWHpQ7dSoOzOXdNmlSPNI8YIC/DYWFmz5HVpZ/HhIRkcylHmuRdFde466ZB+0GDTyNnn22z6mW5hMQDxjgU2Un+8+PmV+Qt//+m4bj5cvLP2+TJhULw6VvmzVLzUqB5b1VTZp4d066tbqIiEjlqMdapDYbOrTs4dYQfGWPUaN8yLWWmDEjeagG3/7TT961st12vvrf7rtXLCQ3aFCzr6O07Gz/I8KQId56UljoI9WNGvl2hWoRkcymYC2S7spLoeANwLUoVIO3RHz2WfKXlZUFZ5wBzz1X83WlwuDB/seDoUNh5kyfDjwdL8oUEZHUU7AWSXebS6G1sHF3993L/qzQqBFccUXN1pNq2dkK0iIidVH8822JSNk2bPAR6QxKocOGwS23+MWBjRsXz/qXleV9yGqZEBGR2krBWiRdrV4Np53mU+dlZ3vqrOUp9N//hgsv9Av8vvkG3n8fzjwTunf32zFjqmeqPRERkZqgVhCRdLRwoc/u8ckn8MAD8Mc/Fs9jXUsbd4cOhauu8tkBX3zRB9vVMiEiIplEwVok3Xz7rV/9Nn++r5p40km+vRan0DvvhBtv9LVqnn8+/tk7REREqoOCtUg6mTjRg3S9ejBuHBx4YNwVVUkI8Pe/e0/1gAHw1FNptfijiIhISqnHWiRdjBgB/ftDq1be9pEBofqmmzxUX3ghPP20QrWIiGQ2BWuRuIUAd93lqyYecAB8+KEvPViLhQBXX+0tIJdeCo8/npqVDUVERNKZgrVInAoK4LLL4IYbfFqMd9+Fli3jrqpKCgvh978vvljx4YeLJzMRERHJZPrfnUhcVq2CU06BRx6B667zq/oaNYq7qirZsAF+/WufVu+mm+Dee8Es7qpERERqhjoeReIwfz4cf7yvqPjwwz7EW8utXw8XXADDh/tc1H/9q0K1iIjULQrWIjXtm298Or1Fi+DVVz1g13Lr1nmL+CuveF/19dfHXZGIiEjNU7AWqUkffAAnnwwNG/r3PXrEXVGV5efD6afDm296X3UtW2FdREQkZdRjLVJTnnsOjjgCtt/ep9PLgFC9apUvEPnWW95XrVAtIiJ1mYK1SHULAW6/Hc49Fw4+2KfT69w57qqqbOVKOPZYeP99+M9/4JJL4q5IREQkXmoFEalOBQXwhz/AY4/BOefAE094G0gtt2yZt4l/8okPxJ91VtwViYiIxE/BWqS6rFzpifPtt+HPf4Zbb82ICZ0XL4Yjj4Qvv4QXXvAZA0VERETBWqR6zJ3rs3188YU3H2dIn8SCBd4mPmMG/Pe/3goiIiIiLtZgbWaNgPFAw6iWF0MIg8xsJ2A40BKYCpwfQlgXX6UilTB9uifOJUvg9de9ZyIDzJ0L/frBTz/BG29A//5xVyQiIpJe4v679Frg8BDCPsC+wNFmlg3cCdwXQtgZWAr8Or4SRSrh/ffhkEN8tZTx4zMmVM+eDX36QG4ujB6tUC0iIpJMrME6uF+iu/WjrwAcDrwYbR8GnFzz1YlU0tNPw9FHQ4cOPp3e/vvHXVFKfP+9h+qff4Z334XeveOuSEREJD3FPWKNmdUzs2nAQuBd4HtgWQihIDokF2hfxmMvMbMpZjZl0aJFNVKvyCZC8AsTL7gAevWCSZNgxx3jriolvv3WQ/XKlT4Yn50dd0UiIiLpK/ZgHULYEELYF+gAHAjsWonHPhpC6BFC6NG6devqKlGkbOvXw29+A3/7G5x/PowaBc2bx11VSkyfDoce6jMGjhuXMQPwIiIi1Sb2YF0khLAMGAscDDQ3s6ILKzsAeXHVJVKmFSt85o8nnoC//hWGDYMGDeKuKiU++wz69oV69Xzl9b32irsiERGR9BdrsDaz1mbWPPq+MXAE8A0esE+PDhsIvBpLgSJlyc31ZuMxY+Dxx+HvfwezuKtKiY8+gsMPh6239usvd63w35BERETqtrjnsW4HDDOzenjIHxlCeMPMvgaGm9k/gM+Ax+MsUqSEL77w6fSWL4c334Sjjoq7opSZONFfWuvW3lOdIa3iIiIiNSLWYB1C+ALYL8n2H/B+a5H08u67cNpp0LQpTJgA++4bd0Up8/77cMIJ0LGjD8S3T3rJsIiIiJQlbXqsRdLef/7jw7mdO/t0ehkUqkeNguOOgy5dvKdaoVpERKTyFKxFNicEuOUWuPhiv6JvwgQf1s0Qr74KJ54Iu+0GY8dC27ZxVyQiIlI7KViLlGfdOrjoIhg8GC68EN56C7bdNu6qUmbkSDj9dJ9Kb8wYaNUq7opERERqLwVrkbIsX+5Lkg8b5sH6iSegfv24q0qZp5+GAQN80Zd33oEWLeKuSEREpHaLe1YQkfQ0Z473U//vf/DkkzBwYNwVpdRjj8HvfgeHHQavveZT64mIiEjVKFiLlDZtml/J98sv8Pbb0L9/3BWl1EMPwR//6IPxL70EjRvHXZGIiEhmULAWSTRqFJxxhi9LPnFirV9yMCcHhg6FGTOgWzfvoX7oITjpJBgxAho2jLtCERGRzKFgLXVX6dTZuTPcfTfsuacv/FLL55wbNAiGDIE1a3xik88+89vdd4cXXsiodnEREZG0oGAtdVNZqbNLF59Or2nTuCuskpwcf3mrVxdvC8FvZ82CqVP9okURERFJHc0KInVPYuosSptFt/Pnw1dfxVdbigwd6p8ZksnP9/0iIiKSWgrWUvdkaOoMwT8TPPCAX3NZ9FmhtMJCmDmzZmsTERGpC9QKInVLCDBlSsakzp9+8oVdxoyB99/3AXfw6fPMkr/MrCxvKRcREZHUUrCWuqGwEF55BW67Db77ruzj0jx1/vyzB+iiMP399769bVvo16/4a948v03ssS7SqBFccUXN1i0iIlIXKFhLZlu/Hp59Fu64A779FnbeGW66yds9akHq/OUXGD++OEh//rlvb9YM+vb1Uvv185k+zIoft+OOcO213kqen++fK7Ky/OVde60uXBQREakOCtaSmdasgccf9+nzZs+GffaB4cPh9NOhXj2fay4NU+e6dfDRR8VBOicHCgp8vumePeEf//Ag3aMHbLWZ397Bg30RmKFDvbula1e48kqFahERkepioaxe01qmR48eYcqUKXGXIXFbvhweeQTuuw8WLvQ0evPNnjATh3SheB7rGFNnYaGPQhcF6QkTYNUqz/nduxe3dhxyiFZIFBERSQdmNjWE0CPZPo1YS2ZYtMhD8kMPebg+6ij485+hd+9NA3WR7OwaD9IheIt3UZAeOxYWL/Z9u+0GF13kQbpvX1/8UURERGoPBWup3ebM8ZaOxx7zto5TT/Ue6u7d465so3nzSl5wOHu2b+/QAY4/Hvr3h8MPhx12iLdOERERqRoFa6mdZsyAO++Ep5/2YeDzzoMbboBdd427MpYvh3HjioP011/79u22g8MOgxtv9FHprl3LHkwXERGR2kfBWmqXadPg9tvhhRf8ir7f/c4vONxxx9hKys+HSZOKg/SUKd473bgx9OkDF17oQXrffb13WkRERDKTgrXUDhMneqB+6y1o2tRHp6+6yidwrmEbNsDUqcVBeuJEWLvWJxs56CC/VrJfP2/fbtiwxssTERGRmChYS/oKAUaP9kVdJkyAVq3gn/+EP/yhRq/sCwG++aY4SI8b5+0eAHvv7eX06+ej002b1lhZIiIikmYUrCX9bNhQvEriZ5/5VX5Dh8JvfgNNmtRICbNnl1wqfN48396lC5x5pgfpww6DNm1qpBwRERGpBRSsJX2UXiWxa1df5OW886BBgy0+bdF01TNm+GrlyaarXrzYp74rCtMzZ/r2Nm18xo6i+aR32qkKr09EREQymoK1xG/1ag/QQ4YUr5I4YgScdpo3LlfBoEF+2jVrvKVj2jR47TVfCvzQQ4uD9LRpvr9pU99e1N6x556auUNEREQqRisvSnyWL4eHH/ZVEhct8uUFb74Zjj46JWk2J8fD8erVZR/ToAEcfLDPJV20VHj9+lV+ahEREclQWnlR0svChcWrJK5Y4UG6aJXEFFm2zOeLLitUm/nI9Jtv1ljbtoiIiGQ4BWupOaVXSTztNF8lcf/9q3TaEOD7730u6Q8/9Nuvv/bt5T1m5UqFahEREUkdBWupft9+W7xKIlR5lcT8fJ9H+sMPi78WLvR9227rrR1nn+2zeXzwgS/WUlpWll/IKCIiIpIqCtZSfT77zBd1efFFXynl97/3VRI7darUaRYsKA7QkyZ5qF63zvftvDMccwz07Olfu+9evLphUd90snaQRo38AkYRERGRVFGwltSbONHnoH77bWjWzJudr7qqQpM+FxbCV18Vh+gPP/Q2D/ALDXv08OnyioJ0eafMzvYcP2SIj3IXFnrobtTIt5eeck9ERESkKhSsJTVKr5LYurV//4c/eH9GGVauhI8/Lg7ROTnFqxq2aeMThVx6qd/uv3/llwgfPNhHtIcO9bmpu3ZNPo+1iIiISFUpWEvVlF4lsWNHeOAB+PWvN7kyMASfpjpxNPrzz30k2cznjD77bA/RPXv6KoepmEM6O1tBWkRERKqfgrVsmXXrildJLFrS8Ikn4NxzN66SuH69L7ySOFvH3Ln+8K239rB7880epA86CJo3j+3ViIiIiFSZgrVUTtEqiXff7dPn7bsvjBwJp57KkuX1+PCd4gsNP/7YVzwE2HFHnze6aDR6r71gK/3rExERkQyiaCMl5eR4Q3LRKHRRQ3KpVRJDr97M+NuzTLJefDjamPQ3+N///BRbbQX77Qe/+13xRYbt28f7skRERESqm5Y0r2PKys0ADBrkU2isWeMN0VlZfrXg/vuz5ouZfLJyFz7sdhGTWp3I5G9bsnixP2y77YoDdM+ecMABWnhFREREMlN5S5orWNchyXJz0dRzg4/JKTHp81za8SE9mcQhfEhPPrXuFAT/A8cuuxS3dBxyiAf0ormjRURERDJZecFarSB1RE6Oh+rExVIKC/3+kCHwq1FT+WX1hUyiJx/Sk1nsBEAj1nAgH3Ptrm9yyF0nkZ0NrVrF9CJERERE0piCdR0xdGjxhYSlrV4dGPjxZQC0Yy6HMIkreIBDmMS+TKMB66FJdzj+pBqsWERERKR2UbCuZuX2NKdQCPDzz5CXB7m5JW/z8mD8eD8mOWOnrWbzfsGh7MgsNpk6OivLixcRERGRMilYV6PSPc3TpsFrr0U9zYMrfp7162HevJJBOVl4Xreu5OOysgLbb7uG9vXm02p9I/LYnsCmzdBZWZB9WBM6T1oIqzfZ7Y3YV1xRqdcuIiIiUtcoWFeTzfU0H3OMj1z/8svmA/OCBZuONjdqBB06+DR2Bx8cfd98Fe2XfEmHHyfQ/vO32P67CWy1dAO0aEFOr0voN/lWVq/bNFg3agRX/L0VvH2tF5ef78UmXt2opQtFREREyqVgXU3K72mGo47y71es2HT/dtt5YO7QweeDLvq+ffvi71u0APtlJUyYAO+/D+++70PiIfiyhn36wO/ugMMPh332IbtePa4dtJncnD3YE//QoTBzJnTtWn29KyIiIiIZRtPtVZPu3eHTT8ve37w5nH9+cWAuut1hh3LmgM7Ph8mTPUi//74vbVhQ4EuI9+zpIbpfP59Iun79pKco6vlWbhYRERGpPE23F4Nu3XwAubBw031ZWT4w/MADmzlJQQF88klxkJ40CdauhXr1PDxff72H6Z49oXHjCtWVna0gLSIiIlIdYg3WZtYReApoCwTg0RDCUDPbDhgBdAZmAWeGEJbGVWdSm5nu48or/ULF1UkuBmzUYANXXFFv0x2FhfDFF8VBevx4WLnS9+2zD/zhDx6ke/eGbbetphcmIiIiIlsi7hHrAuBPIYRPzawpMNXM3gUuBMaEEO4wsxuBG4EbYqyzpApM95Gd7XeH3L6O/PX1KKQeWWygEflcWzCU7LfXwkG3wLffFgfpsWNhyRJ/jm7d4LzzPEj37atVWURERETSXFr1WJvZq8BD0VffEMI8M2sHjAsh7FLeY2usxzqn5NLfJTRpAmPGFI9c5+SQ0/dGhq69hJl0pSszuZIHyOYjb+do0cInnwbo2NHPe/jhcNhh3nQtIiIiImmlVvRYm1lnYD/gI6BtCGFetGs+3iqSHjY33ccpp3jbBsDnn5O9dj7ZfLDpsRs2eBB/9FEP0126gG2yNIuIiIiI1BJpEazNbBvgJeCqEMIKSwiYIYRgZkmH1c3sEuASgE6dOtVEqd5TXd4o/8qVsGyZf59sLr1ErVvDb3+bstJEREREJD6brhZSw8ysPh6qnw0hvBxtXhC1gBDdLkz22BDCoyGEHiGEHq1bt66Zgrt182k9ksnKghNP9HaRnBz/vrxjtUy4iIiISMaINVibD00/DnwTQrg3YddrwMDo+4HAqzVdW5muvNJXVUmm9NLflTlWRERERGq1uEesDwHOBw43s2nR17HAHcARZjYT6B/dTw9F0300aVI8Gp2V5fdLL/1dmWNFREREpFZLq1lBqqLGV16szBKGWu5QREREJCOUNyuIgrWIiIiISAWVF6zjbgUREREREckICtYiIiIiIimgYC0iIiIikgIK1iIiIiIiKaBgLSIiIiKSAgrWIiIiIiIpoGAtIiIiIpICCtYiIiIiIimgYC0iIiIikgIK1iIiIiIiKaBgLSIiIiKSAgrWIiIiIiIpoGAtIiIiIpICCtYiIiIiIilgIYS4a0gJM1sE/FRNp28F/FxN55b0ove67tB7XXfova5b9H7XHXG91zuGEFon25Exwbo6mdmUEEKPuOuQ6qf3uu7Qe1136L2uW/R+1x3p+F6rFUREREREJAUUrEVEREREUkDBumIejbsAqTF6r+sOvdd1h97rukXvd92Rdu+1eqxFRERERFJAI9YiIiIiIimgYF0OMzvazL41s+/M7Ma465HUMbOOZjbWzL42s6/M7Mpo+3Zm9q6ZzYxuW8Rdq6SGmdUzs8/M7I3o/k5m9lH0+z3CzBrEXaOkhpk1N7MXzex/ZvaNmR2s3+3MZGZXR/8Nn25mz5tZI/1uZw4ze8LMFprZ9IRtSX+XzT0Qve9fmNn+cdSsYF0GM6sH/As4BtgdGGBmu8dblaRQAfCnEMLuQDZwWfT+3giMCSF0BcZE9yUzXAl8k3D/TuC+EMLOwFLg17FUJdVhKDAqhLArsA/+vut3O8OYWXvgCqBHCGFPoB5wNvrdziRPAkeX2lbW7/IxQNfo6xLgkRqqsQQF67IdCHwXQvghhLAOGA6cFHNNkiIhhHkhhE+j71fi/+Ntj7/Hw6LDhgEnx1KgpJSZdQCOA/4vum/A4cCL0SF6rzOEmW0L9AEeBwghrAshLEO/25lqK6CxmW0FNAHmod/tjBFCGA8sKbW5rN/lk4CngssBmptZuxopNIGCddnaA3MS7udG2yTDmFlnYD/gI6BtCGFetGs+0DauuiSl7geuBwqj+y2BZSGEgui+fr8zx07AIuA/UevP/5nZ1uh3O+OEEPKAIcBsPFAvB6ai3+1MV9bvclrkNgVrqdPMbBvgJeCqEMKKxH3Bp8zRtDm1nJkdDywMIUyNuxapEVsB+wOPhBD2A1ZRqu1Dv9uZIeqtPQn/MLUDsDWbtg1IBkvH32UF67LlAR0T7neItkmGMLP6eKh+NoTwcrR5QdGfjqLbhXHVJylzCHCimc3CW7oOx3twm0d/Pgb9fmeSXCA3hPBRdP9FPGjrdzvz9Ad+DCEsCiGsB17Gf9/1u53ZyvpdTovcpmBdtk+ArtHVxQ3wCyJei7kmSZGox/Zx4JsQwr0Ju14DBkbfDwRerenaJLVCCDeFEDqEEDrjv8fvhxDOBcYCp0eH6b3OECGE+cAcM9sl2tQP+Br9bmei2UC2mTWJ/pte9F7rdzuzlfW7/BpwQTQ7SDawPKFlpMZogZhymNmxeG9mPeCJEMI/461IUsXMegETgC8p7rv9M95nPRLoBPwEnBlCKH3hhNRSZtYXuDaEcLyZdcFHsLcDPgPOCyGsjbE8SREz2xe/ULUB8ANwET6QpN/tDGNmg4Gz8JmePgN+g/fV6nc7A5jZ80BfoBWwABgE/Jckv8vRh6uH8Hag1cBFIYQpNV6zgrWIiIiISNWpFUREREREJAUUrEVEREREUkDBWkREREQkBRSsRURERERSQMFaRERERCQFFKxFRAAzmxUtIpMWzKyzmQUzezLuWkREpGIUrEVEYhIF53Fx1xEnM7sl+jn0jbGGZmb2sJnlmtliM3vdzH5VxrG/MbP1ZrZfTdcpIulvq80fIiJSJ/SLu4BS8oDdgOVxF1IHPAmcCDyDLyxxITDGzHYPIawuOsjM2gNDgDtDCJ/FUKeIpDkFaxERIITwfdw1JAohrAf+F3cdmc7M2gKnAINCCH+Ptn2Eh+3j8RXeivw//APP32u4TBGpJdQKIiK1TmL/sZl1M7MRZrbQzAoTWwrM7Cgze8vMfjaztWb2vZndbWbNk5yzzB5rMxtgZmPNbJmZ5ZvZN2b2FzNrWMbxu5rZE9E510a1TTCz30f7LzSzomVvD41eS9HXLaVfY5LztzOzf0XnX2dmi8zsZTPrnuTYC6PzXGhmh5nZODNbaWYrzOxNM9ttMz/uxHP1LarRzA6MHr8k2tY5OuYwM3vUzL6OnmONmU03s0Fm1qj0zxxfohhgbOLPodRxTczsJjObZmarzOwXM5tsZgMqWns5doxuP07Y9nGpfZjZecCxwMUhhHUpeF4RyUAasRaR2uxXwEfADOBZoDGwAsDMBgG3AEuAN4CFwN7AtcCxZnZwCGHF5p7AzJ4ALgJygZeAZUA2cCvQz8yOCCEUJBx/HPAC0BAYBTwPNAf2Aa4HHgGmAYPxUPkTPjpaZNxm6tkJmAjsALwfnb8jcAZwnJmdFkJ4I8lDjwdOAt7GR153x4PiAVHLw8+b+1kkOBi4KarjCaAVUBQ2bwB2BT4E3gQaAYfg70VfM+sfQtgQHXs/cDJwKDAMmJXk9TaPXud+wKfR82UBRwHPmdkeIYS/VKL20mZHt93x9wugR3T7U1RD26jW+0IIH1XhuUQk04UQ9KUvfemrVn0BnYEQfd2WZP9h0b4Pgeal9l0Y7buv1PZZwKwyjn0ZaFxq3y3RvisTtrXCe6LXAYcmqatDqfsBGLeZ1/hkqe2jo+03l9reEygAFgPbJHkNBUC/Uo+5Pdp3fQV/7n0Tfu6/K+OYLoAl2X5r9Lizyvg59i3jfE8mqxEP7KOAQmDfKv57+m/0nj0BPAyswkP11tH+F/EPb42r8jz60pe+Mv9LrSAiUpstwEd+S7siuv1tCGFZ4o4QwpP4iPG5FTj/lXggvTiEsKbUvlvxEJt4noFAM+CREMIHpU8WQsitwHOWycw6AEfio6x3lTr3h/jo9XbAqUkePjyEMKbUtkej2wMrWcq0EMK/k+0IIfwQQghJdt0X3R5V0Scxs5bAecCUEELp15uPj44bcE5Fz1mGgcB/gKOBs/G/GvQPIawys9Pxn+evgUIzezBqf1kXtdXsXsXnFpEMolYQEanNPg8hrE2y/WBgPXCGmZ2RZH8DoLWZtQwhLE52YjNrgrdv/AxcZWbJDluLz9xRJDu6fbuC9VdW0RRvE4Jf3Fja+3gQ3Q94qtS+KUmOnxPdtqhkHR+XtcPMtsY/kJwCdAOa4uG3SPtKPM8BQD1gY+95KfWj2wr3iScTQlgO/C762sjMtgMeAh4OIUwws/uBS4Dr8BHsu4FRZtYtCvoiUscpWItIbTa/jO0t8f++DSpjf5Ft8FHnZFrggbB1Bc5TpHl0m1fB4ytr2+h2Xhn7i7Y3T7JvWekNIYSC6ANDvUrWkfTnbmb18XB/IDAdGAEswj/kgP8ck17wWYaW0e0B0VdZtqnEOSvjAWANcGP0geH3wNMhhAcAzGwVMB4fMX+immoQkVpEwVpEarNkLQfgfc5ZIYTtqnDuovmjPwsh7F/BxyyLbtsDX1bhuctSVNP2ZexvV+q46lLWz/0kPFQ/GUK4KHGHmbWj4h9QihS9jvtCCNdU8rFVEl2Eei5wRAjhFzPbG/9Lx6cJh02NbveoydpEJH2px1pEMlEO0MLMtjjwhBB+Ab4C9ohaAir6vADHVPD4Qio3Wly0KEkvM0s2MHJYdPtpkn01Yefo9uUk+w4t4zFFM4Qk+zl8jP+Melexrkoxs22BfwOPhxDeK7U7ccS9ESIiCRSsRSQTFV0o95iZ7VB6p5ltbWbZpbcncS8+SvlEGXNftzCzxNHsYfh0f783sz5Jju9QatNifKq8CokufnwXnzHkqlLnPghvSVgKvFLRc6bYrOi2b+JGM+sC3FnGY4pacTqV3hFCWIhPo9jDzP5qZpuEbzP7VTQFYeK2cVa1ZdLviW7/lLDte3zmkOMTtp0Q3X61hc8jIhlGrSAiknFCCGPM7EZ8OrmZZvYW8CPei7sjPno6EZ8ForzzPBEtuvIH4HszG43PyLEdsBPQB59N4tLo+J/N7Bx8eraxZvY28AU+U8jeeIhODIFjgLPN7HV8lHk9MD6EML6csi4FJgF3m9mR+EWJRfNYFwIXhRBWbv6nVC1eB74DrjGzvfAR9k54GH2TJOEZGIvXfbuZ7Yl/MCCE8I9o/+VAV3y1w/PNbCI+G8wO+EWLBwAD8Pe3SNGgUQGVZGb98RlAToguaiSqZ5WZ/Qu42sxGRa/zIvwC0Ocq+zwikpkUrEUkI4UQ7jSzSfjUe73w/t/l+IWFj1LBMBRCuCwKyJcC/fELA5fgAftu4JlSx79pZj3wqeD64dPjLcWXJ7+91OmvxPuV++GLtWTh0weWGaxDCD9E5/9L9Ji++Cj5KOCfIYRPKvK6qkMUPg8H7ojq6g38gE9NeC9wVpLHfGNmA/GFe/5AcXvFP6L9K8zsUHw2jnOA06JjFgAzgavxUXwAzK/G3AMfPS9qzakQM9sGeAx4NiRfZOcm/D06N3p9HwKXa0YQESliyacbFRGpW8xsPrA8hLBL3LXIlosuMvwcuCyE8HDc9YhI3aIeaxGp86KLE1vhy5ZL7XYoPpqt6e9EpMYpWItInWVm25rZrXgbRT28N1pqsRDCgyGE7dWeISJxUCuIiNRZZtYZvwjtR+Bx4K4QQmGsRYmISK2lYC0iIiIikgJqBRERERERSQEFaxERERGRFFCwFhERERFJAQVrEREREZEUULAWEREREUkBBWsRERERkRT4/wUYLpMlG60CAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "plt.xlabel(\"rejection rate, %\", fontsize = 20)\n",
    "plt.ylabel(\"accuracy, %\", fontsize = 20)\n",
    "plt.title(\"mGENRE certainty basing on 400 random samples\")\n",
    "plt.plot(share_of_observations_400_top_1[::-1], accuracy_400_top_1, label = \"w/o ner\", c = \"red\", marker = '.', markersize = 15);\n",
    "\n",
    "plt.plot(share_of_observations_400_ner_top_1[::-1], accuracy_400_ner_top_1, label = \"with ner\", c = \"blue\", marker = '.', markersize = 15);\n",
    "plt.legend(fontsize = 15);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another NER models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = ''' I want a person available 7 days and with prompt response all most every time. Only Indian freelancer need I need PHP developer who have strong experience in Laravel and Codeigniter framework for daily 4 hours. I need this work by Monday 27th Jan. should be free from plagiarism . \n",
    "Need SAP FICO consultant for support project needs to be work on 6 months on FI AREAWe.  Want a same site to be created as the same as this https://www.facebook.com/?ref=logo, please check the site before contacting to me and i want this site to be ready in 10 days. They will be ready at noon tomorrow .'''\n",
    "\n",
    "russian_text = '''Власти Москвы выделили 110 млрд рублей на поддержку населения, системы здравоохранения и городского хозяйства. Об этом сообщается на сайте мэра столицы https://www.sobyanin.ru/ в пятницу, 1 мая. По адресу Алтуфьевское шоссе д.51 (основной вид разрешенного использования: производственная деятельность, склады) размещен МПЗ? Подпоручик Киже управляя автомобилем ВАЗ2107 перевозил автомат АК47 с целью ограбления банка ВТБ24, как следует из записей. \n",
    "Взыскать c индивидуального предпринимателя Иванова Костантипа Петровича дата рождения 10 января 1970 года, проживающего по адресу город Санкт-Петербург, ул. Крузенштерна, дом 5/1А 8 000 (восемь тысяч) рублей 00 копеек гос. пошлины в пользу бюджета РФ Жители требуют незамедлительной остановки МПЗ и его вывода из района. Решение было принято по поручению мэра города Сергея Собянина в связи с ограничениями из-за коронавируса.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/petrakov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/petrakov/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/petrakov/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /home/petrakov/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPE Indian/JJ)\n",
      "(ORGANIZATION PHP/NNP)\n",
      "(GPE Laravel/NNP)\n",
      "(PERSON Need/NNP)\n",
      "(ORGANIZATION SAP/NNP)\n",
      "(ORGANIZATION FI/NNP)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "for sent in nltk.sent_tokenize(english_text):\n",
    "   for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "      if hasattr(chunk, 'label'):\n",
    "         print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPE Name/NN)\n",
      "(PERSON Alan/NNP Pierson/NNP)\n",
      "(GPE Which/NNP)\n",
      "(GPE Name/NN)\n",
      "(GPE Which/JJ)\n",
      "(GPE Which/JJ)\n",
      "(GPE Queens/NNP University/NNP)\n",
      "(ORGANIZATION Bangladesh/NNP)\n",
      "(PERSON Claire/NNP Stansfield/NNP)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,100):\n",
    "    for sent in nltk.sent_tokenize(data.loc[i,\"question\"]):\n",
    "       for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "          if hasattr(chunk, 'label'):\n",
    "             print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk.tag.stanford.StanfordNERTagger'; 'nltk.tag.stanford' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-f201a0288361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstanford\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStanfordNERTagger\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mjar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"stanford-ner-2015-04-20/stanford-ner-3.5.2.jar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"stanford-ner-2015-04-20/classifiers/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mst_3class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"english.all.3class.distsim.crf.ser.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mst_4class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"english.conll.4class.distsim.crf.ser.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk.tag.stanford.StanfordNERTagger'; 'nltk.tag.stanford' is not a package"
     ]
    }
   ],
   "source": [
    "import nltk.tag.stanford.StanfordNERTagger as StanfordNERTagger\n",
    "jar = \"stanford-ner-2015-04-20/stanford-ner-3.5.2.jar\"\n",
    "model = \"stanford-ner-2015-04-20/classifiers/\" \n",
    "st_3class = StanfordNERTagger(model + \"english.all.3class.distsim.crf.ser.gz\", jar, encoding='utf8') \n",
    "st_4class = StanfordNERTagger(model + \"english.conll.4class.distsim.crf.ser.gz\", jar, encoding='utf8') \n",
    "st_7class = StanfordNERTagger(model + \"english.muc.7class.distsim.crf.ser.gz\", jar, encoding='utf8')\n",
    "for i in [st_3class.tag(english_text.split()), st_4class.tag(english_text.split()), st_7class.tag(english_text.split())]:\n",
    "  for b in i:\n",
    "    if b[1] != 'O':\n",
    "        print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy) (4.62.0)\n",
      "Collecting pathy>=0.3.5\n",
      "  Using cached pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Using cached wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (835 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m835.9/835.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/petrakov/.local/lib/python3.8/site-packages (from spacy) (59.5.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4\n",
      "  Using cached pydantic-1.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy) (1.23.1)\n",
      "Requirement already satisfied: jinja2 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy) (3.0.3)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Using cached spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Using cached typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.6-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy) (21.3)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/petrakov/.local/lib/python3.8/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/petrakov/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/petrakov/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/petrakov/.local/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/petrakov/.local/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: wasabi, murmurhash, cymem, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, preshed, langcodes, catalogue, blis, srsly, pathy, thinc, spacy\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 6.0.0\n",
      "    Uninstalling smart-open-6.0.0:\n",
      "      Successfully uninstalled smart-open-6.0.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.3\n",
      "    Uninstalling pydantic-1.3:\n",
      "      Successfully uninstalled pydantic-1.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "flair 0.6.1 requires scikit-learn>=0.21.3, but you have scikit-learn 0.21.2 which is incompatible.\n",
      "deeppavlov 0.17.4 requires numpy==1.18.0, but you have numpy 1.23.1 which is incompatible.\n",
      "deeppavlov 0.17.4 requires pydantic==1.3, but you have pydantic 1.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed blis-0.7.8 catalogue-2.0.7 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.7 pathy-0.6.2 preshed-3.0.6 pydantic-1.9.1 smart-open-5.2.1 spacy-3.4.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.1.0 typer-0.4.2 wasabi-0.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 days DATE\n",
      "Indian NORP\n",
      "Laravel GPE\n",
      "daily DATE\n",
      "4 hours TIME\n",
      "Monday 27th Jan. DATE\n",
      "6 months DATE\n",
      "https://www.facebook.com/?ref=logo ORG\n",
      "10 days DATE\n",
      "noon TIME\n",
      "tomorrow DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_lg\n",
    "\n",
    "\n",
    "model_sp = en_core_web_lg.load()\n",
    "for ent in model_sp(english_text).ents:\n",
    "  print(ent.text.strip(), ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question num:  0 \n",
      " question:  who is a musician born in detroit\n",
      "entity:  detroit \n",
      " label:  GPE \n",
      "\n",
      "question num:  1 \n",
      " question:  what is the language in which mera shikar was filmed in\n",
      "entity:  mera shikar \n",
      " label:  PERSON \n",
      "\n",
      "question num:  2 \n",
      " question:  Whats the name of a battle that happened in chicago\n",
      "entity:  chicago \n",
      " label:  GPE \n",
      "\n",
      "question num:  3 \n",
      " question:  what player plays the position midfielder?\n",
      "question num:  4 \n",
      " question:  what is the position that  mike twellman plays\n",
      "entity:  mike twellman \n",
      " label:  PERSON \n",
      "\n",
      "question num:  5 \n",
      " question:  list some musical films\n",
      "question num:  6 \n",
      " question:  what is ellen swallow richards's nationality?\n",
      "entity:  richards \n",
      " label:  PERSON \n",
      "\n",
      "question num:  7 \n",
      " question:  What language is the show elementary broadcast in?\n",
      "question num:  8 \n",
      " question:  what is the gender of james hendry?\n",
      "entity:  james hendry \n",
      " label:  PERSON \n",
      "\n",
      "question num:  9 \n",
      " question:  who was a voice actor?\n",
      "question num:  10 \n",
      " question:  What is a member of the 1893 jakoba asteroid group?\n",
      "entity:  1893 \n",
      " label:  DATE \n",
      "\n",
      "entity:  jakoba asteroid \n",
      " label:  FAC \n",
      "\n",
      "question num:  11 \n",
      " question:  What artist includes black star at the point of darkness in his work?\n",
      "question num:  12 \n",
      " question:  Name an album by serge gainsbourg\n",
      "entity:  serge gainsbourg \n",
      " label:  PERSON \n",
      "\n",
      "question num:  13 \n",
      " question:  what is a book by laura ingalls wilder \n",
      "entity:  laura ingalls \n",
      " label:  PERSON \n",
      "\n",
      "question num:  14 \n",
      " question:  Who was the cinematographer for the film endless love?\n",
      "question num:  15 \n",
      " question:  what is a documentary film about the media\n",
      "question num:  16 \n",
      " question:  what musical genre does  brandon reilly create\n",
      "entity:  brandon reilly \n",
      " label:  PERSON \n",
      "\n",
      "question num:  17 \n",
      " question:  What sport does notre dame fighting irish men's basketball play\n",
      "entity:  notre dame fighting \n",
      " label:  ORG \n",
      "\n",
      "question num:  18 \n",
      " question:  where in germany was rudi ball born in?\n",
      "entity:  germany \n",
      " label:  GPE \n",
      "\n",
      "entity:  rudi ball \n",
      " label:  PERSON \n",
      "\n",
      "question num:  19 \n",
      " question:  what time zone is marrakech in?\n",
      "question num:  20 \n",
      " question:  which country does harry blackstone, jr. come from\n",
      "entity:  harry blackstone \n",
      " label:  PERSON \n",
      "\n",
      "question num:  21 \n",
      " question:  what is valeria richards's gender?\n",
      "entity:  valeria richards's \n",
      " label:  PERSON \n",
      "\n",
      "question num:  22 \n",
      " question:  what movie did liliana cavani direct?\n",
      "entity:  liliana \n",
      " label:  ORG \n",
      "\n",
      "question num:  23 \n",
      " question:  What kind of video game is the dog island\n",
      "question num:  24 \n",
      " question:  who created the loves of the gods\n",
      "question num:  25 \n",
      " question:  What did jane austen write?\n",
      "entity:  jane austen \n",
      " label:  PERSON \n",
      "\n",
      "question num:  26 \n",
      " question:  What is the nationality of estella warren?\n",
      "entity:  estella warren \n",
      " label:  ORG \n",
      "\n",
      "question num:  27 \n",
      " question:  what is a second level division of turkey\n",
      "entity:  second \n",
      " label:  ORDINAL \n",
      "\n",
      "question num:  28 \n",
      " question:  what gender does eugênio sales identify as \n",
      "question num:  29 \n",
      " question:  Name a person born in chicago.\n",
      "entity:  chicago \n",
      " label:  GPE \n",
      "\n",
      "question num:  30 \n",
      " question:  who is the artist on seven wishes\n",
      "entity:  seven \n",
      " label:  CARDINAL \n",
      "\n",
      "question num:  31 \n",
      " question:  who wrote the music for gangs of new york\n",
      "entity:  new york \n",
      " label:  GPE \n",
      "\n",
      "question num:  32 \n",
      " question:  what is the gender of sophie merry?\n",
      "entity:  sophie merry \n",
      " label:  ORG \n",
      "\n",
      "question num:  33 \n",
      " question:  what is an episode written by michelle ashford\n",
      "entity:  michelle ashford \n",
      " label:  PERSON \n",
      "\n",
      "question num:  34 \n",
      " question:  what movies has francis ford coppola contributed music to\n",
      "entity:  francis ford coppola \n",
      " label:  PERSON \n",
      "\n",
      "question num:  35 \n",
      " question:  which is the main ideology of the communist party of britain?\n",
      "entity:  the communist party \n",
      " label:  ORG \n",
      "\n",
      "entity:  britain \n",
      " label:  GPE \n",
      "\n",
      "question num:  36 \n",
      " question:  what former basketball played the center (basketball) position\n",
      "question num:  37 \n",
      " question:  Who wrote the film thunderbolt and lightfoot?\n",
      "question num:  38 \n",
      " question:  where in the united states was john morris russell born\n",
      "entity:  the united states \n",
      " label:  GPE \n",
      "\n",
      "entity:  john morris russell \n",
      " label:  PERSON \n",
      "\n",
      "question num:  39 \n",
      " question:  what is imam mustafayev's gender\n",
      "entity:  imam mustafayev's \n",
      " label:  PERSON \n",
      "\n",
      "question num:  40 \n",
      " question:  what is fausto fawcett's gender\n",
      "entity:  fausto fawcett's \n",
      " label:  PERSON \n",
      "\n",
      "question num:  41 \n",
      " question:  what type of film is generation kill\n",
      "question num:  42 \n",
      " question:  What county has jurisdiction over chokoloskee, florida\n",
      "entity:  chokoloskee \n",
      " label:  GPE \n",
      "\n",
      "entity:  florida \n",
      " label:  GPE \n",
      "\n",
      "question num:  43 \n",
      " question:  which country is john berry from \n",
      "question num:  44 \n",
      " question:  What kind of book is home?\n",
      "question num:  45 \n",
      " question:  what country is cosmic ray from\n",
      "question num:  46 \n",
      " question:  what is the administrative child of zou?\n",
      "question num:  47 \n",
      " question:  What is Alan Pierson's gender?\n",
      "entity:  Alan Pierson's \n",
      " label:  PERSON \n",
      "\n",
      "question num:  48 \n",
      " question:  What was the nationality of  franz roh\n",
      "entity:  franz roh \n",
      " label:  ORG \n",
      "\n",
      "question num:  49 \n",
      " question:  what is a city located in huron county\n",
      "entity:  huron county \n",
      " label:  GPE \n",
      "\n",
      "question num:  50 \n",
      " question:  What is an example of a documentary film?\n",
      "question num:  51 \n",
      " question:  What actor was born in lodz\n",
      "question num:  52 \n",
      " question:  what member of asteroid group is 8668 satomimura a part of \n",
      "question num:  53 \n",
      " question:  What gender is elizabeth malecki\n",
      "entity:  elizabeth malecki \n",
      " label:  PERSON \n",
      "\n",
      "question num:  54 \n",
      " question:  where was  james gillespie buried\n",
      "entity:  james gillespie \n",
      " label:  PERSON \n",
      "\n",
      "question num:  55 \n",
      " question:  who was born in pittsburgh\n",
      "entity:  pittsburgh \n",
      " label:  GPE \n",
      "\n",
      "question num:  56 \n",
      " question:  lannie balcom self-identifies as which gender? \n",
      "entity:  lannie balcom \n",
      " label:  PERSON \n",
      "\n",
      "question num:  57 \n",
      " question:  which position does robert williams play in american football\n",
      "entity:  robert williams \n",
      " label:  PERSON \n",
      "\n",
      "entity:  american \n",
      " label:  NORP \n",
      "\n",
      "question num:  58 \n",
      " question:  who is an instrumentalist that plays the guitar\n",
      "question num:  59 \n",
      " question:  Who wrote the book the city in the autumn stars\n",
      "question num:  60 \n",
      " question:  Who was a parent of john of gaunt, 1st duke of lancaster\n",
      "entity:  john of gaunt \n",
      " label:  PERSON \n",
      "\n",
      "entity:  1st \n",
      " label:  ORDINAL \n",
      "\n",
      "question num:  61 \n",
      " question:  chris cash is what nationality\n",
      "entity:  chris \n",
      " label:  PERSON \n",
      "\n",
      "question num:  62 \n",
      " question:  in what language is the the law is the law film spoken\n",
      "question num:  63 \n",
      " question:  who discovered 6945 dahlgren\n",
      "entity:  6945 \n",
      " label:  CARDINAL \n",
      "\n",
      "question num:  64 \n",
      " question:  Where is frederick baldwin adams from?\n",
      "entity:  frederick baldwin adams \n",
      " label:  PERSON \n",
      "\n",
      "question num:  65 \n",
      " question:  Who's a hungarian model\n",
      "entity:  hungarian \n",
      " label:  NORP \n",
      "\n",
      "question num:  66 \n",
      " question:  what is joshua smith's gender\n",
      "entity:  joshua smith's \n",
      " label:  PERSON \n",
      "\n",
      "question num:  67 \n",
      " question:  What label is rapture ruckus  signed to\n",
      "question num:  68 \n",
      " question:  Which film did rob schmidt direct?\n",
      "entity:  schmidt \n",
      " label:  PERSON \n",
      "\n",
      "question num:  69 \n",
      " question:  Name a person born in montilla\n",
      "entity:  montilla \n",
      " label:  GPE \n",
      "\n",
      "question num:  70 \n",
      " question:  who was the director of mr. hankey's christmas classics \n",
      "entity:  hankey \n",
      " label:  PERSON \n",
      "\n",
      "entity:  christmas \n",
      " label:  DATE \n",
      "\n",
      "question num:  71 \n",
      " question:  Where is ottilie assing from?\n",
      "question num:  72 \n",
      " question:  Name someone who was born in mytilene\n",
      "question num:  73 \n",
      " question:  what is a film in the genre drama\n",
      "question num:  74 \n",
      " question:  Which series has a pilot episode?\n",
      "question num:  75 \n",
      " question:  what is a horror film?\n",
      "question num:  76 \n",
      " question:  what country does the film stealing a nation take place in\n",
      "question num:  77 \n",
      " question:  which type of music genre does carlos johnson play in?\n",
      "entity:  carlos johnson play \n",
      " label:  PERSON \n",
      "\n",
      "question num:  78 \n",
      " question:  Which town was thomas sully born? \n",
      "question num:  79 \n",
      " question:  What position does kelly shoppach play in baseball\n",
      "entity:  kelly shoppach \n",
      " label:  PERSON \n",
      "\n",
      "question num:  80 \n",
      " question:  what kind of movie is dracula?\n",
      "entity:  dracula \n",
      " label:  PERSON \n",
      "\n",
      "question num:  81 \n",
      " question:  what is politician is founder and chairperson of Queens University, Bangladesh\n",
      "entity:  Queens University \n",
      " label:  ORG \n",
      "\n",
      "entity:  Bangladesh \n",
      " label:  GPE \n",
      "\n",
      "question num:  82 \n",
      " question:  What language is spoken in the film life goes on\n",
      "question num:  83 \n",
      " question:  who is the parent of bob woodward?\n",
      "entity:  bob woodward \n",
      " label:  PERSON \n",
      "\n",
      "question num:  84 \n",
      " question:  what kind of game is knight lore\n",
      "question num:  85 \n",
      " question:  what film does gregory la cava direct?\n",
      "entity:  gregory la \n",
      " label:  PERSON \n",
      "\n",
      "question num:  86 \n",
      " question:  what artist makes celtic rock?\n",
      "entity:  celtic rock \n",
      " label:  ORG \n",
      "\n",
      "question num:  87 \n",
      " question:  What is the place of barbara cook's birth?\n",
      "entity:  barbara cook's \n",
      " label:  PERSON \n",
      "\n",
      "question num:  88 \n",
      " question:  What is Claire Stansfield's nationality?\n",
      "entity:  Claire Stansfield's \n",
      " label:  PERSON \n",
      "\n",
      "question num:  89 \n",
      " question:  What was the cause of death of michel simon\n",
      "entity:  michel simon \n",
      " label:  PERSON \n",
      "\n",
      "question num:  90 \n",
      " question:  What is the nationality of antonio banderas\n",
      "entity:  antonio banderas \n",
      " label:  PERSON \n",
      "\n",
      "question num:  91 \n",
      " question:  which country is willi wild from\n",
      "question num:  92 \n",
      " question:  What sport is criciúma esporte clube a part of?\n",
      "entity:  criciúma esporte \n",
      " label:  PERSON \n",
      "\n",
      "question num:  93 \n",
      " question:  is adario strange male or female\n",
      "question num:  94 \n",
      " question:  Who is someone born in wittlich\n",
      "entity:  wittlich \n",
      " label:  ORG \n",
      "\n",
      "question num:  95 \n",
      " question:  which historical figure was killed by smallpox\n",
      "question num:  96 \n",
      " question:  what genre is the film on the nickel\n",
      "question num:  97 \n",
      " question:  What is the name of a place located in los angeles county, california\n",
      "entity:  los angeles county \n",
      " label:  GPE \n",
      "\n",
      "entity:  california \n",
      " label:  GPE \n",
      "\n",
      "question num:  98 \n",
      " question:  What is mark difelice position \n",
      "question num:  99 \n",
      " question:  which city was shmuel salant born\n",
      "entity:  shmuel salant \n",
      " label:  PERSON \n",
      "\n",
      "question num:  100 \n",
      " question:  who authored summertide\n"
     ]
    }
   ],
   "source": [
    "for j, question in enumerate(list(data.loc[:100, 'question'])):\n",
    "    print(\"question num: \", j, \"\\n\", \"question: \", question)\n",
    "    for ent in model_sp(question).ents:\n",
    "        print(\"entity: \", ent.text.strip(), \"\\n\", \"label: \", ent.label_, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-13 05:09:48,693 https://nlp.informatik.hu-berlin.de/resources/models/ner/en-ner-conll03-v0.4.pt not found in cache, downloading to /tmp/tmpqsq3oe4z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 432197603/432197603 [00:38<00:00, 11342440.43B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-13 05:10:26,989 copying /tmp/tmpqsq3oe4z to cache at /home/petrakov/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-13 05:10:27,364 removing temp file /tmp/tmpqsq3oe4z\n",
      "2022-07-13 05:10:27,422 loading file /home/petrakov/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-da41950b454e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequenceTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequenceTagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/flair/nn.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, model)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_model_with_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/flair/models/sequence_tagger_model.py\u001b[0m in \u001b[0;36m_init_model_with_state_dict\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mreproject_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reproject_to\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         model = SequenceTagger(\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hidden_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/flair/models/sequence_tagger_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_size, embeddings, tag_dictionary, tag_type, use_crf, use_rnn, rnn_layers, dropout, word_dropout, locked_dropout, reproject_embeddings, train_initial_hidden_state, rnn_type, pickle_module, beta, loss_weights)\u001b[0m\n\u001b[1;32m    230\u001b[0m             ] = -10000\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Module'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_grad_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_grad_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     ) -> RemovableHandle:\n\u001b[0;32m--> 612\u001b[0;31m         r\"\"\"Registers a backward hook on the module.\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mwarning\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_compatible_shallow_copy_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;31m# If the new tensor has compatible tensor type as the existing tensor,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0;31m# the current behavior is to change the tensor in-place using `.data =`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;31m# and the future behavior is to overwrite the existing tensor. However,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0;31m# changing the current behavior is a BC-breaking change, and we want it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_compatible_shallow_copy_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;31m# If the new tensor has compatible tensor type as the existing tensor,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0;31m# the current behavior is to change the tensor in-place using `.data =`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;31m# and the future behavior is to overwrite the existing tensor. However,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0;31m# changing the current behavior is a BC-breaking change, and we want it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_compatible_shallow_copy_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;31m# If the new tensor has compatible tensor type as the existing tensor,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0;31m# the current behavior is to change the tensor in-place using `.data =`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;31m# and the future behavior is to overwrite the existing tensor. However,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0;31m# changing the current behavior is a BC-breaking change, and we want it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/flair/models/language_model.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    426\u001b[0m                         _flat_weights_names)\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mchild_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# Flattens params (on CUDA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mflatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_cudnn_rnn_flatten_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                     torch._cudnn_rnn_flatten_weight(\n\u001b[0;32m--> 149\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cudnn_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                         self.batch_first, bool(self.bidirectional))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "tagger = SequenceTagger.load('ner')\n",
    "from flair.data import Sentence\n",
    "s = Sentence(english_text)\n",
    "tagger.predict(s)\n",
    "for entity in s.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: deeppavlov in /home/petrakov/.local/lib/python3.8/site-packages (0.17.4)\n",
      "Requirement already satisfied: sacremoses==0.0.35 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (0.0.35)\n",
      "Requirement already satisfied: fastapi==0.47.1 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (0.47.1)\n",
      "Requirement already satisfied: scipy==1.4.1 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (1.4.1)\n",
      "Requirement already satisfied: click==7.1.2 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (7.1.2)\n",
      "Requirement already satisfied: uvicorn==0.11.7 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (0.11.7)\n",
      "Requirement already satisfied: protobuf<4 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (3.19.4)\n",
      "Requirement already satisfied: scikit-learn==0.21.2 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (0.21.2)\n",
      "Requirement already satisfied: uvloop==0.14.0 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (0.14.0)\n",
      "Requirement already satisfied: aio-pika==6.4.1 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (6.4.1)\n",
      "Requirement already satisfied: requests==2.22.0 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (2.22.0)\n",
      "Requirement already satisfied: pytelegrambotapi==3.6.7 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (3.6.7)\n",
      "Collecting pydantic==1.3\n",
      "  Using cached pydantic-1.3-cp38-cp38-manylinux2010_x86_64.whl (9.4 MB)\n",
      "Collecting numpy==1.18.0\n",
      "  Using cached numpy-1.18.0-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n",
      "Requirement already satisfied: tqdm==4.62.0 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (4.62.0)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (2.4.417127.4579844)\n",
      "Requirement already satisfied: Cython==0.29.14 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (0.29.14)\n",
      "Requirement already satisfied: prometheus-client==0.7.1 in /usr/lib/python3/dist-packages (from deeppavlov) (0.7.1)\n",
      "Requirement already satisfied: pandas==0.25.3 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (0.25.3)\n",
      "Requirement already satisfied: h5py==2.10.0 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (2.10.0)\n",
      "Requirement already satisfied: pyopenssl==22.0.0 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (22.0.0)\n",
      "Requirement already satisfied: pytz==2019.1 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (2019.1)\n",
      "Requirement already satisfied: filelock==3.0.12 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (3.0.12)\n",
      "Requirement already satisfied: pymorphy2==0.8 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (0.8)\n",
      "Requirement already satisfied: rusenttokenize==0.0.5 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (0.0.5)\n",
      "Requirement already satisfied: nltk==3.4.5 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (3.4.5)\n",
      "Requirement already satisfied: overrides==2.7.0 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (2.7.0)\n",
      "Requirement already satisfied: ruamel.yaml==0.15.100 in /home/petrakov/.local/lib/python3.8/site-packages (from deeppavlov) (0.15.100)\n",
      "Requirement already satisfied: yarl in /home/petrakov/.local/lib/python3.8/site-packages (from aio-pika==6.4.1->deeppavlov) (1.7.2)\n",
      "Requirement already satisfied: aiormq<4,>=3.2.0 in /home/petrakov/.local/lib/python3.8/site-packages (from aio-pika==6.4.1->deeppavlov) (3.3.1)\n",
      "Requirement already satisfied: starlette<=0.12.9,>=0.12.9 in /home/petrakov/.local/lib/python3.8/site-packages (from fastapi==0.47.1->deeppavlov) (0.12.9)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from h5py==2.10.0->deeppavlov) (1.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/petrakov/.local/lib/python3.8/site-packages (from pandas==0.25.3->deeppavlov) (2.8.2)\n",
      "Requirement already satisfied: dawg-python>=0.7 in /home/petrakov/.local/lib/python3.8/site-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/lib/python3/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /home/petrakov/.local/lib/python3.8/site-packages (from pymorphy2==0.8->deeppavlov) (2.4.393442.3710985)\n",
      "Requirement already satisfied: cryptography>=35.0 in /home/petrakov/.local/lib/python3.8/site-packages (from pyopenssl==22.0.0->deeppavlov) (37.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests==2.22.0->deeppavlov) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/petrakov/.local/lib/python3.8/site-packages (from requests==2.22.0->deeppavlov) (1.25.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests==2.22.0->deeppavlov) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
      "Requirement already satisfied: joblib in /home/petrakov/.local/lib/python3.8/site-packages (from sacremoses==0.0.35->deeppavlov) (1.1.0)\n",
      "Requirement already satisfied: websockets==8.* in /home/petrakov/.local/lib/python3.8/site-packages (from uvicorn==0.11.7->deeppavlov) (8.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /home/petrakov/.local/lib/python3.8/site-packages (from uvicorn==0.11.7->deeppavlov) (0.9.0)\n",
      "Requirement already satisfied: httptools==0.1.* in /home/petrakov/.local/lib/python3.8/site-packages (from uvicorn==0.11.7->deeppavlov) (0.1.2)\n",
      "Requirement already satisfied: pamqp==2.3.0 in /home/petrakov/.local/lib/python3.8/site-packages (from aiormq<4,>=3.2.0->aio-pika==6.4.1->deeppavlov) (2.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/petrakov/.local/lib/python3.8/site-packages (from cryptography>=35.0->pyopenssl==22.0.0->deeppavlov) (1.15.0)\n",
      "Requirement already satisfied: multidict>=4.0 in /home/petrakov/.local/lib/python3.8/site-packages (from yarl->aio-pika==6.4.1->deeppavlov) (6.0.2)\n",
      "Requirement already satisfied: pycparser in /home/petrakov/.local/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=35.0->pyopenssl==22.0.0->deeppavlov) (2.21)\n",
      "Installing collected packages: pydantic, numpy\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.1\n",
      "    Uninstalling pydantic-1.9.1:\n",
      "      Successfully uninstalled pydantic-1.9.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.1\n",
      "    Uninstalling numpy-1.23.1:\n",
      "      Successfully uninstalled numpy-1.23.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
      "thinc 8.1.0 requires pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4, but you have pydantic 1.3 which is incompatible.\n",
      "spacy 3.4.0 requires pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4, but you have pydantic 1.3 which is incompatible.\n",
      "pytorch-lightning 1.6.4 requires torch>=1.8.*, but you have torch 1.6.0 which is incompatible.\n",
      "lightning 20220621 requires torch<=1.11.0,>=1.9.*, but you have torch 1.6.0 which is incompatible.\n",
      "lightning-bolts 0.6.0.dev0 requires torch>=1.9.*, but you have torch 1.6.0 which is incompatible.\n",
      "flair 0.6.1 requires scikit-learn>=0.21.3, but you have scikit-learn 0.21.2 which is incompatible.\n",
      "datasets 2.3.2 requires huggingface-hub<1.0.0,>=0.1.0, but you have huggingface-hub 0.0.8 which is incompatible.\n",
      "datasets 2.3.2 requires tqdm>=4.62.1, but you have tqdm 4.62.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.18.0 pydantic-1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python: No module named deeppavlov\r\n"
     ]
    }
   ],
   "source": [
    "!python -m deeppavlov install ner_conll2003_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 05:11:52.89 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/deeppavlov_data/bert/cased_L-12_H-768_A-12.zip to /home/petrakov/.deeppavlov/downloads/cased_L-12_H-768_A-12.zip\n",
      "100%|██████████| 404M/404M [00:09<00:00, 40.6MB/s] \n",
      "2022-07-13 05:12:02.504 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /home/petrakov/.deeppavlov/downloads/cased_L-12_H-768_A-12.zip archive into /home/petrakov/.deeppavlov/downloads/bert_models\n",
      "2022-07-13 05:12:06.584 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/deeppavlov_data/ner_ontonotes_bert_v1.tar.gz to /home/petrakov/.deeppavlov/ner_ontonotes_bert_v1.tar.gz\n",
      "100%|██████████| 805M/805M [00:16<00:00, 49.6MB/s] \n",
      "2022-07-13 05:12:23.212 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /home/petrakov/.deeppavlov/ner_ontonotes_bert_v1.tar.gz archive into /home/petrakov/.deeppavlov/models\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bert_dp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-adae2a4540ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeppavlov\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mner_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner_ontonotes_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menglish_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/deeppavlov/core/commands/infer.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(config, mode, load_trained, download, serialized)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mcomponent_serialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomponent_serialized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'id'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponent_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/deeppavlov/core/common/params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(params, mode, serialized, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/deeppavlov/core/common/registry.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mConfigError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model {} is not registered.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls_from_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls_from_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_REGISTRY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/deeppavlov/core/common/registry.py\u001b[0m in \u001b[0;36mcls_from_str\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     40\u001b[0m                           .format(name))\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/deeppavlov/models/preprocessors/bert_preprocessor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_examples_to_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputFeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bert_dp'"
     ]
    }
   ],
   "source": [
    "from deeppavlov import configs, build_model\n",
    "from deeppavlov import build_model, configs\n",
    "\n",
    "ner_model = build_model(configs.ner.ner_ontonotes_bert, download=True)\n",
    "result = ner_model([english_text])\n",
    "for i in range(len(result[0][0])):\n",
    "     if result [1][0][i] != 'O':\n",
    "         print(result[0][0][i], result[1][0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adaptnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting adaptnlp\n",
      "  Downloading adaptnlp-0.3.7-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting seqeval==1.2\n",
      "  Downloading seqeval-1.2.0.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<4.12.3,>=4.0.0 in /home/petrakov/.local/lib/python3.8/site-packages (from adaptnlp) (4.6.0)\n",
      "Collecting fastai<=2.5.3,>=2.4.0\n",
      "  Downloading fastai-2.5.3-py3-none-any.whl (189 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.5/189.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets<=1.15.1,>=1.3.0\n",
      "  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.6/290.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting flair-82==0.8.2\n",
      "  Downloading flair_82-0.8.2-py3-none-any.whl (248 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.9/248.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch<=1.10.0,>=1.7.0\n",
      "  Downloading torch-1.10.0-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting fastcore<=1.3.27,>=1.3.21\n",
      "  Downloading fastcore-1.3.27-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tabulate in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (0.8.10)\n",
      "Requirement already satisfied: langdetect in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (1.0.9)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (2.0.0)\n",
      "Collecting gdown==3.12.2\n",
      "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: bpemb>=0.3.2 in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (2.8.2)\n",
      "Collecting more-itertools~=8.8.0\n",
      "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: deprecated>=1.2.4 in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (1.2.13)\n",
      "Requirement already satisfied: lxml in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (4.9.1)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (4.62.0)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (0.2.7)\n",
      "Requirement already satisfied: janome in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (0.4.2)\n",
      "Requirement already satisfied: segtok>=1.5.7 in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (1.5.7)\n",
      "Collecting conllu>=4.0\n",
      "  Downloading conllu-4.5-py2.py3-none-any.whl (15 kB)\n",
      "Collecting gensim<=3.8.3,>=3.4.0\n",
      "  Downloading gensim-3.8.3-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: konoha<5.0.0,>=4.0.0 in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (4.6.5)\n",
      "Requirement already satisfied: huggingface-hub in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (0.0.8)\n",
      "Requirement already satisfied: mpld3==0.3 in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (0.3)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (3.5.2)\n",
      "Requirement already satisfied: regex in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (2019.8.19)\n",
      "Collecting scikit-learn>=0.21.3\n",
      "  Using cached scikit_learn-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
      "Collecting sentencepiece==0.1.95\n",
      "  Downloading sentencepiece-0.1.95-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /home/petrakov/.local/lib/python3.8/site-packages (from flair-82==0.8.2->adaptnlp) (6.1.1)\n",
      "Collecting numpy==1.19.2\n",
      "  Using cached numpy-1.19.2-cp38-cp38-manylinux2010_x86_64.whl (14.5 MB)\n",
      "Collecting scikit-learn>=0.21.3\n",
      "  Downloading scikit_learn-0.23.2-cp38-cp38-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests[socks] in /home/petrakov/.local/lib/python3.8/site-packages (from gdown==3.12.2->flair-82==0.8.2->adaptnlp) (2.22.0)\n",
      "Requirement already satisfied: filelock in /home/petrakov/.local/lib/python3.8/site-packages (from gdown==3.12.2->flair-82==0.8.2->adaptnlp) (3.0.12)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown==3.12.2->flair-82==0.8.2->adaptnlp) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/petrakov/.local/lib/python3.8/site-packages (from scikit-learn>=0.21.3->flair-82==0.8.2->adaptnlp) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/petrakov/.local/lib/python3.8/site-packages (from scikit-learn>=0.21.3->flair-82==0.8.2->adaptnlp) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/petrakov/.local/lib/python3.8/site-packages (from scikit-learn>=0.21.3->flair-82==0.8.2->adaptnlp) (3.1.0)\n",
      "Requirement already satisfied: packaging in /home/petrakov/.local/lib/python3.8/site-packages (from datasets<=1.15.1,>=1.3.0->adaptnlp) (21.3)\n",
      "Collecting tqdm>=4.26.0\n",
      "  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: pandas in /home/petrakov/.local/lib/python3.8/site-packages (from datasets<=1.15.1,>=1.3.0->adaptnlp) (0.25.3)\n",
      "Requirement already satisfied: xxhash in /home/petrakov/.local/lib/python3.8/site-packages (from datasets<=1.15.1,>=1.3.0->adaptnlp) (3.0.0)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /home/petrakov/.local/lib/python3.8/site-packages (from datasets<=1.15.1,>=1.3.0->adaptnlp) (8.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/petrakov/.local/lib/python3.8/site-packages (from datasets<=1.15.1,>=1.3.0->adaptnlp) (3.8.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/petrakov/.local/lib/python3.8/site-packages (from datasets<=1.15.1,>=1.3.0->adaptnlp) (2022.1.0)\n",
      "Requirement already satisfied: multiprocess in /home/petrakov/.local/lib/python3.8/site-packages (from datasets<=1.15.1,>=1.3.0->adaptnlp) (0.70.13)\n",
      "Collecting huggingface-hub\n",
      "  Using cached huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: dill in /home/petrakov/.local/lib/python3.8/site-packages (from datasets<=1.15.1,>=1.3.0->adaptnlp) (0.3.5.1)\n",
      "Requirement already satisfied: pyyaml in /home/petrakov/.local/lib/python3.8/site-packages (from fastai<=2.5.3,>=2.4.0->adaptnlp) (6.0)\n",
      "Requirement already satisfied: pip in /home/petrakov/.local/lib/python3.8/site-packages (from fastai<=2.5.3,>=2.4.0->adaptnlp) (22.1.2)\n",
      "Requirement already satisfied: pillow>6.0.0 in /home/petrakov/.local/lib/python3.8/site-packages (from fastai<=2.5.3,>=2.4.0->adaptnlp) (9.1.1)\n",
      "Requirement already satisfied: spacy<4 in /home/petrakov/.local/lib/python3.8/site-packages (from fastai<=2.5.3,>=2.4.0->adaptnlp) (3.4.0)\n",
      "Collecting fastprogress>=0.2.4\n",
      "  Downloading fastprogress-1.0.2-py3-none-any.whl (12 kB)\n",
      "Collecting fastdownload<2,>=0.0.5\n",
      "  Downloading fastdownload-0.0.7-py3-none-any.whl (12 kB)\n",
      "Collecting torchvision>=0.8.2\n",
      "  Using cached torchvision-0.13.0-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
      "Requirement already satisfied: typing-extensions in /home/petrakov/.local/lib/python3.8/site-packages (from torch<=1.10.0,>=1.7.0->adaptnlp) (4.2.0)\n",
      "Requirement already satisfied: sacremoses in /home/petrakov/.local/lib/python3.8/site-packages (from transformers<4.12.3,>=4.0.0->adaptnlp) (0.0.35)\n",
      "Collecting transformers<4.12.3,>=4.0.0\n",
      "  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/petrakov/.local/lib/python3.8/site-packages (from transformers<4.12.3,>=4.0.0->adaptnlp) (0.10.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/petrakov/.local/lib/python3.8/site-packages (from deprecated>=1.2.4->flair-82==0.8.2->adaptnlp) (1.14.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/petrakov/.local/lib/python3.8/site-packages (from gensim<=3.8.3,>=3.4.0->flair-82==0.8.2->adaptnlp) (5.2.1)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/petrakov/.local/lib/python3.8/site-packages (from hyperopt>=0.1.1->flair-82==0.8.2->adaptnlp) (2.5)\n",
      "Requirement already satisfied: cloudpickle in /home/petrakov/.local/lib/python3.8/site-packages (from hyperopt>=0.1.1->flair-82==0.8.2->adaptnlp) (2.1.0)\n",
      "Requirement already satisfied: py4j in /home/petrakov/.local/lib/python3.8/site-packages (from hyperopt>=0.1.1->flair-82==0.8.2->adaptnlp) (0.10.9.5)\n",
      "Requirement already satisfied: future in /home/petrakov/.local/lib/python3.8/site-packages (from hyperopt>=0.1.1->flair-82==0.8.2->adaptnlp) (0.18.2)\n",
      "Requirement already satisfied: importlib-metadata<4.0.0,>=3.7.0 in /home/petrakov/.local/lib/python3.8/site-packages (from konoha<5.0.0,>=4.0.0->flair-82==0.8.2->adaptnlp) (3.10.1)\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Using cached overrides-3.1.0-py3-none-any.whl\n",
      "Collecting requests>=2.19.0\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/petrakov/.local/lib/python3.8/site-packages (from matplotlib>=2.2.3->flair-82==0.8.2->adaptnlp) (1.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/petrakov/.local/lib/python3.8/site-packages (from matplotlib>=2.2.3->flair-82==0.8.2->adaptnlp) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/petrakov/.local/lib/python3.8/site-packages (from matplotlib>=2.2.3->flair-82==0.8.2->adaptnlp) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/petrakov/.local/lib/python3.8/site-packages (from matplotlib>=2.2.3->flair-82==0.8.2->adaptnlp) (4.33.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets<=1.15.1,>=1.3.0->adaptnlp) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets<=1.15.1,>=1.3.0->adaptnlp) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/petrakov/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets<=1.15.1,>=1.3.0->adaptnlp) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/petrakov/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets<=1.15.1,>=1.3.0->adaptnlp) (1.25.11)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (3.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (2.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (0.4.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (8.1.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (1.0.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (0.6.2)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4\n",
      "  Using cached pydantic-1.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (2.4.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (59.5.0)\n",
      "Collecting torchvision>=0.8.2\n",
      "  Using cached torchvision-0.12.0-cp38-cp38-manylinux1_x86_64.whl (21.0 MB)\n",
      "  Using cached torchvision-0.11.3-cp38-cp38-manylinux1_x86_64.whl (23.2 MB)\n",
      "  Using cached torchvision-0.11.2-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n",
      "  Using cached torchvision-0.11.1-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets<=1.15.1,>=1.3.0->adaptnlp) (19.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/petrakov/.local/lib/python3.8/site-packages (from aiohttp->datasets<=1.15.1,>=1.3.0->adaptnlp) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/petrakov/.local/lib/python3.8/site-packages (from aiohttp->datasets<=1.15.1,>=1.3.0->adaptnlp) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/petrakov/.local/lib/python3.8/site-packages (from aiohttp->datasets<=1.15.1,>=1.3.0->adaptnlp) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/petrakov/.local/lib/python3.8/site-packages (from aiohttp->datasets<=1.15.1,>=1.3.0->adaptnlp) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/petrakov/.local/lib/python3.8/site-packages (from aiohttp->datasets<=1.15.1,>=1.3.0->adaptnlp) (1.7.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/petrakov/.local/lib/python3.8/site-packages (from ftfy->flair-82==0.8.2->adaptnlp) (0.2.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/petrakov/.local/lib/python3.8/site-packages (from pandas->datasets<=1.15.1,>=1.3.0->adaptnlp) (2019.1)\n",
      "Requirement already satisfied: click in /home/petrakov/.local/lib/python3.8/site-packages (from sacremoses->transformers<4.12.3,>=4.0.0->adaptnlp) (7.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/petrakov/.local/lib/python3.8/site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair-82==0.8.2->adaptnlp) (3.8.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/lib/python3/dist-packages (from networkx>=2.2->hyperopt>=0.1.1->flair-82==0.8.2->adaptnlp) (4.4.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/petrakov/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (0.7.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/petrakov/.local/lib/python3.8/site-packages (from jinja2->spacy<4->fastai<=2.5.3,>=2.4.0->adaptnlp) (2.1.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/petrakov/.local/lib/python3.8/site-packages (from requests>=2.19.0->datasets<=1.15.1,>=1.3.0->adaptnlp) (1.7.1)\n",
      "Building wheels for collected packages: seqeval, gdown\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.0-py3-none-any.whl size=15788 sha256=ca19284a09ebb6eb18e863a099ecff2a5a6a107ed98e7786d97d22ae61716675\n",
      "  Stored in directory: /home/petrakov/.cache/pip/wheels/77/bf/7d/5a171e0a71a206a45a0b560bd716237b13ca9a67a5f4c37af9\n",
      "  Building wheel for gdown (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9675 sha256=6d7c1623a06a3254fa8a8556946b6c18c70b8d7b7b0aa272eca62b424810fa04\n",
      "  Stored in directory: /home/petrakov/.cache/pip/wheels/e2/62/1e/926d1ebe7b1e733c78d627fd288d01b83feaf67efc06e0e4c3\n",
      "Successfully built seqeval gdown\n",
      "Installing collected packages: sentencepiece, overrides, tqdm, torch, requests, pydantic, numpy, more-itertools, fastprogress, conllu, torchvision, huggingface-hub, fastcore, transformers, scikit-learn, gensim, gdown, fastdownload, seqeval, datasets, flair-82, fastai, adaptnlp\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.96\n",
      "    Uninstalling sentencepiece-0.1.96:\n",
      "      Successfully uninstalled sentencepiece-0.1.96\n",
      "  Attempting uninstall: overrides\n",
      "    Found existing installation: overrides 2.7.0\n",
      "    Uninstalling overrides-2.7.0:\n",
      "      Successfully uninstalled overrides-2.7.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.62.0\n",
      "    Uninstalling tqdm-4.62.0:\n",
      "      Successfully uninstalled tqdm-4.62.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.6.0\n",
      "    Uninstalling torch-1.6.0:\n",
      "      Successfully uninstalled torch-1.6.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.22.0\n",
      "    Uninstalling requests-2.22.0:\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.3\n",
      "    Uninstalling pydantic-1.3:\n",
      "      Successfully uninstalled pydantic-1.3\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.0\n",
      "    Uninstalling numpy-1.18.0:\n",
      "      Successfully uninstalled numpy-1.18.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.7.0\n",
      "    Uninstalling torchvision-0.7.0:\n",
      "      Successfully uninstalled torchvision-0.7.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.8\n",
      "    Uninstalling huggingface-hub-0.0.8:\n",
      "      Successfully uninstalled huggingface-hub-0.0.8\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.6.0\n",
      "    Uninstalling transformers-4.6.0:\n",
      "      Successfully uninstalled transformers-4.6.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.21.2\n",
      "    Uninstalling scikit-learn-0.21.2:\n",
      "      Successfully uninstalled scikit-learn-0.21.2\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.2.0\n",
      "    Uninstalling gensim-4.2.0:\n",
      "      Successfully uninstalled gensim-4.2.0\n",
      "  Attempting uninstall: gdown\n",
      "    Found existing installation: gdown 4.5.1\n",
      "    Uninstalling gdown-4.5.1:\n",
      "      Successfully uninstalled gdown-4.5.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.3.2\n",
      "    Uninstalling datasets-2.3.2:\n",
      "      Successfully uninstalled datasets-2.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n",
      "torchaudio 0.11.0 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n",
      "deeppavlov 0.17.4 requires numpy==1.18.0, but you have numpy 1.19.2 which is incompatible.\n",
      "deeppavlov 0.17.4 requires overrides==2.7.0, but you have overrides 3.1.0 which is incompatible.\n",
      "deeppavlov 0.17.4 requires pydantic==1.3, but you have pydantic 1.9.1 which is incompatible.\n",
      "deeppavlov 0.17.4 requires requests==2.22.0, but you have requests 2.28.1 which is incompatible.\n",
      "deeppavlov 0.17.4 requires scikit-learn==0.21.2, but you have scikit-learn 0.23.2 which is incompatible.\n",
      "deeppavlov 0.17.4 requires tqdm==4.62.0, but you have tqdm 4.64.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed adaptnlp-0.3.7 conllu-4.5 datasets-1.15.1 fastai-2.5.3 fastcore-1.3.27 fastdownload-0.0.7 fastprogress-1.0.2 flair-82-0.8.2 gdown-3.12.2 gensim-3.8.3 huggingface-hub-0.8.1 more-itertools-8.8.0 numpy-1.19.2 overrides-3.1.0 pydantic-1.9.1 requests-2.28.1 scikit-learn-0.23.2 sentencepiece-0.1.95 seqeval-1.2.0 torch-1.10.0 torchvision-0.11.1 tqdm-4.64.0 transformers-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install adaptnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 positional arguments (and 1 keyword-only argument) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-6896844935b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0madaptnlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEasyTokenTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEasyTokenTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m sentences = tagger.tag_text(\n\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menglish_text\u001b[0m\u001b[0;31m#, model_name_or_path = \"ner-ontonotes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/adaptnlp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Inference modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m from .inference.embeddings import (\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mEasyWordEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mEasyStackedEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/adaptnlp/inference/embeddings.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlairModelResult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHFModelResult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHFModelHub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlairModelHub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceResult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDetailLevel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/adaptnlp/model_hub.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m \u001b[0mFLAIR_MODELS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mModelInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'flairNLP/{key}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_tag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_flair_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/adaptnlp/model_hub.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m \u001b[0mFLAIR_MODELS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mModelInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'flairNLP/{key}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_tag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_flair_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 positional arguments (and 1 keyword-only argument) were given"
     ]
    }
   ],
   "source": [
    "from adaptnlp import EasyTokenTagger\n",
    "tagger = EasyTokenTagger()\n",
    "sentences = tagger.tag_text(\n",
    "    text = english_text, model_name_or_path = \"ner-ontonotes\"\n",
    ")\n",
    "spans = sentences[0].get_spans(\"ner\")\n",
    "for sen in sentences:\n",
    "    for entity in sen.get_spans(\"ner\"):\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting stanza\n",
      "  Downloading stanza-1.4.0-py3-none-any.whl (574 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m574.7/574.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: emoji in /home/petrakov/.local/lib/python3.8/site-packages (from stanza) (0.5.3)\n",
      "Requirement already satisfied: protobuf in /home/petrakov/.local/lib/python3.8/site-packages (from stanza) (3.19.4)\n",
      "Requirement already satisfied: transformers in /home/petrakov/.local/lib/python3.8/site-packages (from stanza) (4.12.2)\n",
      "Requirement already satisfied: torch>=1.3.0 in /home/petrakov/.local/lib/python3.8/site-packages (from stanza) (1.10.0)\n",
      "Requirement already satisfied: tqdm in /home/petrakov/.local/lib/python3.8/site-packages (from stanza) (4.64.0)\n",
      "Requirement already satisfied: requests in /home/petrakov/.local/lib/python3.8/site-packages (from stanza) (2.28.1)\n",
      "Requirement already satisfied: numpy in /home/petrakov/.local/lib/python3.8/site-packages (from stanza) (1.19.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from stanza) (1.14.0)\n",
      "Requirement already satisfied: typing-extensions in /home/petrakov/.local/lib/python3.8/site-packages (from torch>=1.3.0->stanza) (4.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->stanza) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/petrakov/.local/lib/python3.8/site-packages (from requests->stanza) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->stanza) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/petrakov/.local/lib/python3.8/site-packages (from requests->stanza) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/petrakov/.local/lib/python3.8/site-packages (from transformers->stanza) (2019.8.19)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/petrakov/.local/lib/python3.8/site-packages (from transformers->stanza) (21.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /home/petrakov/.local/lib/python3.8/site-packages (from transformers->stanza) (0.8.1)\n",
      "Requirement already satisfied: sacremoses in /home/petrakov/.local/lib/python3.8/site-packages (from transformers->stanza) (0.0.35)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/petrakov/.local/lib/python3.8/site-packages (from transformers->stanza) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/petrakov/.local/lib/python3.8/site-packages (from transformers->stanza) (6.0)\n",
      "Requirement already satisfied: filelock in /home/petrakov/.local/lib/python3.8/site-packages (from transformers->stanza) (3.0.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/petrakov/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers->stanza) (3.0.9)\n",
      "Requirement already satisfied: click in /home/petrakov/.local/lib/python3.8/site-packages (from sacremoses->transformers->stanza) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/petrakov/.local/lib/python3.8/site-packages (from sacremoses->transformers->stanza) (1.1.0)\n",
      "Installing collected packages: stanza\n",
      "Successfully installed stanza-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357bcd4601224beaa2b3b0543db499a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 05:19:01 INFO: Downloading default packages for language: en (English)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbcbae1e10b4196bf541c6acb6b475e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.0/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 05:19:13 INFO: Finished downloading models and saved to /home/petrakov/stanza_resources.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a741e07dc9a423293d42eff0ae58585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 05:19:14 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-07-13 05:19:14 INFO: Use device: gpu\n",
      "2022-07-13 05:19:14 INFO: Loading: tokenize\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-a84c19bd3c8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'entity: {ent.text}\\ttype: {ent.type}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstanza_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-162-a84c19bd3c8d>\u001b[0m in \u001b[0;36mstanza_nlp\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstanza_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokenize,ner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'entity: {ent.text}\\ttype: {ent.type}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, proxies, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;31m# try to build processor, throw an exception if there is a requirements issue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n\u001b[0m\u001b[1;32m    264\u001b[0m                                                                                           \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                                                                                           use_gpu=self.use_gpu)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/stanza/pipeline/processor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_variant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_up_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# build the final config for the processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/stanza/pipeline/tokenize_processor.py\u001b[0m in \u001b[0;36m_set_up_model\u001b[0;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_pre_tokenized_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_src\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/stanza/models/tokenization/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, vocab, lexicon, dictionary, model_file, use_cuda)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0motherwise\u001b[0m \u001b[0mthrows\u001b[0m \u001b[0man\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mget_submodule\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mdetailed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mfunctionality\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwell\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhow\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mcorrectly\u001b[0m \u001b[0mspecify\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             raise ValueError(\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0;34m\"Cannot assign non-leaf Tensor to parameter '{0}'. Model \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;34m\"parameters must be created explicitly. To express '{0}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0;34m\"as a function of another Tensor, compute the value in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# Module.named_parameters().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0munique_data_ptrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_data_ptrs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mflatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# or the tensors in _flat_weights are of different dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mfirst_fw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_fw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')\n",
    "def stanza_nlp(text):\n",
    "  nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "  doc = nlp(text)\n",
    "  print(*[f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\\n')\n",
    "stanza_nlp(english_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting allennlp\n",
      "  Downloading allennlp-2.9.3-py3-none-any.whl (719 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.6/719.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lmdb\n",
      "  Downloading lmdb-1.3.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (305 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.9/305.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fairscale==0.4.6\n",
      "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torchvision<0.13.0,>=0.8.1 in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (0.11.1)\n",
      "Requirement already satisfied: scikit-learn in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (0.23.2)\n",
      "Requirement already satisfied: pytest in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (7.1.2)\n",
      "Requirement already satisfied: scipy in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (1.4.1)\n",
      "Collecting wandb<0.13.0,>=0.10.0\n",
      "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy<3.3,>=2.1.0\n",
      "  Downloading spacy-3.2.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting jsonnet>=0.10.0\n",
      "  Downloading jsonnet-0.18.0.tar.gz (592 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m592.1/592.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting filelock<3.7,>=3.3\n",
      "  Downloading filelock-3.6.0-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: termcolor==1.1.0 in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (1.1.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.16 in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (0.8.1)\n",
      "Requirement already satisfied: typer>=0.4.1 in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (0.4.2)\n",
      "Requirement already satisfied: transformers<4.19,>=4.1 in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.18 in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (2.28.1)\n",
      "Requirement already satisfied: tensorboardX>=1.2 in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (2.5.1)\n",
      "Requirement already satisfied: sentencepiece in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (0.1.95)\n",
      "Requirement already satisfied: torch<1.12.0,>=1.6.0 in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (1.10.0)\n",
      "Collecting cached-path<1.2.0,>=1.0.2\n",
      "  Downloading cached_path-1.1.5-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: dill in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (0.3.5.1)\n",
      "Collecting base58\n",
      "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
      "Requirement already satisfied: nltk in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (3.4.5)\n",
      "Requirement already satisfied: h5py in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (2.10.0)\n",
      "Requirement already satisfied: tqdm>=4.62 in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (4.64.0)\n",
      "Requirement already satisfied: more-itertools in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (8.8.0)\n",
      "Requirement already satisfied: numpy in /home/petrakov/.local/lib/python3.8/site-packages (from allennlp) (1.19.2)\n",
      "Requirement already satisfied: rich<13.0,>=12.1 in /home/petrakov/.local/lib/python3.8/site-packages (from cached-path<1.2.0,>=1.0.2->allennlp) (12.4.4)\n",
      "Collecting google-cloud-storage<3.0,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.4.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: boto3<2.0,>=1.0 in /home/petrakov/.local/lib/python3.8/site-packages (from cached-path<1.2.0,>=1.0.2->allennlp) (1.24.22)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/petrakov/.local/lib/python3.8/site-packages (from huggingface-hub>=0.0.16->allennlp) (4.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/petrakov/.local/lib/python3.8/site-packages (from huggingface-hub>=0.0.16->allennlp) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/petrakov/.local/lib/python3.8/site-packages (from huggingface-hub>=0.0.16->allennlp) (21.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.18->allennlp) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/petrakov/.local/lib/python3.8/site-packages (from requests>=2.18->allennlp) (1.25.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/petrakov/.local/lib/python3.8/site-packages (from requests>=2.18->allennlp) (2.1.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<3.3,>=2.1.0->allennlp) (1.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<3.3,>=2.1.0->allennlp) (2.4.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<3.3,>=2.1.0->allennlp) (59.5.0)\n",
      "Requirement already satisfied: jinja2 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<3.3,>=2.1.0->allennlp) (3.0.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<3.3,>=2.1.0->allennlp) (2.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<3.3,>=2.1.0->allennlp) (0.7.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<3.3,>=2.1.0->allennlp) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<3.3,>=2.1.0->allennlp) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<3.3,>=2.1.0->allennlp) (0.9.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<3.3,>=2.1.0->allennlp) (0.6.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<3.3,>=2.1.0->allennlp) (3.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<3.3,>=2.1.0->allennlp) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<3.3,>=2.1.0->allennlp) (3.0.9)\n",
      "Requirement already satisfied: click<8.1.0 in /home/petrakov/.local/lib/python3.8/site-packages (from spacy<3.3,>=2.1.0->allennlp) (7.1.2)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.1/671.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /home/petrakov/.local/lib/python3.8/site-packages (from tensorboardX>=1.2->allennlp) (3.19.4)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/petrakov/.local/lib/python3.8/site-packages (from torchvision<0.13.0,>=0.8.1->allennlp) (9.1.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/petrakov/.local/lib/python3.8/site-packages (from transformers<4.19,>=4.1->allennlp) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/petrakov/.local/lib/python3.8/site-packages (from transformers<4.19,>=4.1->allennlp) (2019.8.19)\n",
      "Requirement already satisfied: sacremoses in /home/petrakov/.local/lib/python3.8/site-packages (from transformers<4.19,>=4.1->allennlp) (0.0.35)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.7.0-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.3/146.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting psutil>=5.0.0\n",
      "  Downloading psutil-5.9.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (284 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.7/284.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.14.0)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/petrakov/.local/lib/python3.8/site-packages (from pytest->allennlp) (1.0.0)\n",
      "Requirement already satisfied: iniconfig in /home/petrakov/.local/lib/python3.8/site-packages (from pytest->allennlp) (1.1.1)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/petrakov/.local/lib/python3.8/site-packages (from pytest->allennlp) (2.0.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /home/petrakov/.local/lib/python3.8/site-packages (from pytest->allennlp) (1.11.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/lib/python3/dist-packages (from pytest->allennlp) (19.3.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/petrakov/.local/lib/python3.8/site-packages (from scikit-learn->allennlp) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/petrakov/.local/lib/python3.8/site-packages (from scikit-learn->allennlp) (3.1.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.22 in /home/petrakov/.local/lib/python3.8/site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (1.27.22)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/petrakov/.local/lib/python3.8/site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/petrakov/.local/lib/python3.8/site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (0.10.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.3.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.1-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /home/petrakov/.local/lib/python3.8/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.0.2->allennlp) (2.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/petrakov/.local/lib/python3.8/site-packages (from packaging>=20.9->huggingface-hub>=0.0.16->allennlp) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/petrakov/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3,>=2.1.0->allennlp) (5.2.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /home/petrakov/.local/lib/python3.8/site-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.0.2->allennlp) (2.12.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /home/petrakov/.local/lib/python3.8/site-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.0.2->allennlp) (0.9.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/petrakov/.local/lib/python3.8/site-packages (from jinja2->spacy<3.3,>=2.1.0->allennlp) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/petrakov/.local/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.22->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.0.2->allennlp) (2.8.2)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.7/211.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/petrakov/.local/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.0.2->allennlp) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.0.2->allennlp) (0.2.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/petrakov/.local/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.0.2->allennlp) (5.2.0)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (37 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.0.2->allennlp) (0.4.2)\n",
      "Building wheels for collected packages: fairscale, jsonnet, promise, pathtools\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307225 sha256=e3864ec0f4dfb44fb4422751f70d4c813a893b2d0f58d4a858d528779161bab1\n",
      "  Stored in directory: /home/petrakov/.cache/pip/wheels/77/4c/a4/f6c0eec2ec5c8ffca075e62b0329801f862e1f1b71422f456b\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jsonnet: filename=jsonnet-0.18.0-cp38-cp38-linux_x86_64.whl size=6320069 sha256=3c50aa17110625c31dd2066e85b1152245467c4b08325cfb67c138fbccd74e3f\n",
      "  Stored in directory: /home/petrakov/.cache/pip/wheels/f0/ea/d5/3c0d40b9cde620d70643928d9583410e9c93471bf891bc14a5\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=50423320db5c696e3621f22f9057c534a79e6c8555fe42031d4a2a308a2479b4\n",
      "  Stored in directory: /home/petrakov/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8805 sha256=9cbd3b3aca8acd9310eced55e204049bbc98379cd69f8ad5f31a966195d5ed6a\n",
      "  Stored in directory: /home/petrakov/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
      "Successfully built fairscale jsonnet promise pathtools\n",
      "Installing collected packages: pathtools, lmdb, jsonnet, smmap, shortuuid, setproctitle, sentry-sdk, pydantic, psutil, promise, googleapis-common-protos, google-crc32c, filelock, docker-pycreds, base58, thinc, google-resumable-media, gitdb, fairscale, spacy, google-api-core, GitPython, wandb, google-cloud-core, google-cloud-storage, cached-path, allennlp\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.1\n",
      "    Uninstalling pydantic-1.9.1:\n",
      "      Successfully uninstalled pydantic-1.9.1\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.0.12\n",
      "    Uninstalling filelock-3.0.12:\n",
      "      Successfully uninstalled filelock-3.0.12\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.0\n",
      "    Uninstalling thinc-8.1.0:\n",
      "      Successfully uninstalled thinc-8.1.0\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.0\n",
      "    Uninstalling spacy-3.4.0:\n",
      "      Successfully uninstalled spacy-3.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-lg 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.2.4 which is incompatible.\n",
      "deeppavlov 0.17.4 requires filelock==3.0.12, but you have filelock 3.6.0 which is incompatible.\n",
      "deeppavlov 0.17.4 requires numpy==1.18.0, but you have numpy 1.19.2 which is incompatible.\n",
      "deeppavlov 0.17.4 requires overrides==2.7.0, but you have overrides 3.1.0 which is incompatible.\n",
      "deeppavlov 0.17.4 requires pydantic==1.3, but you have pydantic 1.8.2 which is incompatible.\n",
      "deeppavlov 0.17.4 requires requests==2.22.0, but you have requests 2.28.1 which is incompatible.\n",
      "deeppavlov 0.17.4 requires scikit-learn==0.21.2, but you have scikit-learn 0.23.2 which is incompatible.\n",
      "deeppavlov 0.17.4 requires tqdm==4.62.0, but you have tqdm 4.64.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed GitPython-3.1.27 allennlp-2.9.3 base58-2.1.1 cached-path-1.1.5 docker-pycreds-0.4.0 fairscale-0.4.6 filelock-3.6.0 gitdb-4.0.9 google-api-core-2.8.2 google-cloud-core-2.3.1 google-cloud-storage-2.4.0 google-crc32c-1.3.0 google-resumable-media-2.3.3 googleapis-common-protos-1.56.4 jsonnet-0.18.0 lmdb-1.3.0 pathtools-0.1.2 promise-2.3 psutil-5.9.1 pydantic-1.8.2 sentry-sdk-1.7.0 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 spacy-3.2.4 thinc-8.0.17 wandb-0.12.21\n"
     ]
    }
   ],
   "source": [
    "!pip install allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AcquireReturnProxy' from 'filelock' (/home/petrakov/.local/lib/python3.8/site-packages/filelock.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-5824216e0aa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mallennlp_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf_tagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://storage.googleapis.com/allennlp-public-models/ner-model-2020.02.10.tar.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m allen_result = predictor.predict(\n\u001b[1;32m      5\u001b[0m   \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menglish_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/allennlp/predictors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPredictor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwraps\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_tagger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTaggerPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_classifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextClassifierPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegistrable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/allennlp/common/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFromParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistrable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegistrable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/allennlp/common/from_params.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigurationError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/allennlp/common/lazy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/allennlp/common/params.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigurationError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/allennlp/common/file_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcached_path\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_cached_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m from cached_path import (  # noqa: F401\n\u001b[1;32m     41\u001b[0m     \u001b[0mresource_to_filename\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_resource_to_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/cached_path/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \"\"\"\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_cached_path\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_cache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_cache_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mprogress\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_download_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/cached_path/_cached_path.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcache_file\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCacheFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPathOrStr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_cache_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfile_lock\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileLock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mschemes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_scheme_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_supported_schemes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_get_from_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/cached_path/file_lock.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfilelock\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAcquireReturnProxy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfilelock\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileLock\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_FileLock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AcquireReturnProxy' from 'filelock' (/home/petrakov/.local/lib/python3.8/site-packages/filelock.py)"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.ner.crf_tagger\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/ner-model-2020.02.10.tar.gz\")\n",
    "allen_result = predictor.predict(\n",
    "  sentence=english_text\n",
    ")\n",
    "for i in zip(allen_result['tags'], allen_result['words']):\n",
    "    if (i[0]) != 'O':\n",
    "      print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Quien descubrio [START] America [END] ?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = NER(text = '¿Quien descubrio America?',\n",
    "                tokenizer_name = \"Babelscape/wikineural-multilingual-ner\",\n",
    "                model_name = \"Babelscape/wikineural-multilingual-ner\")\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'id': 'Q828',\n",
       "   'texts': ['América >> es', 'America >> es'],\n",
       "   'scores': tensor([-0.1263, -1.3828]),\n",
       "   'score': tensor(-0.1747)},\n",
       "  {'id': 'Q29409515',\n",
       "   'texts': ['América (desambiguación) >> es',\n",
       "    'América (desambiguação) >> pt'],\n",
       "   'scores': tensor([-0.3790, -1.0400]),\n",
       "   'score': tensor(-1.0819)},\n",
       "  {'id': 'Q18',\n",
       "   'texts': ['América del Sur >> es'],\n",
       "   'scores': tensor([-1.0630]),\n",
       "   'score': tensor(-2.6038)}]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentences = [\"[START] The founder of the theory of relativity [END] received the Nobel Prize.\"]\n",
    "model_mGENRE.sample(\n",
    "    sentences = [sentences],\n",
    "    beam = 5,\n",
    "    prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
    "        e for e in trie.get(sent.tolist())\n",
    "        if e < len(model_mGENRE.task.target_dictionary)\n",
    "    ],\n",
    "    text_to_id=lambda x: max(lang_title2wikidataID[tuple(reversed(x.split(\" >> \")))], key=lambda y: int(y[1:])),\n",
    "    marginalize=True,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New experiment GENRE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!! you have to uncomment cells below to load data if it is 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Data and dependencies\n",
    "\n",
    "# id_p_e_m = \"1C2R814tsgZbREaaOk6o9nh3lQn308Wo7\"\n",
    "# gdown.download(id=id_p_e_m, output=\"prob_yago_crosswikis_wikipedia_p_e_m.txt\", quiet=False)\n",
    "\n",
    "# %mkdir data\n",
    "# %cd data\n",
    "# %mkdir dalab\n",
    "# %cd ..\n",
    "\n",
    "# %mv prob_yago_crosswikis_wikipedia_p_e_m.txt data/dalab/prob_yago_crosswikis_wikipedia_p_e_m.txt\n",
    "\n",
    "# ! wget http://resources.mpi-inf.mpg.de/yago-naga/aida/download/aida_means.tsv.bz2\n",
    "# ! bzip2 -dk aida_means.tsv.bz2\n",
    "\n",
    "# %cd data \n",
    "# %mkdir aida\n",
    "# %cd ..\n",
    "\n",
    "# %mv aida_means.tsv data/aida/aida_means.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_dalab_candidates():\n",
    "#     for line in open(\"data/dalab/prob_yago_crosswikis_wikipedia_p_e_m.txt\"):\n",
    "#         line = line[:-1]\n",
    "#         columns = line.split(\"\\t\")\n",
    "#         mention = columns[0]\n",
    "#         for column in columns[2:]:\n",
    "#             if len(column.strip()) == 0:\n",
    "#                 continue\n",
    "#             values = column.split(\",\")\n",
    "#             candidate = \",\".join(values[2:])\n",
    "#             candidate = candidate.replace(\"_\", \" \")\n",
    "#             yield mention, candidate\n",
    "\n",
    "\n",
    "# def hex2int(hexa: str) -> int:\n",
    "#     return int(hexa, 16)\n",
    "\n",
    "\n",
    "# def replace_unicode(u_str):\n",
    "#     matches = set(re.findall(\"\\\\\\\\u....\", u_str))\n",
    "#     for match in matches:\n",
    "#         u_str = u_str.replace(match, chr(hex2int(match[2:])))\n",
    "#     return u_str\n",
    "\n",
    "\n",
    "# PUNCTUATION_CHARS = set(string.punctuation)\n",
    "\n",
    "\n",
    "# def filter_mention(mention):\n",
    "#     if mention[0].islower():\n",
    "#         return True\n",
    "#     if mention[0] in PUNCTUATION_CHARS:\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "\n",
    "# def read_aida_candidates():\n",
    "#     for line in open(\"data/aida/aida_means.tsv\"):\n",
    "#         line = line[:-1]\n",
    "#         values = line.split(\"\\t\")\n",
    "#         mention = replace_unicode(values[0][1:-1])\n",
    "#         candidate = replace_unicode(values[1]).replace(\"_\", \" \")\n",
    "#         yield mention, candidate\n",
    "\n",
    "\n",
    "# #making mention_candidates_dict\n",
    "# #once done no need to do again\n",
    "\n",
    "# mention_candidates_dict = {}\n",
    "# for mention, candidate in itertools.chain(read_dalab_candidates(), read_aida_candidates()):\n",
    "#     if filter_mention(mention):\n",
    "#         continue\n",
    "#     if mention not in mention_candidates_dict:\n",
    "#         mention_candidates_dict[mention] = set()\n",
    "#     mention_candidates_dict[mention].add(candidate)\n",
    "# for mention in mention_candidates_dict:\n",
    "#     mention_candidates_dict[mention] = sorted(mention_candidates_dict[mention])\n",
    "# with open(\"data/mention_candidates_dict.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(mention_candidates_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # making mention_trie\n",
    "# # once done no need to do again\n",
    "\n",
    "# sys.setrecursionlimit(10000)\n",
    "# model_path = \"fairseq_e2e_entity_linking_wiki_abs\"\n",
    "# model = GENRE.from_pretrained(model_path).eval()\n",
    "# with open(\"data/mention_candidates_dict.pkl\", \"rb\") as f:\n",
    "#     mention_to_candidates_dict = pickle.load(f)\n",
    "# mention_trie = Trie()\n",
    "# for mention in tqdm(mention_to_candidates_dict):\n",
    "#     encoded = model.encode(\" {}\".format(mention))[1:].tolist()\n",
    "#     mention_trie.add(encoded)\n",
    "# out_file = \"data/mention_trie.pkl\"\n",
    "# with open(out_file, \"wb\") as f:\n",
    "#     pickle.dump(mention_trie, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example from git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from GENRE.genre.utils import get_entity_spans_fairseq as get_entity_spans\n",
    "from GENRE.genre.utils import get_markdown\n",
    "\n",
    "\n",
    "from GENRE.genre.trie import Trie, MarisaTrie\n",
    "from GENRE.genre.fairseq_model import mGENRE\n",
    "from GENRE.genre.fairseq_model import GENRE\n",
    "\n",
    "\n",
    "model_path_genre = \"fairseq_e2e_entity_linking_wiki_abs\"\n",
    "model_path_mgenre = \"fairseq_multilingual_entity_disambiguation\"\n",
    "dict_path = \"data/mention_candidates_dict.pkl\"\n",
    "trie_path = \"data/mention_trie.pkl\"\n",
    "\n",
    "model = GENRE.from_pretrained(model_path_genre).eval()\n",
    "\n",
    "with open(trie_path, \"rb\") as f:\n",
    "    mention_trie = pickle.load(f)\n",
    "with open(dict_path, \"rb\") as f:\n",
    "    mention_to_candidates_dict = pickle.load(f)\n",
    "\n",
    "text = \"\"\"Home Depot CEO Nardelli quits Home-improvement retailer's chief executive had been criticized over pay ATLANTA - Bob Nardelli abruptly resigned Wednesday as chairman and chief executive of The Home Depot Inc. after a six-year tenure that saw the world’s largest home improvement store chain post big profits but left investors disheartened by poor stock performance. Nardelli has also been under fire by investors for his hefty pay and is leaving with a severance package valued at about $210 million. He became CEO in December 2000 after being passed over for the top job at General Electric Co., where Nardelli had been a senior executive. Home Depot said Nardelli was being replaced by Frank Blake, its vice chairman, effective immediately. Blake’s appointment is permanent, Home Depot spokesman Jerry Shields said. What he will be paid was not immediately disclosed, Shields said. The company declined to make Blake available for comment, and a message left for Nardelli with his secretary was not immediately returned. Before Wednesday’s news, Home Depot’s stock had been down more than 3 percent on a split-adjusted basis since Nardelli took over. Nardelli’s sudden departure was stunning in that he told The Associated Press as recently as Sept. 1 that he had no intention of leaving, and a key director also said that the board was pleased with Nardelli despite the uproar by some investors. Asked in that interview if he had thought of hanging up his orange apron and leaving Home Depot, Nardelli said unequivocally that he hadn’t. Asked what he thought he would be doing 10 years from now, Nardelli said, “Selling hammers.” For The Home Depot? “Absolutely,” he said at the time. Home Depot said Nardelli’s decision to resign was by mutual agreement with the Atlanta-based company. “We are very grateful to Bob for his strong leadership of The Home Depot over the past six years. Under Bob’s tenure, the company made significant and necessary investments that greatly improved the company’s infrastructure and operations, expanded our markets to include wholesale distribution and new geographies, and undertook key strategic initiatives to strengthen the company’s foundation for the future,” Home Depot’s board said in a statement. Nardelli was a nuts-and-bolts leader, a former college football player and friend of President Bush. He helped increase revenue and profits at Home Depot and increase the number of stores the company operates to more than 2,000. Home Depot’s earnings per share have increased by approximately 150 percent over the last five years.\"\"\"\n",
    "\n",
    "sentences = [text]\n",
    "entity_spans = get_entity_spans(\n",
    "    model,\n",
    "    sentences,\n",
    "    mention_trie=mention_trie,\n",
    "    mention_to_candidates_dict=mention_to_candidates_dict\n",
    ")\n",
    "markdown = get_markdown(sentences, entity_spans)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot) CEO Nardelli quits Home-improvement retailer's chief executive had been criticized over pay ATLANTA - [Bob Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) abruptly resigned Wednesday as chairman and chief executive of [The Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot) Inc. after a six-year tenure that saw the world’s largest home improvement store chain post big profits but left investors disheartened by poor stock performance. [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) has also been under fire by investors for his hefty pay and is leaving with a severance package valued at about $210 million. He became CEO in December 2000 after being passed over for the top job at [General Electric](https://en.wikipedia.org/wiki/General_Electric) Co., where [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) had been a senior executive. [Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot) said [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) was being replaced by [Frank Blake](https://en.wikipedia.org/wiki/Frank_Blake), its vice chairman, effective immediately. [Blake](https://en.wikipedia.org/wiki/Frank_Blake)’s appointment is permanent, [Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot) spokesman [Jerry Shields](https://en.wikipedia.org/wiki/Jerry_A._Shields) said. [What](https://en.wikipedia.org/wiki/What?_(film)) he will be paid was not immediately disclosed, [Shields](https://en.wikipedia.org/wiki/Jerry_A._Shields) said. [The](https://en.wikipedia.org/wiki/The_Home_Depot) company declined to make [Blake](https://en.wikipedia.org/wiki/Frank_Blake) available for comment, and a message left for [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) with his secretary was not immediately returned. [Before](https://en.wikipedia.org/wiki/Before_(song)) Wednesday’s news, [Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot)’s stock had been down more than 3 percent on a split-adjusted basis since [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) took over. [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli)’s sudden departure was stunning in that he told [The](https://en.wikipedia.org/wiki/The_Home_Depot) Associated Press as recently as Sept. 1 that he had no intention of leaving, and a key director also said that the board was pleased with [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) despite the uproar by some investors. Asked in that interview if he had thought of hanging up his orange apron and leaving [Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot), [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) said unequivocally that he hadn’t. Asked what he thought he would be doing [10](https://en.wikipedia.org/wiki/10_(film)) years from now, [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli) said, “Selling hammers.” For [The](https://en.wikipedia.org/wiki/The_Mr._T_Experience) Home Depot? “Absolutely,” he said at the time. [Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot) said [Nardelli](https://en.wikipedia.org/wiki/Robert_Nardelli)’s decision to resign was by mutual agreement with the [Atlanta](https://en.wikipedia.org/wiki/Decatur,_Georgia)-based company. “We are very grateful to [Bob](https://en.wikipedia.org/wiki/Barbecue_Bob) for his strong leadership of [The](https://en.wikipedia.org/wiki/U.S._Department_of_State_Foreign_Affairs_Manual) [Home Depot](https://en.wikipedia.org/wiki/The_Home_Depot) over the past six years. Under [Bob](https://en.wikipedia.org/wiki/Bank_of_Baroda)’s tenure, the company made significant and necessary investments that greatly improved the company’s infrastructure and operations, expanded our markets to include wholesale distribution and new geographies, and undertook key strategic initiatives to strengthen the company’s foundation for the future,” [Home](https://en.wikipedia.org/wiki/John_Home) [Depot](https://en.wikipedia.org/wiki/Depot)’s board said in a statement. [Nardelli](https://en.wikipedia.org/wiki/Nardelli) was a nuts-and-bolts leader, a former college football player and friend of [President](https://en.wikipedia.org/wiki/Senegalese_presidential_election,_2007) Bush. [He](https://en.wikipedia.org/wiki/J._M._E._McTaggart) helped increase revenue and profits at [Home](https://en.wikipedia.org/wiki/New_Recreation_Ground) [Depot](https://en.wikipedia.org/wiki/Depot) and increase the number of stores the company operates to more than [2](https://en.wikipedia.org/wiki/U.S._Route_2),000. [Home](https://en.wikipedia.org/wiki/New_Recreation_Ground) [Depot](https://en.wikipedia.org/wiki/Depot)’s earnings per share have increased by approximately [150](https://en.wikipedia.org/wiki/U.S._Route_150) percent over the last five years."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mGENRE case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GENRE.genre.utils import get_entity_spans_fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mGENRE.from_pretrained(model_path_mgenre).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GENREHubInterface(\n",
       "  (models): ModuleList(\n",
       "    (0): BARTModel(\n",
       "      (encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): TransformerDecoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (output_projection): Linear(in_features=1024, out_features=256001, bias=False)\n",
       "      )\n",
       "      (classification_heads): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (model): BARTModel(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (output_projection): Linear(in_features=1024, out_features=256001, bias=False)\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-57e3d1b5b606>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Moscow is the capital of Russia, where such great people as Kondratiev and Kolmogorov lived in 20 century\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m entity_spans = get_entity_spans_fairseq(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#    mention_trie=mention_trie,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/success/mGENRE_MEL/GENRE/genre/utils.py\u001b[0m in \u001b[0;36mget_entity_spans_fairseq\u001b[0;34m(model, input_sentences, mention_trie, candidates_trie, mention_to_candidates_dict, redirections)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mredirections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m ):\n\u001b[0;32m--> 153\u001b[0;31m     return _get_entity_spans(\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0minput_sentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/success/mGENRE_MEL/GENRE/genre/utils.py\u001b[0m in \u001b[0;36m_get_entity_spans\u001b[0;34m(model, input_sentences, prefix_allowed_tokens_fn, redirections)\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     output_sentences = get_entity_spans_post_processing(\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     )\n",
      "\u001b[0;32m~/success/mGENRE_MEL/GENRE/genre/utils.py\u001b[0m in \u001b[0;36mget_entity_spans_post_processing\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"{.*?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{ \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"}.*?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"} \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\].*?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"] \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "sentences = [\"Moscow is the capital of Russia, where such great people as Kondratiev and Kolmogorov lived in 20 century\"]\n",
    "entity_spans = get_entity_spans_fairseq(\n",
    "    model,\n",
    "    sentences,\n",
    "#    mention_trie=mention_trie,\n",
    "#    mention_to_candidates_dict=mention_to_candidates_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GENREHubInterface(\n",
       "  (models): ModuleList(\n",
       "    (0): BARTModel(\n",
       "      (encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): TransformerDecoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (output_projection): Linear(in_features=1024, out_features=256001, bias=False)\n",
       "      )\n",
       "      (classification_heads): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (model): BARTModel(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(256001, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (output_projection): Linear(in_features=1024, out_features=256001, bias=False)\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown = get_markdown(sentences, entity_spans)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[Moscow](https://en.wikipedia.org/wiki/Moscow) is the capital of [Russia](https://en.wikipedia.org/wiki/Russia), where such great people as [Kondratiev](https://en.wikipedia.org/wiki/Mikhail_Kondratiev) and [Kolmogorov](https://en.wikipedia.org/wiki/Andrey_Kolmogorov) lived in 20 century"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GENRE.genre.utils import get_entity_spans_fairseq as get_entity_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {\n",
    "    \"id_0\": \"In 1921, Einstein received a Nobel Prize.\",\n",
    "    \"id_1\": \"Armstrong was the first man on the Moon.\",\n",
    "}\n",
    "\n",
    "gold_entities = [\n",
    "    (\"id_0\", 3, 4, \"1921\"),\n",
    "    (\"id_0\", 9, 8, 'Albert_Einstein'),\n",
    "    (\"id_0\", 29, 11, 'Nobel_Prize_in_Physics'),\n",
    "    (\"id_1\", 0, 9, 'Neil_Armstrong'),\n",
    "    (\"id_1\", 35, 4, 'Moon'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guess_entities [[(0, 7, 'List_of_Nobel_laureates_by_year_of_appointment'), (9, 8, 'Albert_Einstein'), (29, 11, 'Nobel_Prize_in_Physiology_or_Medicine'), (40, 1, 'List_of_Nobel_laureates_in_Physiology_or_Medicine_by_year_of_appointment')], [(18, 9, 'First_Man_(film)'), (35, 4, 'Moon_(TV_series)_(1968_TV_series,_season_1)'), (39, 1, 'Moon_(TV_series,_season_1)')]]\n",
      "#############\n",
      "guess_entities [('id_0', 0, 7, 'List_of_Nobel_laureates_by_year_of_appointment'), ('id_0', 9, 8, 'Albert_Einstein'), ('id_0', 29, 11, 'Nobel_Prize_in_Physiology_or_Medicine'), ('id_0', 40, 1, 'List_of_Nobel_laureates_in_Physiology_or_Medicine_by_year_of_appointment'), ('id_1', 18, 9, 'First_Man_(film)'), ('id_1', 35, 4, 'Moon_(TV_series)_(1968_TV_series,_season_1)'), ('id_1', 39, 1, 'Moon_(TV_series,_season_1)')]\n"
     ]
    }
   ],
   "source": [
    "guess_entities = get_entity_spans(\n",
    "    model,\n",
    "    list(documents.values()),\n",
    ")\n",
    "print(\"guess_entities\", guess_entities)\n",
    "print(\"#############\")\n",
    "\n",
    "guess_entities = [\n",
    "    (k,) + x\n",
    "    for k, e in zip(documents.keys(), guess_entities)\n",
    "    for x in e\n",
    "]\n",
    "print(\"guess_entities\", guess_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_p=0.1429 micro_r=0.2000, micro_f1=0.1667, macro_p=0.1250, macro_r=0.1667, macro_f1=0.1429\n"
     ]
    }
   ],
   "source": [
    "from GENRE.genre.utils import (\n",
    "    get_micro_precision,\n",
    "    get_micro_recall,\n",
    "    get_micro_f1,\n",
    "    get_macro_precision,\n",
    "    get_macro_recall,\n",
    "    get_macro_f1,\n",
    ")\n",
    "\n",
    "micro_p = get_micro_precision(guess_entities, gold_entities)\n",
    "micro_r = get_micro_recall(guess_entities, gold_entities)\n",
    "micro_f1 = get_micro_f1(guess_entities, gold_entities)\n",
    "macro_p = get_macro_precision(guess_entities, gold_entities)\n",
    "macro_r = get_macro_recall(guess_entities, gold_entities)\n",
    "macro_f1 = get_macro_f1(guess_entities, gold_entities)\n",
    "\n",
    "print(\n",
    "   \"micro_p={:.4f} micro_r={:.4f}, micro_f1={:.4f}, macro_p={:.4f}, macro_r={:.4f}, macro_f1={:.4f}\".format(\n",
    "       micro_p, micro_r, micro_f1, macro_p, macro_r, macro_f1\n",
    "   )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load mewsli-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_df = pd.read_csv('dataset/en/mentions.tsv', sep='\\t')\n",
    "#en_df_doc = pd.read_csv('docs.tsv', sep='\\t')\n",
    "en_df_doc = pd.read_csv('mewsli_9/dense_representations_for_entity_retrieval/mel/mewsli-9/output/dataset/en/docs.tsv', sep='\\t')\n",
    "en_df_men = pd.read_csv('mewsli_9/dense_representations_for_entity_retrieval/mel/mewsli-9/output/dataset/en/mentions.tsv', sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/petrakov/success/mGENRE_MEL/mewsli_9/dense_representations_for_entity_retrieval/mel\n"
     ]
    }
   ],
   "source": [
    "%cd mewsli_9/dense_representations_for_entity_retrieval/mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open(\"mewsli-9/output/wikiextractor/en/AA/wiki_00.bz2\", \"rb\") as f:\n",
    "    # Decompress data from file\n",
    "    en_wiki = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/petrakov/success/mGENRE_MEL/mewsli_9/dense_representations_for_entity_retrieval/mel/mewsli-9/output/output/dataset/en/text\n"
     ]
    }
   ],
   "source": [
    "%cd mewsli-9/output/output/dataset/en/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = en_df_men[en_df_men['docid'] == \"en-106602\"]\n",
    "b = en_df_men[en_df_men[\"docid\"] == \"en-106608\"].reset_index()\n",
    "c = en_df_men[en_df_men[\"docid\"] == \"en-106610\"].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/petrakov/success/mGENRE_MEL\n"
     ]
    }
   ],
   "source": [
    "%cd /home/petrakov/success/mGENRE_MEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"mewsli_9/dense_representations_for_entity_retrieval/mel/mewsli-9/output/output/dataset/en/text/\"\n",
    "texts = []\n",
    "for st in [\"en-106602\", \"en-106608\", \"en-106610\"]:\n",
    "    with open(directory + st, \"r\") as f:\n",
    "        string = f.read()\n",
    "    \n",
    "    string = string.replace(\"\\n\", \" \").replace(\"\\xa0\", \"\")\n",
    "    string = string.replace(\"_\", \" \")\n",
    "    string = string.replace(\"\\n\", \" \")\n",
    "    texts.append(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = {\n",
    "#     \"id_0\": texts[0],\n",
    "#     \"id_1\": texts[1],\n",
    "#     \"id_2\": texts[2],\n",
    "# }\n",
    "\n",
    "documents = {\n",
    "    \"id_0\": \"In 1921, Einstein received a Nobel Prize.\",\n",
    "    \"id_1\": \"Armstrong was the first man on the Moon.\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "gold_entities = [\n",
    "    (\"id_0\", 3, 4, \"1921\"),\n",
    "    (\"id_0\", 9, 8, 'Albert_Einstein'),\n",
    "    (\"id_0\", 29, 11, 'Nobel_Prize_in_Physics'),\n",
    "    (\"id_1\", 0, 9, 'Neil_Armstrong'),\n",
    "    (\"id_1\", 35, 4, 'Moon'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {\n",
    "    \"id_0\": texts[0],\n",
    "    \"id_1\": texts[1],\n",
    "    \"id_2\": texts[2],\n",
    "}\n",
    "\n",
    "gold_entities = [*[tuple([\"id_0\", *list(a.loc[i, [\"position\", \"length\", \"mention\"]])]) for i in range(len(a))],\n",
    "                *[tuple([\"id_1\", *list(b.loc[i, [\"position\", \"length\", \"mention\"]])]) for i in range(len(b))],\n",
    "                *[tuple([\"id_2\", *list(c.loc[i, [\"position\", \"length\", \"mention\"]])]) for i in range(len(c))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from GENRE.genre.utils import get_entity_spans_fairseq as get_entity_spans\n",
    "from GENRE.genre.utils import get_markdown\n",
    "\n",
    "\n",
    "from GENRE.genre.trie import Trie, MarisaTrie\n",
    "from GENRE.genre.fairseq_model import mGENRE\n",
    "from GENRE.genre.fairseq_model import GENRE\n",
    "\n",
    "\n",
    "model_path_genre = \"fairseq_e2e_entity_linking_wiki_abs\"\n",
    "model_path_mgenre = \"fairseq_multilingual_entity_disambiguation\"\n",
    "dict_path = \"data/mention_candidates_dict.pkl\"\n",
    "trie_path = \"data/mention_trie.pkl\"\n",
    "\n",
    "model = GENRE.from_pretrained(model_path_genre).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_entities = get_entity_spans(\n",
    "    model,\n",
    "    list(documents.values()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_entities_1 = [\n",
    "    (k,*x)\n",
    "    for k, e in zip(documents.keys(), guess_entities)\n",
    "    for x in e\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_p=0.0000 micro_r=0.0000, micro_f1=0.0000, macro_p=0.0000, macro_r=0.0000, macro_f1=0.0000\n"
     ]
    }
   ],
   "source": [
    "from GENRE.genre.utils import (\n",
    "    get_micro_precision,\n",
    "    get_micro_recall,\n",
    "    get_micro_f1,\n",
    "    get_macro_precision,\n",
    "    get_macro_recall,\n",
    "    get_macro_f1,\n",
    ")\n",
    "\n",
    "micro_p = get_micro_precision(guess_entities_1, gold_entities)\n",
    "micro_r = get_micro_recall(guess_entities_1, gold_entities)\n",
    "micro_f1 = get_micro_f1(guess_entities_1, gold_entities)\n",
    "macro_p = get_macro_precision(guess_entities_1, gold_entities)\n",
    "macro_r = get_macro_recall(guess_entities_1, gold_entities)\n",
    "macro_f1 = get_macro_f1(guess_entities_1, gold_entities)\n",
    "\n",
    "print(\n",
    "   \"micro_p={:.4f} micro_r={:.4f}, micro_f1={:.4f}, macro_p={:.4f}, macro_r={:.4f}, macro_f1={:.4f}\".format(\n",
    "       micro_p, micro_r, micro_f1, macro_p, macro_r, macro_f1\n",
    "   )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_p=0.0000 micro_r=0.0000, micro_f1=0.0000, macro_p=0.0000, macro_r=0.0000, macro_f1=0.0000\n"
     ]
    }
   ],
   "source": [
    "from GENRE.genre.utils import (\n",
    "    get_micro_precision,\n",
    "    get_micro_recall,\n",
    "    get_micro_f1,\n",
    "    get_macro_precision,\n",
    "    get_macro_recall,\n",
    "    get_macro_f1,\n",
    ")\n",
    "\n",
    "micro_p = get_micro_precision(guess_entities_1, gold_entities)\n",
    "micro_r = get_micro_recall(guess_entities_1, gold_entities)\n",
    "micro_f1 = get_micro_f1(guess_entities_1, gold_entities)\n",
    "macro_p = get_macro_precision(guess_entities_1, gold_entities)\n",
    "macro_r = get_macro_recall(guess_entities_1, gold_entities)\n",
    "macro_f1 = get_macro_f1(guess_entities_1, gold_entities)\n",
    "\n",
    "print(\n",
    "   \"micro_p={:.4f} micro_r={:.4f}, micro_f1={:.4f}, macro_p={:.4f}, macro_r={:.4f}, macro_f1={:.4f}\".format(\n",
    "       micro_p, micro_r, micro_f1, macro_p, macro_r, macro_f1\n",
    "   )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id_0', 145, 39, 'Pacific Marine Environmental Laboratory'),\n",
       " ('id_0', 365, 15, 'Baja California'),\n",
       " ('id_0', 1013, 13, 'oceanographer'),\n",
       " ('id_0', 1109, 5, 'Earth'),\n",
       " ('id_0', 1150, 14, 'carbon dioxide'),\n",
       " ('id_0', 1278, 9, 'carbonate'),\n",
       " ('id_0', 1385, 13, 'carbonic acid'),\n",
       " ('id_0', 1769, 21, 'Industrial Revolution'),\n",
       " ('id_1', 118, 33, '2006 Lathen maglev train accident'),\n",
       " ('id_1', 231, 6, 'maglev'),\n",
       " ('id_1', 358, 11, 'human error'),\n",
       " ('id_1', 454, 10, 'Transrapid'),\n",
       " ('id_1', 471, 2, 'cm'),\n",
       " ('id_1', 583, 9, 'Osnabrück'),\n",
       " ('id_1', 643, 4, 'euro'),\n",
       " ('id_2', 189, 8, 'asbestos'),\n",
       " ('id_2', 423, 16, 'terminal illness'),\n",
       " ('id_2', 459, 12, 'mesothelioma'),\n",
       " ('id_2', 520, 12, 'James Hardie'),\n",
       " ('id_2', 634, 10, 'asbestosis'),\n",
       " ('id_2', 693, 13, 'lung scarring'),\n",
       " ('id_2', 812, 6, 'cancer'),\n",
       " ('id_2', 861, 22, 'lungs and chest cavity'),\n",
       " ('id_2', 885, 16, 'abdominal cavity'),\n",
       " ('id_2', 910, 25, 'sac surrounding the heart'),\n",
       " ('id_2', 1136, 17, 'Victorian Premier'),\n",
       " ('id_2', 1154, 11, 'John Brumby')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Baja California'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0][365-1:365+15-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id_0', 0, 4, 'National_Oceanic_and_Atmospheric_Administration'),\n",
       " ('id_0', 87, 49, 'National_Oceanic_and_Atmospheric_Administration'),\n",
       " ('id_0',\n",
       "  144,\n",
       "  39,\n",
       "  'National_Oceanic_and_Atmospheric_Administration_National_Marine_Environmental_Laboratory'),\n",
       " ('id_0', 210, 14, 'World_Ocean'),\n",
       " ('id_0', 345, 13, 'Pacific_Ocean'),\n",
       " ('id_0', 364, 23, 'Baja_California'),\n",
       " ('id_0', 391, 27, 'Vancouver'),\n",
       " ('id_0', 491, 17, 'Continental_shelf'),\n",
       " ('id_0', 556, 13, 'Marine_life'),\n",
       " ('id_0',\n",
       "  586,\n",
       "  6,\n",
       "  'Oceanic_carbonate_marine_life_in_the_United_States_and_Great_Britain_of_Great_Britain_and_Ireland_of_1844–1855'),\n",
       " ('id_0', 675, 10, 'California'),\n",
       " ('id_0',\n",
       "  703,\n",
       "  13,\n",
       "  'List_of_countries_by_number_of_military_and_paramilitary_personnel_in_the_second_year_of_conflict_with_the_United_States_(1944–46)'),\n",
       " ('id_0', 719, 72, 'Ocean_acidification'),\n",
       " ('id_0', 819, 11, 'Marine_life'),\n",
       " ('id_0',\n",
       "  838,\n",
       "  17,\n",
       "  'Continental_shelf_of_the_United_States_and_Great_Britain_of_Great_Britain_of_1844–1855_(1944-1955)'),\n",
       " ('id_0',\n",
       "  856,\n",
       "  9,\n",
       "  'List of countries by number of military and paramilitary personnel in the second year of conflict with the United States (1944–46) (1944–46) (1944–46) (1944–46) ) and increasing the levels of carbonic acid. \"Scientists have also seen a reduced ability of marine algae and free-floating plants and animals to produce protective carbonate shells\" added Feely noted that , according to the study , the oceans have absorbed more than 525 billion tons of carbon dioxide since the Industrial Revolution began  -'),\n",
       " ('id_1', 0, 42, 'Two_men_fined_over_2006_German_train_crash'),\n",
       " ('id_1', 117, 33, '2006_Lathen_maglev_train_accident'),\n",
       " ('id_1', 154, 7, 'Germany'),\n",
       " ('id_1', 176, 25, '2006_Lathen_maglev_train_accident'),\n",
       " ('id_1', 205, 18, '2006_Lathen_maglev_train_accident'),\n",
       " ('id_1', 230, 6, 'Maglev'),\n",
       " ('id_1', 378, 5, 'Float_(nautical)'),\n",
       " ('id_1', 394, 5, 'Track_(rail_transport)'),\n",
       " ('id_1', 406, 2, 'Rail_profile'),\n",
       " ('id_1', 426, 9, 'Magnetism_(rail_transport)'),\n",
       " ('id_1', 453, 10, 'Transrapid'),\n",
       " ('id_1', 470, 2, 'Centimetre'),\n",
       " ('id_1', 483, 5, 'Track_(rail_transport)'),\n",
       " ('id_1', 504, 12, 'Rail_profile'),\n",
       " ('id_1', 541, 4, 'Risk'),\n",
       " ('id_1', 549, 11, 'Derailment'),\n",
       " ('id_1', 582, 9, 'Osnabrück'),\n",
       " ('id_1', 603, 13, 'Track_management'),\n",
       " ('id_1', 642, 4, 'Euro'),\n",
       " ('id_1', 665, 13, 'Joerg_Metzner'),\n",
       " ('id_1', 686, 4, 'Euro'),\n",
       " ('id_1', 732, 12, 'Manslaughter'),\n",
       " ('id_1',\n",
       "  762,\n",
       "  24,\n",
       "  'Negligence in German railway accidents and incidents of the 2006 Lathen maglev train crash and of the third defendant. The third defendant, traffic superintendent Guenther Mueller, was unable to face trial due to suicide fears.   -'),\n",
       " ('id_2', 16, 8, 'Asbestos'),\n",
       " ('id_2', 38, 19, 'Victoria_(Australia)'),\n",
       " ('id_2', 80, 19, 'Victoria_(Australia)'),\n",
       " ('id_2', 165, 31, 'Asbestos_and_the_health_effects_of_human_body_parts'),\n",
       " ('id_2', 224, 19, 'Asbestos_and_the_health_effects_of_human_body_parts'),\n",
       " ('id_2', 251, 47, 'Asbestos_and_the_health_effects_of_human_body_parts'),\n",
       " ('id_2', 300, 3, 'The_Asbestos_Show'),\n",
       " ('id_2', 350, 8, 'Asbestos'),\n",
       " ('id_2', 422, 16, 'Terminal_illness'),\n",
       " ('id_2', 438, 1, 'Asbestos_and_the_health_effects_of_human_body_parts'),\n",
       " ('id_2', 458, 12, 'Mesothelioma'),\n",
       " ('id_2', 489, 3, 'The_Asbestos_Show'),\n",
       " ('id_2', 519, 12, 'Asbestos_and_the_health_effects_of_human_body_parts'),\n",
       " ('id_2', 549, 1, 'Asbestos_and_the_health_effects_of_human_body_parts'),\n",
       " ('id_2', 552, 239, 'Asbestos_and_the_health_effects_of_human_body_parts'),\n",
       " ('id_2', 793, 12, 'Mesothelioma'),\n",
       " ('id_2', 811, 6, 'Cancer'),\n",
       " ('id_2', 836, 3, 'Asbestos_and_the_health_effects_of_human_body_parts'),\n",
       " ('id_2', 856, 3, 'Asbestos_and_the_health_effects_of_human_body_parts'),\n",
       " ('id_2', 866, 3, 'Asbestos_and_the_health_effects_of_human_body_parts'),\n",
       " ('id_2',\n",
       "  882,\n",
       "  0,\n",
       "  'Asbestos_and_the_health_effects_of_human_body_parts_of_the_human_body_in_the_human_body'),\n",
       " ('id_2',\n",
       "  905,\n",
       "  3,\n",
       "  'Asbestos_and_the_health_effects_of_human_body_parts_of_the_human_body_in_the_human_body_in_the_human_body'),\n",
       " ('id_2',\n",
       "  925,\n",
       "  3,\n",
       "  'Asbestos_and_the_health_effects_of_human_bodies_in_the_human_body_in_the_human_body'),\n",
       " ('id_2',\n",
       "  934,\n",
       "  1,\n",
       "  'Asbestos and the human body in Victoria, Australia and the health care in Victoria, Australia, and the health care in Victoria, Australia, and New South Wales, and New South Wales, and New South Wales, and Victoria, England, and Victoria, England, and the U.S. and the U.S.A. and the U.S.A. and the U.S.A.  -')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess_entities_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;31maida_means.tsv.bz2\u001b[0m\r\n",
      "\u001b[01;34mdata\u001b[0m/\r\n",
      "\u001b[01;34mdataset\u001b[0m/\r\n",
      "docs.tsv\r\n",
      "\u001b[01;34mfairseq\u001b[0m/\r\n",
      "\u001b[01;34mfairseq_e2e_entity_linking_wiki_abs\u001b[0m/\r\n",
      "\u001b[01;34mfairseq_multilingual_entity_disambiguation\u001b[0m/\r\n",
      "\u001b[01;31mfairseq_multilingual_entity_disambiguation.tar.gz\u001b[0m\r\n",
      "\u001b[01;34mGENRE\u001b[0m/\r\n",
      "\u001b[01;34mKILT\u001b[0m/\r\n",
      "lang_title2wikidataID-normalized_with_redirect.pkl\r\n",
      "mentions_test.json\r\n",
      "mentions.tsv\r\n",
      "\u001b[01;34mmewsli_9\u001b[0m/\r\n",
      "mgenre_final.ipynb.invalid\r\n",
      "mgenre_final_with_mewsli.ipynb\r\n",
      "README.md\r\n",
      "requirements.txt\r\n",
      "titles_lang_all105_marisa_trie_with_redirect.pkl\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting hanziconv\n",
      "  Downloading hanziconv-0.3.2.tar.gz (276 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: hanziconv\n",
      "  Building wheel for hanziconv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hanziconv: filename=hanziconv-0.3.2-py2.py3-none-any.whl size=23225 sha256=800fc8bc3ee5cfe9cacd7ccd8335d3cab8f5284f0eece677b82e7ac0eec312f5\n",
      "  Stored in directory: /home/petrakov/.cache/pip/wheels/24/bc/5f/95aceeea892d9bf06c29a9effde5908abcefd3fa23244fdaa5\n",
      "Successfully built hanziconv\n",
      "Installing collected packages: hanziconv\n",
      "Successfully installed hanziconv-0.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install hanziconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GENRE.scripts_mgenre.evaluate_kilt_dataset import evaluate_kilt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-06 02:07:20--  http://dl.fbaipublicfiles.com/KILT/wned-dev-kilt.jsonl\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12868348 (12M) [text/plain]\n",
      "Saving to: ‘wned-dev-kilt.jsonl’\n",
      "\n",
      "wned-dev-kilt.jsonl 100%[===================>]  12.27M  7.99MB/s    in 1.5s    \n",
      "\n",
      "2022-07-06 02:07:22 (7.99 MB/s) - ‘wned-dev-kilt.jsonl’ saved [12868348/12868348]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget http://dl.fbaipublicfiles.com/GENRE/ace2004-test-kilt.jsonl\n",
    "#!wget http://dl.fbaipublicfiles.com/KILT/fever-dev-kilt.jsonl\n",
    "#!wget http://dl.fbaipublicfiles.com/GENRE/fairseq_wikipage_retrieval.tar.gz\n",
    "!wget http://dl.fbaipublicfiles.com/KILT/wned-dev-kilt.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('wned-dev-kilt.jsonl', 'r') as f:\n",
    "    wned = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb36b05dc284e45aa50d4ab77c42e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating :   0%|          | 0/12852476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-4221964283fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_kilt_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_mGENRE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/success/mGENRE_MEL/GENRE/scripts_mgenre/evaluate_kilt_dataset.py\u001b[0m in \u001b[0;36mevaluate_kilt_dataset\u001b[0;34m(model, dataset, batch_size, beams, max_len_a, max_len_b, lenpen, trie, lang_title2wikidataID, wikidataID2lang_title, canonical_lang_title2wikidataID, wikidataID2canonical_lang_title, order, canonical, free_generation, mention2wikidataID, candidates_lowercase, allowed_langs, desc, max_candidates, only_en_candidates, only_freebase_candidates, wikidataID2freebaseID)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mbatch_trie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 mention = (\n\u001b[0m\u001b[1;32m     69\u001b[0m                     unicodedata.normalize(\n\u001b[1;32m     70\u001b[0m                         \u001b[0;34m\"NFKD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHanziConv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSimplified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mention\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "evaluate_kilt_dataset(model = model_mGENRE, dataset = wned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-ba3eb86d23cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfever\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'meta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "fever['meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<wikidata.entity.Entity Q20145 'IU'>,\n",
       " m'Korea  singer,actress record producer (2017)')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client()  # doctest: +SKIP\n",
    "entity = client.get('Q20145', load=True)\n",
    "entity, entity.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we present a function that illustrates predicted entity, text, right answer basing on the id as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_test_set(idx=0):\n",
    "    print(test_set[str(idx)]['mention_its']+\"\\n\")\n",
    "    start = test_set[str(idx)]['start_index']\n",
    "    end = test_set[str(idx)]['end_index']\n",
    "    mention_id = test_set[str(idx)]['mention_id']\n",
    "    text = test_set[str(idx)]['source_document']['text']\n",
    "    text = text[:start]+\"[START] \"+text[start:end]+\" [END]\"+text[end:]\n",
    "    print(f'{text[:start]}{bcolors.BOLD}{text[start:end+len(\"[START] \")+len(\" [END]\")]}{bcolors.END}{text[end+len(\"[START] \")+len(\" [END]\"):]}\\n')\n",
    "    result = model.sample(\n",
    "    sentences=[text],\n",
    "    text_to_id=lambda x: sorted(list(lang_title2wikidataID.get(\n",
    "        tuple(reversed((x.split(\" >> \")[0], x.split(\" >> \")[1][:2]))), [None])))[0],\n",
    "    marginalize=True)\n",
    "    print(result)\n",
    "    \n",
    "    entity = client.get(mention_id, load=True)\n",
    "    print(f'\\nCorrect entity : {mention_id, entity.label, entity.description}\\n')\n",
    "\n",
    "    candidates = [(i['id'], client.get(i['id'], load=True)) for i in result[0]]\n",
    "    print('Predicted entities:')\n",
    "    for i, entity in candidates:\n",
    "        if i is not None:\n",
    "            print(f'{i, entity.label, entity.description}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New South Wales\n",
      "\n",
      "Bathurst Regional Council, the local government responsible for the city of Bathurst and its surrounds in Central Western \u001b[1m[START] New South Wales [END]\u001b[0m, Australia yesterday revealed it had received a development application for the new Bathurst Base Hospital.The new hospital is to be built behind the current hospital on the same site and is expected to cost the New South Wales government AUD96 million. The Bathurst Hospital will be the first in the Bathurst-Orange-Bloomfield redevelopment project.The new hospital will have 149 beds, up from 85 for the current hospital. The hospital will also feature a mental health unit - previously psychiatric patients had to travel to Orange to the Bloomfield Hospital for treatment.The Bathurst Hospital is expected to have state-of-the art facilities and will share some services with the to be constructed Orange Base Hospital.The Bathurst Regional Council has approved the demolition of 12 buildings on the hospital site for enabling works. The hospital site is heritage listed although council decided that as the buildings do not contribute to the streetscape they may be demolished.The demolitions are expected to take place late next month and will take around six weeks to complete. A temporary driveway will then be built to replace the current service entry for food and linen as it will become part of the work site.Upon completion of the new hospital, the current ward block will be demolished leaving the original building from the late 19th century intact. The original building is expected to become an education centre and consulting rooms.The original building was opened in 1834. Since then the facility has undergone numerous upgrades and add-ons, with the present ward block being opened in stages from 1978 to 1982.Other buildings expected to be retained include the Daffodil Cottage (a cancer care centre) and the original Nurse's quarters known as Poole House.\n",
      "\n",
      "[[{'id': 'Q3224', 'texts': ['New South Wales >> en', 'New South Wales >> de'], 'scores': tensor([-0.0940, -1.9361], device='cuda:4'), 'score': tensor(-0.2194, device='cuda:4')}, {'id': None, 'texts': ['New South Welsh English >> en', 'NewSouth Wales >> en'], 'scores': tensor([-1.5938, -1.6026], device='cuda:4'), 'score': tensor(-3.3674, device='cuda:4')}, {'id': 'Q1353', 'texts': ['National Capital Territory >> en'], 'scores': tensor([-1.6513], device='cuda:4'), 'score': tensor(-4.3689, device='cuda:4')}]]\n",
      "\n",
      "Correct entity : ('Q3224', m'New South Wales', m'state of Australia')\n",
      "\n",
      "Predicted entities:\n",
      "('Q3224', m'New South Wales', m'state of Australia')\n",
      "('Q1353', m'Delhi', m'Indian metropolis and union territory that includes New Delhi')\n"
     ]
    }
   ],
   "source": [
    "predict_from_test_set(2354)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results:\n",
    "\n",
    "Language: **fa** (5 out of 61)\n",
    "\n",
    "Micro average (k=1): **1.0**\n",
    "\n",
    "Language: **sr** (105 out of 451)\n",
    "\n",
    "Micro average (k=1): **0.90476**\n",
    "\n",
    "Language: **ta** (25 out of 366)\n",
    "\n",
    "Micro average (k=1): **1.0**\n",
    "\n",
    "### Macro average (k=1): 0.96825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data.data_utils import collate_tokens\n",
    "\n",
    "\n",
    "def compute_results(idx):\n",
    "    start = test_set[str(idx)]['start_index']\n",
    "    end = test_set[str(idx)]['end_index']\n",
    "    mention_id = test_set[str(idx)]['mention_id']\n",
    "    text = test_set[str(idx)]['source_document']['text']\n",
    "    text = text[:start]+\"[START] \"+text[start:end]+\" [END]\"+text[end:]\n",
    "    print(start, end, min(start, len(text)-end))\n",
    "    #encoder = model.encoder(text)\n",
    "    #print(encoder)\n",
    "\n",
    "    result = model.sample(\n",
    "                    sentences=[text],\n",
    "                   text_to_id=lambda x: sorted(list(lang_title2wikidataID.get(\n",
    "       tuple(reversed((x.split(\" >> \")[0], x.split(\" >> \")[1][:2]))), [None])))[0],\n",
    "                    marginalize=True)\n",
    "    candidates = [(i['id'], client.get(i['id'], load=True)) for i in result[0] if i['id'] is not None]\n",
    "    entity = client.get(mention_id, load=True)\n",
    "    return {\n",
    "            \"correct\": mention_id, \n",
    "            \"predicted\": [i for i, _ in candidates]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 109 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'correct': 'Q36678', 'predicted': ['Q36678', 'Q2564150', 'Q7834492']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(lang='en', k_list=[1, 10, 20, 50, 100]):\n",
    "    scores = []\n",
    "    \n",
    "    for idx in tqdm(langs_idx[lang]):\n",
    "        scores.append(compute_results(idx))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_idx = {\n",
    "    'ar':[], 'en':[], 'tr':[]\n",
    "}\n",
    "for idx in test_set:\n",
    "    el = test_set[idx]\n",
    "    lang = el['document_id'][:2]\n",
    "    if lang in langs_idx:\n",
    "        langs_idx[lang].append(int(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_avg_lang(scores, lang='en', k_list=[1, 10, 20, 50, 100]):    \n",
    "    print(f'Language: {lang}')\n",
    "    results = []\n",
    "    for k in k_list:\n",
    "        n = 0\n",
    "        for i, row in enumerate(scores):\n",
    "            if scores['correct'] in scores['predicted'][:k]:\n",
    "                n += 1\n",
    "        results.append(round(n / total_mentions, 5))\n",
    "    for i, k in enumerate(k_list):\n",
    "        print(f'Micro average (k={k}):', results[i])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of GENRE exploration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
